<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>6.790 Machine Learning | 6.790 Machine Learning, Fall 2023</title> <meta name="generator" content="Jekyll v4.3.2" /> <meta property="og:title" content="6.790 Machine Learning" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="Graduate Machine Learning Course, MIT EECS, Fall 2023." /> <meta property="og:description" content="Graduate Machine Learning Course, MIT EECS, Fall 2023." /> <link rel="canonical" href="https://gradml.mit.edu/classification_foundation/" /> <meta property="og:url" content="https://gradml.mit.edu/classification_foundation/" /> <meta property="og:site_name" content="6.790 Machine Learning" /> <meta property="og:type" content="website" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="6.790 Machine Learning" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","dateModified":"2023-06-18T19:25:55-04:00","description":"Graduate Machine Learning Course, MIT EECS, Fall 2023.","headline":"6.790 Machine Learning","url":"https://gradml.mit.edu/classification_foundation/"}</script> <!-- End Jekyll SEO tag --> <link rel="shortcut icon" type="image/png" href="/assets/images/favicon.png"> <!-- CSS --> <link rel="stylesheet" href="/assets/js/katex.min.css" /> <!-- JavaScript --> <script defer src="/assets/js/katex.min.js"></script> <script defer src="/assets/js/auto-render.min.js" onload="renderMathInElement(document.body,{ delimiters: [ { left: '$$', right: '$$', display: true }, { left: '$', right: '$', display: false }, { left: '\\[', right: '\\]', display: true }, { left: '\\(', right: '\\)', display: false } ] } );"> </script> <script src="/assets/js/hypothesis.js" async></script> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"> 6.790 <br> Machine Learning </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"> <use xlink:href="#svg-menu"></use> </svg> </a> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Info category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/" class="nav-list-link">Info</a><ul class="nav-list"><li class="nav-list-item "><a href="/info/calendar/" class="nav-list-link">Syllabus / Calendar</a></li><li class="nav-list-item "><a href="/info/staff/" class="nav-list-link">Staff</a></li><li class="nav-list-item "><a href="/info/schedule/" class="nav-list-link">Weekly Schedule</a></li><li class="nav-list-item "><a href="/info/grading/" class="nav-list-link">Grading and Late Policy</a></li></ul></li><li class="nav-list-item"><a href="/intro/" class="nav-list-link">Introduction</a></li><li class="nav-list-item"><a href="/review/" class="nav-list-link">Background/Review</a></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Supervised Learning category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/supervised/" class="nav-list-link">Supervised Learning</a><ul class="nav-list"><li class="nav-list-item "><a href="/supervised/classification_foundation/" class="nav-list-link">Classification Fundamentals</a></li><li class="nav-list-item "><a href="/supervised/DiscriminativeGenerative/" class="nav-list-link">Discriminative vs Generative Classification</a></li><li class="nav-list-item "><a href="/supervised/learnability_and_vc/" class="nav-list-link">Learnability and VC Dimension</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Unsupervised Learning category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/unsupervised/" class="nav-list-link">Unsupervised Learning</a><ul class="nav-list"><li class="nav-list-item "><button class="nav-list-expander btn-reset" aria-label="toggle items in Graphical Models category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/unsupervised/graphical/" class="nav-list-link">Graphical Models</a><ul class="nav-list"></ul></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Reinforcement Learning category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/reinforcement/" class="nav-list-link">Reinforcement Learning</a><ul class="nav-list"><li class="nav-list-item "><a href="/reinforcement/mdp/" class="nav-list-link">Markov Decision Process</a></li><li class="nav-list-item "><a href="/reinforcement/value_bellman/" class="nav-list-link">Value functions and Bellman</a></li><li class="nav-list-item "><a href="/reinforcement/bandit/" class="nav-list-link">Bandits</a></li><li class="nav-list-item "><a href="/reinforcement/policy_gradient/" class="nav-list-link">Policy Gradient</a></li><li class="nav-list-item "><a href="/reinforcement/deepRL/" class="nav-list-link">Deep Reinforcement Learning</a></li></ul></li></ul> </nav> <footer class="site-footer"> <a href="/credit">Acknowledgement</a> </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search 6.790 Machine Learning" aria-label="Search 6.790 Machine Learning" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> <nav aria-label="Auxiliary" class="aux-nav"> <ul class="aux-nav-list"> <li class="aux-nav-list-item"> <a href="https://canvas.mit.edu/" class="site-button" target="_blank" rel="noopener noreferrer" > Canvas </a> </li> <li class="aux-nav-list-item"> <a href="https://piazza.com" class="site-button" target="_blank" rel="noopener noreferrer" > Piazza </a> </li> </ul> </nav> </div> <div id="main-content-wrap" class="main-content-wrap"> <div id="main-content" class="main-content"> <p class="warning"> The site is under construction. </p> <main> <h1 id="overview"> <a href="#overview" class="anchor-heading" aria-labelledby="overview"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> Overview </h1> <p>The aim of this lecture is to establish basic terminology and definitions useful for studying classification. We will discuss two basic classifiers: (i) Bayes; and (ii) Nearest Neighbors. The former is an abstract classifier used to understands the theoretical limits of classification, while the latter is a basic technique that one can use without having to use any specific “training algorithm”. We will see performance measures as well as some theoretical results regarding the classifiers introduced, along with some of the intuition behind when and why they work. Thereafter, we will motivate the idea of Empirical Risk Minimization (ERM), which is a leading paradigm for training classifiers in machine learning. We will discuss strengths and weaknesses of this framework, including the key ideas of “over-fitting” and “inductive bias”, as well as some standard trade-offs that one should be aware of when performing classification.</p> <h1 id="notations-setup"> <a href="#notations-setup" class="anchor-heading" aria-labelledby="notations-setup"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> Notations, Setup </h1> <p>We start with notations, setup that are we assume throughout the course for classification or more generally for supervised learning.</p> <ul> <li> <p><strong>Data domain</strong>: An arbitrary set $\mathcal{X}$ from which our training and test data are drawn. As often is the case, $\mathcal{X}=\mathbb{R}^{d}$. For instance, if we assume that the members of $\mathcal{X}$ are represented via <em>feature vectors</em>; we may write $\Phi(x)$ to emphasize the encoding of a data point $x \in \cal X$ as a feature vector in $\mathbb{R}^d$.</p> </li> <li> <p><strong>Label domain</strong>: A discrete set $\mathcal{Y}$; e.g., ${0,1}$ or ${-1,1}$. It is important to <em>not</em> interpret these $0$ and $1$ as “numbers,” but rather as “classes” or categorical variables. For the setting of regression, as we shall see, the label domain $\mathcal{Y}$ could be continuous, e.g. $\mathcal{Y} = [0,1]$ or $\mathbb{R}$.</p> </li> <li> <p><strong>Training data</strong>: A finite collection $S=\left{(x_{1}, y_{1}), \ldots, (x_{N}, y_{N})\right}$ of (data, label) pairs drawn from $\mathcal{X} \times \mathcal{Y}$.</p> </li> <li> <p><strong>Data distribution</strong>: A joint distribution $\mathbb{P}$ on $\mathcal{X} \times \mathcal{Y}$. An <em>important assumption</em> made throughout standard supervised machine learning is that while $\mathbb{P}$ is unknown, it is fixed.   We write $(X, Y)$ to denote a random variable with $X$ taking values in $\mathcal{X}$ and $Y$ taking values in $\mathcal{Y}$.</p> </li> </ul> <p><strong>Classifier</strong>. With these definitions, we are now ready to define a <em>classifier</em>; formally, it is simply a prediction rule \(h : \mathcal{X} \rightarrow \mathcal{Y},\) that is, a map from the data domain to the label domain. We will write $h_{S}$ to emphasize dependence of the classifier $h$ on training data. We will abuse the notation by denoting $h$ as a hypothesis, prediction rule, or classifier, but we do hope that the precise meaning will be clear from the context.</p> <p>Suppose we have a candidate classifier $h$. We need some way to measure its performance or simply to provide us with a mathematical guideline on “what does it mean to train?” Indeed, the goal of supervised learning is to use training data to help train a classifier that works well on unseen test data. To quantify what “works well”, we describe a key idea below.</p> <p><strong>Measuring success</strong>. We consider a quantity that measures the error of classifier. This quantity is called <strong>risk</strong>, which is also known as the <em>generalization error</em>: \(\label{eq:1} L(h) \equiv L_{\mathbb{P}}(h) :=\mathbb{P}(h(X) \neq Y).\) In words, the risk <a href="#eq:1" data-reference-type="eqref" data-reference="eq:1">[eq:1]</a> of a classifier $h$ is the probability of randomly choosing a pair $(X, Y) \sim \mathbb{P}$ for which $h(X) \neq Y$. The central goal of supervised learning is to learn a classifier $h$ using training data so that it has low risk—ideally, a classifier that is guaranteed to minimize <a href="#eq:1" data-reference-type="eqref" data-reference="eq:1">[eq:1]</a>.</p> <h1 id="bayes-classifier"> <a href="#bayes-classifier" class="anchor-heading" aria-labelledby="bayes-classifier"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> Bayes Classifier </h1> <p>Given the goal of task is to minimize the risk <a href="#eq:1" data-reference-type="eqref" data-reference="eq:1">[eq:1]</a> (i.e., the chance of being wrong on unseen data), at least in principle there is a simple strategy that can help attain this risk. Indeed, suppose we know the distribution $\mathbb{P}$ as per which data is generated, then intuitively it makes sense to pick the most likely class given the observation (notice, this intuition not limited to binary classification). This intuitive idea is exactly the idea behind the so-called Bayes classifier. Before describing the Bayes classifier formally, let us introduce some additional notation; we limit our description to binary classification for ease of exposition.</p> <p><strong>Class conditional distribution</strong>: Let $\mathcal{Y}={0,1}$. We define \(\label{eq:2} \eta(x) :=\mathbb{P}(Y=1 | X=x)=\mathbb{E}[Y | X=x],\) which describes the posterior probability of the data being in class $1$ given that you have observed $x$. The Bayes classifier $h^{*}(x)$ is defined by the rule: \(\label{eq:3} h^{*}(x) := \begin{cases} 1, &amp;\text{if}\ \eta(x)=\mathbb{P}(Y=1 | X=x)&gt;\frac{1}{2}\\ 0, &amp;\text{otherwise}. \end{cases}\) The classifier <a href="#eq:3" data-reference-type="eqref" data-reference="eq:3">[eq:3]</a> predicts label $1$ if the conditional probability of being in class $1$ is bigger than half. Remarkably, it can be shown that this simple classifier actually performs as good as any other classifier in terms of minimizing the risk, as established formally via the following theorem (stated formally for $\mathcal{X}=\mathbb{R}^d$ for simplicity).</p> <div id="thm:bayes" class="theorem"> **Theorem 1** (BC optimality). *For any classifier $h : \mathbb{R}^{d} \rightarrow\{0,1\}$, $L(h^*) \le L(h)$.* </div> <div class="proof"> *Proof.* Given $X=x$, the conditional error probability of any classifier $h$ may be written as: $$\begin{aligned} {\quad \mathbb{P}\left(h(X) \neq Y | X=x\right)} &amp;{=1-\mathbb{P}\left(Y=h(X) | X=x\right)} \\ &amp; {=1-\left(\mathbb{P}\left(Y=1, h(X)=1 | X=x\right)+\mathbb{P}\left(Y=0, h(X)=0 | X=x\right)\right)} \\ &amp; =1-\left( [\![h(x)=1]\!] \mathbb{P}\left(Y=1 | X=x\right)+[\![h(x)=0]\!] \mathbb{P}\left(Y=0 | X=x\right)\right)\\ &amp; =1- \left([\![h(x)=1]\!] \eta(x)+ [\![h(x)=0]\!] (1-\eta(x))\right) \end{aligned}$$ where $[\![\cdot]\!]$ is the Iverson bracket, i.e. $[\![z]\!] = 1$ if $z =$ ’true’ and $0$ if $z =$ ’false’. Thus, for every $x \in \mathbb{R}^{d}$, we have: $$\begin{aligned} &amp; \mathbb{P}\left(h(X) \neq Y | X=x\right) - \mathbb{P}\left(h^{*}(X) \neq Y | X=x\right) \\ &amp; \qquad = \eta(x)\left([\![h^{*}(x)=1]\!]-[\![h(x)=1]\!]\right) + \left(1-\eta(x)\right) \left([\![h^{*}(x)=0]\!]-[\![h(x)=0]\!]\right). \end{aligned}$$ Since $[\![h^{*}(x)=0]\!]=1-[\![h^{*}(x)=1]\!]$, the above equals to $\left(2 \eta(x)-1\right)\left([\![h^{*}(x)=1]\!]-[\![h(x)=1]\!]\right)$ which is non-negative based on the definition of $h^{*}$ ($\eta(x) &gt; 1/2 ~\Leftrightarrow ~[\![h^{*}(x)=1]\!]=1$). Thus we have $$\int \ \mathbb{P}(h(X) \neq Y | X=x) d \mathbb{P}(x) \geq \int \ \mathbb{P}(h^{*}(X) \neq Y | X=x) d \mathbb{P}(x).$$ or equivalently, $\mathbb{P}(h(X) \neq Y) \geq \mathbb{P}(h^{*}(X) \neq Y)$. ◻ </div> <p>Related to the manipulations of Theorem <a href="#thm:bayes" data-reference-type="ref" data-reference="thm:bayes">1</a> is a helpful exercise below:</p> <p><sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></p> <p>. Per Theorem <a href="#thm:bayes" data-reference-type="ref" data-reference="thm:bayes">1</a>, we have found the best possible classifier. But it is <em>idealized</em>.</p> <p><span style="color: cdarkred"><strong>Question:</strong></span> What makes the Bayes classifier idealized?<br /> <span style="color: cdarkred"><strong>Answer</strong></span>: The Bayes classifier assumes that we have access to $\mathbb{P}(X,Y)$, but we almost never have access to this joint distribution. So the importance of Bayes classifier is more conceptual: if we had complete power and knew the distribution of the data we would know how to construct the best possible classifier.</p> <p>Because we almost never have access to the true underlying joint distribution, let us now take a look at a fundamental approach to classification that <em>seems</em> distribution-free, namely, the method of nearest neighbors (NN).</p> <h1 id="nearest-neighbors"> <a href="#nearest-neighbors" class="anchor-heading" aria-labelledby="nearest-neighbors"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> Nearest Neighbors </h1> <p>This approach is akin to taking a view opposite to the Bayes classifier: whereas Bayes assumes full knowledge and takes advantage of the data distribution, Nearest neighbors (NN) ignores the underlying probability distribution. At a high level, the NN method is based on the belief that features that are used to describe the data are relevant to the labelings in a way that makes “close by” points likely to have the same label.</p> <p>NN is one of the simplest possible classifiers, where the training process is essentially to memorize the training data. Then during testing, for a given point $x$, it finds the $k$ points in training data nearest to $x$ and predicts a label by taking (weighted) majority label over these $k$ points. Precisely, given training data $S=\left{\left(x_{i}, y_{i}\right) | 1 \leq i \leq N\right}$, define \(\mathrm{NN}_{k}(x) :=\left\{j | 1 \leq j \leq k, x_{j}\text { is within } k \text { closest to } x\right\}.\) Notice that $\mathrm{NN}<em>{1}(x)=\operatorname{argmin}</em>{1 \leq i \leq N} \operatorname{dist}\left(x, x_{i}\right)$. Then, the $k$-NN classifier \(\begin{aligned} h_{k\text{-NN}}(x) &amp; = \frac{1}{k} \sum_{\ell \in \mathrm{NN}_{k}(x)} y_\ell. \end{aligned}\)</p> <h2 id="nn-and-bayes"> <a href="#nn-and-bayes" class="anchor-heading" aria-labelledby="nn-and-bayes"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> NN and Bayes </h2> <figure> <img src="./knn.png" style="width:30.0%" /> <figcaption><span class="math inline"><em>k</em></span>-NN can learn complex nonlinear classifiers (<span>Image Credit: Elements of Statistical Learning Theory)</span></figcaption> </figure> <p>Despite its simplicity, NN is capable of learning complex nonlinear classifiers. Precisely, the risk of NN \(\begin{aligned} L_{k\text{-NN}} &amp; = \mathbb{P}(h_{k\text{-NN}}(X) \neq Y). \end{aligned}\) NN classifier has excellent asymptotic performance even for $k=1$ as stated below (see for more details).</p> <div id="thm:nn" class="theorem"> **Theorem 2**. *Let $\mathcal{X} \subset \mathbb{R}^d, ~d\geq 1$. Let $\eta$ be continuous. Then, $$\begin{aligned} \lim_{n\to\infty} L_{1\text{-NN}} &amp; = 2 \mathbb{E}\big[\eta(X) (1-\eta(X))\big] ~\leq~2 \mathbb{E}\big[\min\{\eta(X), 1-\eta(X)\}] ~=~ 2 L^*. \end{aligned}$$* </div> <p>That is, with large number of data points, even $1$-NN algorithm has risk that is within factor $2$ of the optimal risk.</p> <div class="proof"> *Proof.* Given $X = x$, let $X'(n)$ the closest data point to $x$ amongst given $n$ observations. Then due to $\mathcal{X} \subset \mathbb{R}^d$ (i.e. complete, separable metric space), it can be argued that $X'(n) \to x$ as $n\to \infty$ with probability $1$. Further, $\eta$ is continuous. Therefore, $\eta(X'(n)) \to \eta(x)$ as $n\to \infty$ with probability $1$. Let $Y'(n)$ be the label observed associated $X'(n)$. Then, $$\begin{aligned} \mathbb{P}(h_{1\text{-NN}}(x) \neq Y | X = x) &amp; = \mathbb{P}(Y'(n) \neq Y | X = x) \\ &amp; = \mathbb{P}(Y'(n) = 1, Y = 0| X = x) + \mathbb{P}(Y'(n) = 0, Y = 1| X = x) \\ &amp; \stackrel{(a)}{=} \mathbb{P}(Y'(n) = 1 | X = x) \mathbb{P}( Y = 0 | X = x) + \mathbb{P}(Y'(n) = 0 | X = x) \mathbb{P}( Y = 1| X = x) \\ &amp; = \eta(X'(n)) (1-\eta(x)) + (1-\eta(X'(n)) \eta(x) \\ &amp; \to 2 \eta(x) (1-\eta(x)) \\ &amp; \stackrel{(b)}{=} 2 \min\{\eta(x), 1-\eta(x)\} \max\{\eta(x), 1-\eta(x)\} \\ &amp; \stackrel{(c)}{\leq} 2 \min\{\eta(x), 1-\eta(x)\}. \end{aligned}$$ In above, (a) follows from the fact that $Y'(n)$ and $Y$ are generated independently per our generative model; (b) from that the fact for $\alpha, \beta \in \mathbb{R}$ we have $\alpha \beta = \min\{\alpha, \beta\} \max\{\alpha, \beta\}$; and (c) from the fact that $\eta(x) \in [0,1]$ as it is probability. Then, the claim of theorem follows by recalling that the Bayes risk $L^* = \mathbb{E}[\min\{\eta(X), 1-\eta(X)\}]$. ◻ </div> <p>Theorem <a href="#thm:nn" data-reference-type="ref" data-reference="thm:nn">2</a> provides asymptotic guarantee for $1\text{-NN}$ algorithm. But in practice, we have only finite amount of data. A refined analysis (cf. see ) suggests that if $\mathcal{X} \subset \mathbb{R}^d$, $\eta$ is $\alpha \geq 1$-Holder continuous (see for precise definition, for example), then for any $\varepsilon \in (0,1)$, for $k\text{-NN}$ with $k = \Theta(\varepsilon^{-2})$ and $n = \Theta(\varepsilon^{-\frac{d}{\alpha} + 3} \log \varepsilon^{-1})$, the algorithm find approximation of $\eta$ that is within $\varepsilon$ error on average. This leads to good approximation of Bayes risk.</p> <p>The nearest neighbor approach is powerful in that it works for any (reasonable) setting. For that reason, it is called non-parametric method. However, it comes at the cost of not utilizing the potentially simpler, a priori known structure in the data. For that reason, it suffers from requiring large amount of data (there is no ‘free lunch’). However, if we have prior knowledge about the model class, it may make sense to incorporate it. And this is where the framework of Empirical Risk Minimization through incorporating “inductive bias” comes handy and we describe that next.</p> <h1 id="empirical-risk-minimization"> <a href="#empirical-risk-minimization" class="anchor-heading" aria-labelledby="empirical-risk-minimization"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> Empirical Risk Minimization </h1> <p>ERM makes the following trade-off: we accept that we do not know the full distribution (unlike the Bayes Classifier), but we do get to see the training data and we are not ignoring the training distribution (unlike NN classifiers). By not throwing away the training distribution, we can at least measure the error incurred by a classifier on the training data, aka the <em>training error</em> or <em>empirical risk</em>: \(L_{S}(h) :=\frac{1}{N} \big|\left\{i \in[N] | h\left(x_{i}\right) \neq y_{i}\right\} \big|.\) As the name suggests, the paradigm of Empirical Risk Minimization (ERM) seeks a predictor that minimizes $L_{S}(h)$. In other words, it uses $L_{S}(h)$ as a proxy for the true risk. Notice how this implies that while we do not throw away any “knowledge” gained from the training data, we are implicitly assuming the distribution of the training data is “representative” of the true underlying distribution.</p> <h2 id="over-fitting"> <a href="#over-fitting" class="anchor-heading" aria-labelledby="over-fitting"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> Over-fitting </h2> <p>Of course, greedily minimizing the empirical loss can lead to overfitting. If we get “unlucky” where the training data is not true example of the actual distribution, then over-fitting would lead to high error on unseen data, which is obviously problematic.</p> <p><span style="color: cdarkred"><strong>Food for thought</strong></span>: What is over-fitting? Essentially memorizing the training data. But is it necessarily bad? In itself not necessarily a bad idea; some argue that deep neural nets are essentially memorizing, yet they generalize well. More on this later today and also in Lecture 7.</p> <h2 id="inductive-bias"> <a href="#inductive-bias" class="anchor-heading" aria-labelledby="inductive-bias"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> Inductive bias </h2> <p>Since it is so easy to overfit when minimizing empirical risk, we search for settings where overfitting can be alleviated. One idea is to introduce the so-called “inductive bias,” which restricts the family of classifiers that we search over; this choice is made <em>in advance</em> before having seen any training data. Common examples of inductive-bias include linear models, neural networks, random forests, etc. The reason this is called a bias is because we are limiting ourselves to a “pre-determined” hypothesis class that we chose (i.e. our “bias” is present). And by doing so, it may not have any classifier that perfectly fits the data – that is, overfitting is avoided. Formally, let $\mathrm{ERM}_{\mathcal{H}}$ uses ERM to learn $h : \mathcal{X} \rightarrow \mathcal{Y}$ over $h \in {\cal H}$ by using the training data $S$: \(\operatorname{ERM}_{\mathcal{H}}(S) \in \underset{h \in \mathcal{H}}{\operatorname{argmin}} L_{S}(h).\) In other words, we are now minimizing a constrained empirical risk. Ideally, the choice of ${\mathcal{H}}$ should be governed by knowledge of data. But even “simple” choices of ${\mathcal{H}}$ can overfit if we are not careful. Of course, overly strong inductive bias can lead to underfitting. How to balance between the overfit and underfit is precisely the job of learning designers.</p> <h2 id="loss-function-erm-setup"> <a href="#loss-function-erm-setup" class="anchor-heading" aria-labelledby="loss-function-erm-setup"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> Loss function, ERM setup </h2> <p>Now let us write the ERM as a mathematical problem. Recall, risk is defined as \(L(h)=\mathbb{P}(h(X) \neq Y).\) Here we introduce a generic framework that encapsulates variety of “risks”. To that end, consider a loss function \(\ell : \mathcal{H} \times \mathcal{X} \times \mathcal{Y} \rightarrow \mathbb{R}_{+}.\) Then, the risk can then be written as the expected loss upon using $h \in \mathcal{H}$ with respect to the data distribution $\mathbb{P}$. Thus, \(\label{eq:4} L(h) :=\mathbb{E}[\ell(h, X, Y)].\) Similarly, the empirical risk is using $N$ data points is \(\label{eq:5} L_{S}(h) :=\frac{1}{N} \sum_{i=1}^{N} \ell\left(h, x_{i}, y_{i}\right).\) The classification risk we have considered thus far corresponds to 0/1-loss: \(\ell_{0 / 1}(h,x, y) :=\left\{\begin{array}{ll}{1,} &amp; {h(x) \neq y} \\ {0,} &amp; {h(x)=y},\end{array}\right.\) which incurs a loss of $1$ if the current data is mis-classified. When we shall discuss regression, we will introduce different (squared) loss.</p> <p>Under the $0/1$-loss, the task of ERM reduces to the computational question: \(\min _{h \in \mathcal{H}} \frac{1}{N} \sum_{i=1}^{N} \ell_{0 / 1}\left(h, x_{i}, y_{i}\right)=\frac{1}{N} \sum_{i=1}^{N}\left\llbracket h\left(x_{i}\right) \neq y_{i}\right\rrbracket.\label{eq:6}\) This empirical risk minimization is typically computationally hard. One way to get around the computational hardness is to utilize surrogate for 0/1-loss that leads to computational simplicity. For example, support vector machine that shall be covered in future lecture, the computationally feasible surrogate of 0/1-loss turns out to be the so called “hinge loss”.</p> <h2 id="erm-theory"> <a href="#erm-theory" class="anchor-heading" aria-labelledby="erm-theory"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> ERM: Theory </h2> <p>The ERM theory asks this question: <em>when does ERM work?</em> In other words, if we minimize the empirical risk $L_{S}(h)$, what bearing does that have on the population risk $L(h)$? The goal of learning theory is to study this (and such) question(s). Informally, if for all $h \in \mathcal{H}$, the empirical risk $L_{S}(h)$ is a good approximation to $L(h),$ then ERM will also return a good hypothesis within ${\cal H}$, and we may be able to establish a bound of the form \(\label{eq4} L_{\mathbb{P}}\left(\operatorname{ERM}_{\mathcal{H}}(S) \right) \leq \min _{h \in \mathcal{H}} L_{\mathbb{P}}(h) + \varepsilon(N, {\cal H}).\) where recall that $\operatorname{ERM}<em>{\mathcal{H}}(S)$ is the classifier learned using ERM; both risks are taken over the data distribution as well as randomly generated $S$ per the data distribution. If such is that case, then empirical risk of $\operatorname{ERM}</em>{\mathcal{H}}(S)$ will provide a good proxy of the best population risk achievable by ${\cal H}$. Naturally, larger ${\cal H}$ we have better the best population risk minimization is achieved over ${\cal H}$; so what stops us from choosing very large ${\cal H}$?</p> <h2 id="erm-bias-complexity-tradeoff"> <a href="#erm-bias-complexity-tradeoff" class="anchor-heading" aria-labelledby="erm-bias-complexity-tradeoff"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> ERM: Bias-complexity tradeoff </h2> <p>At the highest level, typically the error $\varepsilon(N, {\cal H})$ in <a href="#eq4" data-reference-type="eqref" data-reference="eq4">[eq4]</a> increases with ${\cal H}$ for a fixed $N$. And $\min <em>{h \in \mathcal{H}} L</em>{\mathbb{P}}(h)$ decreases as ${\cal H}$ increases. That is, if we increase “complexity” of ${\cal H}$, then the ‘bias’, represented as $\min <em>{h \in \mathcal{H}} L</em>{\mathbb{P}}(h)$ decreases. But ‘variance’, represented as $\varepsilon(N, {\cal H})$ in <a href="#eq4" data-reference-type="eqref" data-reference="eq4">[eq4]</a> increases. The ultimate goal is to achieve the right trade-off between these two for a given number of data points, $N$.</p> <figure id="classic"> <img src="./classic-tradeoff.png" style="width:40.0%" /> <figcaption>Classical training vs test curve. (Figure taken from: <span class="citation" data-cites="belkin2019reconciling"></span>.)</figcaption> </figure> <p>This bias-variance tradeoff or tension is captured by Figure <a href="#classic" data-reference-type="ref" data-reference="classic">1</a>. This is the classical view point. In the recent times, practitioners have found “rich” models e.g. over parametrized neural networks and empirically observed that once passed a certain complexity (of the model class $\mathcal{H}$), beyond the so-called interpolation threshold point, the data can be fit perfectly as well as the generalization continues to improve. This is shown by the double-descent curve in Figure <a href="#modern-tradeoff" data-reference-type="ref" data-reference="modern-tradeoff">2</a>. It is believed that this behavior is similar to that of non-parametric method like the nearest neighbor that generalizes well for all setting with enough observations even through the corresponding model class is extremely rich.</p> <figure id="modern-tradeoff"> <img src="./modern-tradeoff.png" style="width:55.0%" /> <figcaption>Modern training vs test curve. (Figure taken from: <span class="citation" data-cites="belkin2019reconciling"></span>.)</figcaption> </figure> <p><span style="color: cdarkred"><strong>Extra reading:</strong></span> all sections are from :</p> <ul> <li> <p>Definitions: §2.1,</p> </li> <li> <p>ERM and inductive bias: §2.2, §2.3,</p> </li> <li> <p>Loss function: §12.3, §3.2.2,</p> </li> <li> <p>Nearest neighbor §19.0, §19.1.</p> </li> </ul> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>These exercises are beyond those covered in recitation; they supplement the lecture material and notes, and it will be valuable for the reader to think about them carefully. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div> </main> <hr> <footer> <p><a href="#top" id="back-to-top">Back to top</a></p> <div class="d-flex mt-2"> <p class="text-small text-grey-dk-000 mb-0"> <a href="https://shenshen.mit.edu/git/shensquared/gradml/tree/main/classification_foundation.md" id="edit-this-page">Edit this page on Git</a> | This page was last updated on 18-Jun-2023 </p> </div> </footer> </div> </div> <div class="search-overlay"></div> </div> </body> </html>
