{"0": {
    "doc": "Discriminative vs Generative Classification",
    "title": "Overview",
    "content": "In this lecture, we will mainly discuss two different approaches to build classifiers, the generative approach and the discriminative approach. As concrete examples, we will look at the Naive Bayes classifier for the generative approach and compare it the logistic regression, as an example of discriminative approach. We will show under center conditions, the Naive Bayes is a linear classifier, just as Logistic Regression, but it assumes stronger assumptions and therefore is a more biased classifier. ",
    "url": "/supervised/DiscriminativeGenerative/#overview",
    
    "relUrl": "/supervised/DiscriminativeGenerative/#overview"
  },"1": {
    "doc": "Discriminative vs Generative Classification",
    "title": "Four Approaches to Build\nClassifiers",
    "content": " ",
    "url": "/supervised/DiscriminativeGenerative/#four-approaches-to-build-classifiers",
    
    "relUrl": "/supervised/DiscriminativeGenerative/#four-approaches-to-build-classifiers"
  },"2": {
    "doc": "Discriminative vs Generative Classification",
    "title": "Review: Bayes Classifier",
    "content": "We started our discussion of classification from the Bayes Classifier in Lecture 2. Recall the Bayes classifier h*(x) is defined by the rule: $$h^{*}(x) := \\begin{cases} 1, &amp;\\text{if}\\ \\eta(x)=\\mathbb{P}(Y=1 | X=x)&gt;\\frac{1}{2}\\\\ 0, &amp;\\text{otherwise}. \\end{cases}$$ The classifier predicts label 1 if the conditional probability of being in class 1 is bigger than half. We also showed this classifier is actually the optimal possible classifier, as the underlying distribution ‚Ñô is assumed known. ",
    "url": "/supervised/DiscriminativeGenerative/#review-bayes-classifier",
    
    "relUrl": "/supervised/DiscriminativeGenerative/#review-bayes-classifier"
  },"3": {
    "doc": "Discriminative vs Generative Classification",
    "title": "When Bayes Classifier is\nnot feasible",
    "content": "However, we know this idealized situation is seldom the case in reality, as we usually do not have access to ‚Ñô. Therefore, we introduced two different approaches, distance based classification (e.g. Nearest Neighbors) and Empirical Risk Minimization (e.g.¬†SVM). Here, we give a formal summary of four possible methods to learn a classifier. | Distance based method . | Empirical Risk Minimization . | Discriminative Approach: Fit a model ‚ÑôÃÇ(Y|X) to approximate the conditional distribution ‚Ñô(Y|X), add classify using: h(x)‚ÄÑ=‚ÄÑargmaxy‚ÑôÃÇ(Y=y|X=x) . | Generative Approach: Fit a model ‚ÑôÃÇ(X,Y) to approximate the joint distribution ‚Ñô(X,Y), add classify using: h(x)‚ÄÑ=‚ÄÑargmaxyP(X=x,Y=y)‚ÄÑ=‚ÄÑP(X=x|Y=y)P(Y=y) . | . ",
    "url": "/supervised/DiscriminativeGenerative/#when-bayes-classifier-is-not-feasible",
    
    "relUrl": "/supervised/DiscriminativeGenerative/#when-bayes-classifier-is-not-feasible"
  },"4": {
    "doc": "Discriminative vs Generative Classification",
    "title": "Discriminative\nvs.¬†Generative",
    "content": "The discriminative and the generative methods are two approaches to approximate the unknown underlying true distribution, and they are related by the Bayes rule P(X,Y)‚ÄÑ=‚ÄÑP(X|Y)P(Y)‚ÄÑ=‚ÄÑP(Y|X)P(X). Concretely, the generative approach learn what the individual classes looks like and models the data distribution P(X). It is a potentially harder problem and computationally more challenging. But the advantage of this approach is that it can be used to sample new data. On the other hand, the discriminative approach learn the boundary between classes and models P(Y|X), the conditional distribution and ignores P(X). It solves a potentially easier problem and computationally simpler. However, it cannot be used to sample new data. ",
    "url": "/supervised/DiscriminativeGenerative/#discriminative-vs.-generative",
    
    "relUrl": "/supervised/DiscriminativeGenerative/#discriminative-vs.-generative"
  },"5": {
    "doc": "Discriminative vs Generative Classification",
    "title": "Discrimitive\nClassifiers: Logistic Regression",
    "content": "Logistic Regression can be viewed as an approach of fitting a discriminative model. It assumes a parametric form of the conditional distribution P(Y|X) as $$P(Y=1|X=x;w) = \\frac{e^{w^Tx}}{1+e^{w^Tx}} = \\sigma (w^Tx)$$ where $\\sigma(z) = \\frac{1}{1+e^{-z}}$ and is often called the Sigmoid function. Therefore, $$P(Y=0|X=x;w) = 1- \\frac{e^{w^Tx}}{1+e^{w^Tx}} = 1- \\sigma (w^Tx)$$ In the training process, we are given a training data set S‚ÄÑ=‚ÄÑ{(x1,y1),‚ÄÜ...,‚ÄÜ(xN,yN)} to estimate the weights w. Naturally, we will apply the maximum likelihood method where we can write the likelihood of the training data, assuming i.i.d., as $$\\begin{aligned} P(S|w) &amp; = \\prod_{i=1}^N P(y_i|x_i;w) \\\\ &amp; = \\prod_{i=1}^N ( \\sigma (w^Tx_i))^{y_i} (1-\\sigma (w^Tx_i) ) ^{1-y_i} \\end{aligned}$$ Equivalent to maximize P(S|w), we can minimize the negative log-likelihood of S, which is $$\\begin{aligned} L(w) &amp;= - \\sum_{i=1}^N \\log [(\\sigma(w^Tx_i))^{y_i}(1-\\sigma(w^Tx_i))^{1-y_i}] \\\\ &amp; = - \\sum_{i=1}^N [y_i \\log \\sigma(w^Tx_i) + (1-y_i) \\log (1-\\sigma (w^Tx_i))] \\end{aligned}$$ This is often called the cross-entropy error. ",
    "url": "/supervised/DiscriminativeGenerative/#discrimitive-classifiers-logistic-regression",
    
    "relUrl": "/supervised/DiscriminativeGenerative/#discrimitive-classifiers-logistic-regression"
  },"6": {
    "doc": "Discriminative vs Generative Classification",
    "title": "Generative Classifiers:\nNaive Bayes",
    "content": "Now we take a closer look at the generative approach. As said before, in the generative approach, we fit a model of the joint distribution P(X,Y) and derived our classifier using the Bayes rule: $$P(Y|X) = \\frac{P(X|Y)P(Y)}{P(X)}$$ where P(X)‚ÄÑ=‚ÄÑ‚àëYP(X|Y)P(Y). For classification, we have $$\\begin{aligned} h(x) &amp; = argmax_y P(Y=y|X=x)\\\\ &amp; = argmax_y \\frac{P(X=x|Y=y)P(Y=y)}{P(X=x)} \\\\ &amp; = argmax_y P(X=x|Y=y)P(Y=y) \\end{aligned}$$ The denominator P(X=x) is only a normalization constant and thus can be ignored for deriving argmaxyP(Y=y|X=x). Now let‚Äôs suppose both X and Y are discrete random variables, where X‚ÄÑ‚àà‚ÄÑùí≥d and Y‚ÄÑ‚àà‚ÄÑùí¥. Then we have P(Y=yi|X=xi)‚ÄÑ‚àù‚ÄÑP(X=xk|Y=yi)P(Y=yi) Remember X is a d dimensional random variable, so to fully express this conditional distribution, we will approximately need |ùí≥|d|ùí¥|‚ÄÖ+‚ÄÖ|ùí¥| parameters. As a simple example, suppose ùí≥‚ÄÑ=‚ÄÑùí¥‚ÄÑ=‚ÄÑ{0,‚ÄÜ1}, then X‚ÄÑ‚àà‚ÄÑ{0,‚ÄÜ1}d. To specify P(X=xk|Y=0), we need 2d parameters. Unless d is sufficiently small, this full distribution is usually not computational tractable. ",
    "url": "/supervised/DiscriminativeGenerative/#generative-classifiers-naive-bayes",
    
    "relUrl": "/supervised/DiscriminativeGenerative/#generative-classifiers-naive-bayes"
  },"7": {
    "doc": "Discriminative vs Generative Classification",
    "title": "The Naive Bayes Assumption",
    "content": "To make the computation tractable and make the problem simpler, the Naive Bayes model make a strong assumption that the d features are conditional independent of each other given the class label Y. Therefore, $$\\begin{aligned} P(X|Y) &amp;= P(X_1,X_2,...,X_d|Y) \\\\ &amp;=P(X_1|X_2,...X_d|Y)P(X_2|X_3,...,X_d|Y)...P(X_d|Y) \\\\ &amp;=P(X_1|Y)P(X_2|Y)...P(X_d|Y) \\end{aligned}$$ where the second ‚Äò=‚Äô used the conditional independence. Therefore, our classifier becomes $$argmax_y P(Y=y|X_1,...,X_d) \\propto P(Y=y)\\prod_{j=1}^d P(X_j|Y=y)$$ . Now, using maximum likelihood on training data S, we can estimate the parameters for the Naive Bayes classifier, assuming a specific distribution for P(Xj|Y), 1‚ÄÑ‚â§‚ÄÑj‚ÄÑ‚â§‚ÄÑd. We can show for a multinomial distribution of P(Xj|Y), the parameters of Naive Bayes are $$\\begin{aligned} P(Y=y) &amp;= \\frac{\\# \\text{Example with }Y = y}{N} \\\\ P(X_i=x|Y=y) &amp;= \\frac{\\# \\text{Example with} X_i = x \\text{ and } Y=y}{\\# \\text{Example with }Y = y} \\end{aligned}$$ . ",
    "url": "/supervised/DiscriminativeGenerative/#the-naive-bayes-assumption",
    
    "relUrl": "/supervised/DiscriminativeGenerative/#the-naive-bayes-assumption"
  },"8": {
    "doc": "Discriminative vs Generative Classification",
    "title": "Naive Bayes can be a\nlinear classifier",
    "content": "We can show that under some common assumptions of P(Xj|Y), the Naive Bayes classifier is actually a linear classifier. Here we provide a proof for Xj‚ÄÑ‚àà‚ÄÑ0,‚ÄÜ1 and P(Xj|Y) is a Bernoulli distribution. (See Exercise 2 Problem 2 for other more general situations.) . Theorem 1 (). Let X‚ÄÑ‚àà‚ÄÑ{0,‚ÄÜ1}d, and P(Xj|Y),1‚ÄÑ‚â§‚ÄÑj‚ÄÑ‚â§‚ÄÑd is a Bernoulli distribution. The Naive Bayes classifier is defined by h(x)‚ÄÑ=‚ÄÑsgn(wTx+w0) for a suitable choice of w,w0. Proof. As Xj‚ÄÑ‚àà‚ÄÑ{0,‚ÄÜ1}, the Bernoulli distribution is therefore $$\\begin{aligned} P(X_j|Y=1) &amp; = a_j^{X_j} (1-a_j)^{(1-X_j)} \\\\ P(X_j|Y=0) &amp; = b_j^{X_j} (1-b_j)^{(1-X_j)} \\end{aligned}$$ where aj and bj are parameters for the jth dimension of X. With the conditional independence, we have $$\\begin{aligned} P(Y=1|X) &amp; =\\frac{P(X|Y=1)P(Y=1)}{P(X|Y=1)P(Y=1)+P(X|Y=0)P(Y=0)} \\\\ &amp; =\\frac{1}{1+\\frac{P(X|Y=0)P(Y=0)}{P(X|Y=1)P(Y=1)}} \\\\ &amp; = \\frac{1}{1+\\exp (- \\log \\frac{P(X|Y=1)P(Y=1)}{P(X|Y=0)P(Y=0)})} \\\\ &amp; =\\sigma \\left( \\sum_j^d \\log \\frac{P(X_j|Y=1)}{P(X_j|Y=0)} + \\log \\frac{P(Y=1)}{P(Y=0)} \\right) \\end{aligned}$$ and therefore, $$\\begin{aligned} P(Y=1|X) &amp; = \\sigma \\left( \\sum_j^d \\log \\frac{a_j^{X_j}(1-a_j)^{(1-X_j)}}{b_j^{X_j}(1-b_j)^{(1-X_j)}} + \\log \\frac{p}{1-p} \\right) \\\\ &amp; = \\sigma \\left( \\sum_j^d \\left(X_j \\log \\frac{a_j(1-b_j)}{b_j(1-a_j)}\\right) + \\log \\left(\\frac{p}{1-p} \\prod_j^d \\frac{1-a_j}{1-b_j} \\right) \\right) \\\\ &amp; = \\sigma \\left( \\sum_j^d w_j X_j + w_0\\right) \\end{aligned}$$ where $w_j = \\log \\frac{a_j(1-b_j)}{b_j(1-a_j)}$ and $w_0=\\log \\left(\\frac{p}{1-p} \\prod_j^d \\frac{1-a_j}{1-b_j} \\right)$. Therefore, h(x)‚ÄÑ=‚ÄÑsgn(wTx+w0) . and this shows the Naive Bayes is a linear classifier.¬†‚óª . ",
    "url": "/supervised/DiscriminativeGenerative/#naive-bayes-can-be-a-linear-classifier",
    
    "relUrl": "/supervised/DiscriminativeGenerative/#naive-bayes-can-be-a-linear-classifier"
  },"9": {
    "doc": "Discriminative vs Generative Classification",
    "title": "Naive Bayes vs.¬†Logistic\nRegression",
    "content": "We saw in the last section that Naive Bayes can be a linear classifier under some assumptions, but some of the assumptions are quite strong, such as the conditional independence. Thus, the hypothesis class of Naive Bayes is not all the possible linear classifiers, but only a subset of them. We know the Logistic Regression is another common linear classifier, how does Naive Bayes compare to Logistic Regression? In this section, we will look at them more closely through a theoretic view. ",
    "url": "/supervised/DiscriminativeGenerative/#naive-bayes-vs.-logistic-regression",
    
    "relUrl": "/supervised/DiscriminativeGenerative/#naive-bayes-vs.-logistic-regression"
  },"10": {
    "doc": "Discriminative vs Generative Classification",
    "title": "Asymptotic Regime",
    "content": "We first define certain notations for our discussion, let œµ(hA,N)‚ÄÑ‚â°‚ÄÑL(hA,S), where |S|‚ÄÑ=‚ÄÑN. Here, œµ(hA,N) stands for the error of hypothesis h trained using algorithm A from N observations. We will first discuss in the asymptotic setting, which means the number of training data points is infinity. If the two classes is linear separable, which means both models are correct, then we have œµ(hNB,‚àû)‚ÄÑ=‚ÄÑœµ(hLR,‚àû) which means, asymptotically, NB and LR produce the identical classifier. If the linear assumption does not hold, which is usually the case, we claim LR is expected to outperform NB, i.e. œµ(hLR,‚àû)‚ÄÑ‚â§‚ÄÑœµ(hNB,‚àû) Intuitively, this can be shown by observing that, since Logistic Regression assumes no other assumption other than linear classification, œµ(hLR,‚àû) converges to infh‚ÄÑ‚àà‚ÄÑ‚ÑãL(h), where ‚Ñã is the class of all linear classifiers, it must therefore be asymptotically no worse than the linear classifier picked by Naive Bayes, which assumes conditional independence between features. ",
    "url": "/supervised/DiscriminativeGenerative/#asymptotic-regime",
    
    "relUrl": "/supervised/DiscriminativeGenerative/#asymptotic-regime"
  },"11": {
    "doc": "Discriminative vs Generative Classification",
    "title": "Non-asymptotic Regime",
    "content": "In a more real setting, where we do not have infinite number of training data, we must talk about how fast (how many training sample needed) an estimator converges to its asymptotic limit. This ‚Äòrate of convergence‚Äô for Logistic Regression is as below . Theorem 2 (). Let hLR,‚ÄÜN be a Logistic Regression model in d dimensions. Then, with high probability, $$\\epsilon(h_{LR,N} \\leq \\epsilon(h_{LR},\\infty)) + O \\left( \\sqrt{\\frac{d}{N} \\log \\frac{N}{d}} \\right)$$ Thus, for œµ(hLR,N)‚ÄÑ‚â§‚ÄÑœµ(hLR,‚àû)‚ÄÖ+‚ÄÖœµ0 to hold up for a fixed constant œµ0, it suffices to pick N‚ÄÑ=‚ÄÑŒ©(d) . The proof of theorem 2 follows the application of the uniform convergence bounds to logistic regression, and using the fact that the d dimensional linear classifier ‚Ñã has a VC dimension of d‚ÄÖ+‚ÄÖ1. These concepts will be covered in Lecture 5. Now, we want to draw a similar picture for the Naive Bayes classifier and compare it the Logistic Regression. It turns out this is a more challenging task and we will break our analysis into two parts . | How fast do parameters of NB converge to their optimal values . | How fast do the risk if NB converge to the asymptotic risk . | . For the first part, formally, we have the following theroem . Theorem 3 (). *Let any œµ1, delta‚ÄÑ&gt;‚ÄÑ0 and any l‚ÄÑ‚â§‚ÄÑ0 be fixed. Assume that for some fixed œÅ0‚ÄÑ&gt;‚ÄÑ0, we hvae œÅ0‚ÄÑ‚â§‚ÄÑP(y=1)‚ÄÑ‚â§‚ÄÑ1‚ÄÖ‚àí‚ÄÖœÅ0. Let d‚ÄÑ=‚ÄÑO((1/œµ12)log(d/Œ¥)), then with probability at least 1‚ÄÖ‚àí‚ÄÖŒ¥: $$ and $$ for all j‚ÄÑ=‚ÄÑ1,‚ÄÜ...,‚ÄÜd and b‚ÄÑ‚àà‚ÄÑùí¥* . This theorem states that with a number of samples that is only logarithmic, rather than linear, in d, the parameters of Naive Bayes are uniformly close to their asymptotic values in hNB,‚ÄÜ‚àû. Proof of this theorem is a straightforward application of the Hoeffding bound, here we provide a simple setting where X‚ÄÑ‚àà‚ÄÑ{0,‚ÄÜ1} and P(X=1)‚ÄÑ=‚ÄÑp. Suppose we have N i.i.d. samples (x1,...,xN), then the maximum likelihood estimation for p is simply $$\\hat{p} = \\frac{1}{N}\\sum_i x_i$$ The Hoeffding bound for this case states, for all p‚ÄÑ‚àà‚ÄÑ[0,1], œµ‚ÄÑ&gt;‚ÄÑ0 P(|p‚àípÃÇ|&gt;œµ)‚ÄÑ‚â§‚ÄÑ2e‚àí2Nœµ2 Intuitively, this means that the probability of the empirical estimation being epsilon-far from the ground truth decays exponentially fast with the number of samples N. For a detailed proof of theorem 3, please look at paper [Ng]. Now we know the parameters of Naive Bayes converge logarithmically to its optimal values, but this doesn‚Äôt directly imply the error of Naive Bayes also converges with this rate. To intuitively show why the error also converges, we can first show that the convergence of the parameters implies that hNB,‚ÄÜN is very likely to make the same predictions as hNB,‚ÄÜ‚àû. Recall that hNB(x) makes its predictions according to $$\\ell_{NB}(x)=\\log \\frac{\\hat{P}(Y=1)\\prod_j \\hat{P}(x_j|Y=1)}{\\hat{P}(Y=0)\\prod_j \\hat{P}(x_j|Y=0)} &gt; 0$$ For every example for which both ‚ÑìNB(x) and ‚ÑìNB,‚ÄÜ‚àû(x) have the same sign, hNB,‚ÄÜN and hNB,‚ÄÜ‚àû will make the same prediction. If hNB,‚ÄÜ‚àû‚ÄÑ‚â´‚ÄÑ0 or hNB,‚ÄÜ‚àû‚ÄÑ‚â™‚ÄÑ0, as ‚ÑìNB(x) is only a small perturbation of ‚ÑìNB,‚ÄÜ‚àû(x), they will be on the same side of 0 with high probability. With some steps of derivation, we will eventually reach the formal risk convergence of Naive Bayes . Theorem 4 (). Define G(œÑ)‚ÄÑ=‚ÄÑP(x,y)‚ÄÑ‚àº‚ÄÑ‚Ñô[(‚ÑìNB,‚ÄÜ‚àû(x)‚àà[0,œÑd]‚àßy=1)‚à®(‚ÑìNB,‚ÄÜ‚àû(x)‚àà[‚àíœÑd,0]‚àßy=0)]. Assume that for some fixed œÅ0‚ÄÑ&gt;‚ÄÑ0, we have œÅ0‚ÄÑ‚â§‚ÄÑP(y=1)‚ÄÑ‚â§‚ÄÑ1‚ÄÖ‚àí‚ÄÖœÅ0), and that œÅ0‚ÄÑ‚â§‚ÄÑP(xj=1|y=b)‚ÄÑ‚â§‚ÄÑ1‚ÄÖ‚àí‚ÄÖœÅ0) for all j,b, then with high probability, $$\\epsilon(h_{NB,N}) \\leq \\epsilon(h_{NB, \\infty}) + G\\left(O\\left(\\sqrt{\\frac{1}{N}\\log d}\\right)\\right)$$ . Here, G defines the fraction of points that are very close to the decision boundary. Intuitively, if we can understand and control the event G(œÑ), then we can obtain a more precise control on the bound of the error. Theorem 5 (). Let the conditions of Theorem 4 hold, and suppose G(œÑ)‚ÄÑ‚â§‚ÄÑœµ/2‚ÄÖ+‚ÄÖF(œÑ), for some function F(œÑ) that statisfies F(œÑ)‚ÄÑ‚Üí‚ÄÑ0 as œÑ‚ÄÑ‚Üí‚ÄÑ0, and some fixed œµ0‚ÄÑ&gt;‚ÄÑ0. Then for œµ(hNB,‚ÄÜN)‚ÄÑ‚â§‚ÄÑœµ(hNB,‚ÄÜ‚àû)‚ÄÖ+‚ÄÖœµ0 to hold with high probability, it suffices to pick N‚ÄÑ=‚ÄÑŒ©(logd) . Thus, we can conclude that though the asymptotic error of Naive Bayes is greater than Logistic Regression, the convergence rate of NB is only Œ©(logd), which is faster than Œ©(d) of Logistic Regression. ",
    "url": "/supervised/DiscriminativeGenerative/#non-asymptotic-regime",
    
    "relUrl": "/supervised/DiscriminativeGenerative/#non-asymptotic-regime"
  },"12": {
    "doc": "Discriminative vs Generative Classification",
    "title": "Discriminative vs Generative Classification",
    "content": " ",
    "url": "/supervised/DiscriminativeGenerative/",
    
    "relUrl": "/supervised/DiscriminativeGenerative/"
  },"13": {
    "doc": "Bandits",
    "title": "Bandits",
    "content": " ",
    "url": "/reinforcement/bandit/",
    
    "relUrl": "/reinforcement/bandit/"
  },"14": {
    "doc": "Syllabus / Calendar",
    "title": "Calendar",
    "content": " ",
    "url": "/calendar/#calendar",
    
    "relUrl": "/calendar/#calendar"
  },"15": {
    "doc": "Syllabus / Calendar",
    "title": "\nWeek 1 - Introduction\n",
    "content": "Sep 1 Intro Sep 2 Recitation ",
    "url": "/calendar/#week-1-introduction",
    
    "relUrl": "/calendar/#week-1-introduction"
  },"16": {
    "doc": "Syllabus / Calendar",
    "title": "\nWeek 2 - Review and Foundations\n",
    "content": "Sep 1 Background and Review Sep 2 Recitation ",
    "url": "/calendar/#week-2-review-and-foundations",
    
    "relUrl": "/calendar/#week-2-review-and-foundations"
  },"17": {
    "doc": "Syllabus / Calendar",
    "title": "Syllabus / Calendar",
    "content": " ",
    "url": "/calendar/",
    
    "relUrl": "/calendar/"
  },"18": {
    "doc": "Classification Foundation",
    "title": "Overview",
    "content": "The aim of this lecture is to establish basic terminology and definitions useful for studying classification. We will discuss two basic classifiers: (i) Bayes; and (ii) Nearest Neighbors. The former is an abstract classifier used to understands the theoretical limits of classification, while the latter is a basic technique that one can use without having to use any specific ‚Äútraining algorithm‚Äù. We will see performance measures as well as some theoretical results regarding the classifiers introduced, along with some of the intuition behind when and why they work. Thereafter, we will motivate the idea of Empirical Risk Minimization (ERM), which is a leading paradigm for training classifiers in machine learning. We will discuss strengths and weaknesses of this framework, including the key ideas of ‚Äúover-fitting‚Äù and ‚Äúinductive bias‚Äù, as well as some standard trade-offs that one should be aware of when performing classification. ",
    "url": "/supervised/classification_foundation/#overview",
    
    "relUrl": "/supervised/classification_foundation/#overview"
  },"19": {
    "doc": "Classification Foundation",
    "title": "Notations, Setup",
    "content": "We start with notations, setup that are we assume throughout the course for classification or more generally for supervised learning. | Data domain: An arbitrary set ùí≥ from which our training and test data are drawn. As often is the case, ùí≥‚ÄÑ=‚ÄÑ‚Ñùd. For instance, if we assume that the members of ùí≥ are represented via feature vectors; we may write Œ¶(x) to emphasize the encoding of a data point $x \\in \\cal X$ as a feature vector in ‚Ñùd. | Label domain: A discrete set ùí¥; e.g., {0,‚ÄÜ1} or {‚ÄÖ‚àí‚ÄÖ1,‚ÄÜ1}. It is important to not interpret these 0 and 1 as ‚Äúnumbers,‚Äù but rather as ‚Äúclasses‚Äù or categorical variables. For the setting of regression, as we shall see, the label domain ùí¥ could be continuous, e.g.¬†ùí¥‚ÄÑ=‚ÄÑ[0,1] or ‚Ñù. | Training data: A finite collection S‚ÄÑ=‚ÄÑ{(x1,y1),‚Ä¶,(xN,yN)} of (data, label) pairs drawn from ùí≥‚ÄÖ√ó‚ÄÖùí¥. | Data distribution: A joint distribution ‚Ñô on ùí≥‚ÄÖ√ó‚ÄÖùí¥. An important assumption made throughout standard supervised machine learning is that while ‚Ñô is unknown, it is fixed. ¬† We write (X,Y) to denote a random variable with X taking values in ùí≥ and Y taking values in ùí¥. | . Classifier. With these definitions, we are now ready to define a classifier; formally, it is simply a prediction rule h‚ÄÑ:‚ÄÑùí≥‚ÄÑ‚Üí‚ÄÑùí¥, that is, a map from the data domain to the label domain. We will write hS to emphasize dependence of the classifier h on training data. We will abuse the notation by denoting h as a hypothesis, prediction rule, or classifier, but we do hope that the precise meaning will be clear from the context. Suppose we have a candidate classifier h. We need some way to measure its performance or simply to provide us with a mathematical guideline on ‚Äúwhat does it mean to train?‚Äù Indeed, the goal of supervised learning is to use training data to help train a classifier that works well on unseen test data. To quantify what ‚Äúworks well‚Äù, we describe a key idea below. Measuring success. We consider a quantity that measures the error of classifier. This quantity is called risk, which is also known as the generalization error: L(h)‚ÄÑ‚â°‚ÄÑL‚Ñô(h)‚ÄÑ:=‚ÄÑ‚Ñô(h(X)‚â†Y). In words, the risk¬†[eq:1] of a classifier h is the probability of randomly choosing a pair (X,Y)‚ÄÑ‚àº‚ÄÑ‚Ñô for which h(X)‚ÄÑ‚â†‚ÄÑY. The central goal of supervised learning is to learn a classifier h using training data so that it has low risk‚Äîideally, a classifier that is guaranteed to minimize¬†[eq:1]. ",
    "url": "/supervised/classification_foundation/#notations-setup",
    
    "relUrl": "/supervised/classification_foundation/#notations-setup"
  },"20": {
    "doc": "Classification Foundation",
    "title": "Bayes Classifier",
    "content": "Given the goal of task is to minimize the risk¬†[eq:1] (i.e., the chance of being wrong on unseen data), at least in principle there is a simple strategy that can help attain this risk. Indeed, suppose we know the distribution ‚Ñô as per which data is generated, then intuitively it makes sense to pick the most likely class given the observation (notice, this intuition not limited to binary classification). This intuitive idea is exactly the idea behind the so-called Bayes classifier. Before describing the Bayes classifier formally, let us introduce some additional notation; we limit our description to binary classification for ease of exposition. Class conditional distribution: Let ùí¥‚ÄÑ=‚ÄÑ{0,‚ÄÜ1}. We define Œ∑(x)‚ÄÑ:=‚ÄÑ‚Ñô(Y=1|X=x)‚ÄÑ=‚ÄÑùîº[Y|X=x], which describes the posterior probability of the data being in class 1 given that you have observed x. The Bayes classifier h*(x) is defined by the rule: $$\\label{eq:3} h^{*}(x) := \\begin{cases} 1, &amp;\\text{if}\\ \\eta(x)=\\mathbb{P}(Y=1 | X=x)&gt;\\frac{1}{2}\\\\ 0, &amp;\\text{otherwise}. \\end{cases}$$ The classifier [eq:3] predicts label 1 if the conditional probability of being in class 1 is bigger than half. Remarkably, it can be shown that this simple classifier actually performs as good as any other classifier in terms of minimizing the risk, as established formally via the following theorem (stated formally for ùí≥‚ÄÑ=‚ÄÑ‚Ñùd for simplicity). Theorem 1 (BC optimality). For any classifier h‚ÄÑ:‚ÄÑ‚Ñùd‚ÄÑ‚Üí‚ÄÑ{0,‚ÄÜ1}, L(h*)‚ÄÑ‚â§‚ÄÑL(h). Proof. Given X‚ÄÑ=‚ÄÑx, the conditional error probability of any classifier h may be written as: $$\\begin{aligned} {\\quad \\mathbb{P}\\left(h(X) \\neq Y | X=x\\right)} &amp;{=1-\\mathbb{P}\\left(Y=h(X) | X=x\\right)} \\\\ &amp; {=1-\\left(\\mathbb{P}\\left(Y=1, h(X)=1 | X=x\\right)+\\mathbb{P}\\left(Y=0, h(X)=0 | X=x\\right)\\right)} \\\\ &amp; =1-\\left( [\\![h(x)=1]\\!] \\mathbb{P}\\left(Y=1 | X=x\\right)+[\\![h(x)=0]\\!] \\mathbb{P}\\left(Y=0 | X=x\\right)\\right)\\\\ &amp; =1- \\left([\\![h(x)=1]\\!] \\eta(x)+ [\\![h(x)=0]\\!] (1-\\eta(x))\\right) \\end{aligned}$$ where [‚Ää[‚ãÖ]‚Ää] is the Iverson bracket, i.e. [‚Ää[z]‚Ää]‚ÄÑ=‚ÄÑ1 if z= ‚Äòtrue‚Äô and 0 if z= ‚Äòfalse‚Äô. Thus, for every x‚ÄÑ‚àà‚ÄÑ‚Ñùd, we have: $$\\begin{aligned} &amp; \\mathbb{P}\\left(h(X) \\neq Y | X=x\\right) - \\mathbb{P}\\left(h^{*}(X) \\neq Y | X=x\\right) \\\\ &amp; \\qquad = \\eta(x)\\left([\\![h^{*}(x)=1]\\!]-[\\![h(x)=1]\\!]\\right) + \\left(1-\\eta(x)\\right) \\left([\\![h^{*}(x)=0]\\!]-[\\![h(x)=0]\\!]\\right). \\end{aligned}$$ Since [‚Ää[h*(x)=0]‚Ää]‚ÄÑ=‚ÄÑ1‚ÄÖ‚àí‚ÄÖ[‚Ää[h*(x)=1]‚Ää], the above equals to (2Œ∑(x)‚àí1)([‚Ää[h*(x)=1]‚Ää]‚àí[‚Ää[h(x)=1]‚Ää]) which is non-negative based on the definition of h* (Œ∑(x)‚ÄÑ&gt;‚ÄÑ1/2¬†‚áî¬†[‚Ää[h*(x)=1]‚Ää]‚ÄÑ=‚ÄÑ1). Thus we have ‚à´¬†‚Ñô(h(X)‚â†Y|X=x)d‚Ñô(x)‚ÄÑ‚â•‚ÄÑ‚à´¬†‚Ñô(h*(X)‚â†Y|X=x)d‚Ñô(x). or equivalently, ‚Ñô(h(X)‚â†Y)‚ÄÑ‚â•‚ÄÑ‚Ñô(h*(X)‚â†Y).¬†‚óª . Related to the manipulations of Theorem¬†1 is a helpful exercise below: . 1 . Per Theorem¬†1, we have found the best possible classifier. But it is idealized. Question: What makes the Bayes classifier idealized? Answer: The Bayes classifier assumes that we have access to ‚Ñô(X,Y), but we almost never have access to this joint distribution. So the importance of Bayes classifier is more conceptual: if we had complete power and knew the distribution of the data we would know how to construct the best possible classifier. Because we almost never have access to the true underlying joint distribution, let us now take a look at a fundamental approach to classification that seems distribution-free, namely, the method of nearest neighbors (NN). ",
    "url": "/supervised/classification_foundation/#sub:bayes_classifier",
    
    "relUrl": "/supervised/classification_foundation/#sub:bayes_classifier"
  },"21": {
    "doc": "Classification Foundation",
    "title": "Nearest Neighbors",
    "content": "This approach is akin to taking a view opposite to the Bayes classifier: whereas Bayes assumes full knowledge and takes advantage of the data distribution, Nearest neighbors (NN) ignores the underlying probability distribution. At a high level, the NN method is based on the belief that features that are used to describe the data are relevant to the labelings in a way that makes ‚Äúclose by‚Äù points likely to have the same label. NN is one of the simplest possible classifiers, where the training process is essentially to memorize the training data. Then during testing, for a given point x, it finds the k points in training data nearest to x and predicts a label by taking (weighted) majority label over these k points. Precisely, given training data S‚ÄÑ=‚ÄÑ{(xi,yi)|1‚â§i‚â§N}, define NNk(x)‚ÄÑ:=‚ÄÑ{j|1‚â§j‚â§k,xj is within k closest to x}. Notice that NN1(x)‚ÄÑ=‚ÄÑargmin1‚ÄÑ‚â§‚ÄÑi‚ÄÑ‚â§‚ÄÑNdist‚ÄÜ(x,xi). Then, the k-NN classifier $$\\begin{aligned} h_{k\\text{-NN}}(x) &amp; = \\frac{1}{k} \\sum_{\\ell \\in \\mathrm{NN}_{k}(x)} y_\\ell. \\end{aligned}$$ . ",
    "url": "/supervised/classification_foundation/#nearest-neighbors",
    
    "relUrl": "/supervised/classification_foundation/#nearest-neighbors"
  },"22": {
    "doc": "Classification Foundation",
    "title": "NN and Bayes",
    "content": "k-NN can learn complex nonlinear classifiers (Image Credit: Elements of Statistical Learning Theory) Despite its simplicity, NN is capable of learning complex nonlinear classifiers. Precisely, the risk of NN $$\\begin{aligned} L_{k\\text{-NN}} &amp; = \\mathbb{P}(h_{k\\text{-NN}}(X) \\neq Y). \\end{aligned}$$ NN classifier has excellent asymptotic performance even for k‚ÄÑ=‚ÄÑ1 as stated below (see [@cover1967nearest] for more details). Theorem 2. Let ùí≥‚ÄÑ‚äÇ‚ÄÑ‚Ñùd,¬†d‚ÄÑ‚â•‚ÄÑ1. Let Œ∑ be continuous. Then, $$\\begin{aligned} \\lim_{n\\to\\infty} L_{1\\text{-NN}} &amp; = 2 \\mathbb{E}\\big[\\eta(X) (1-\\eta(X))\\big] ~\\leq~2 \\mathbb{E}\\big[\\min\\{\\eta(X), 1-\\eta(X)\\}] ~=~ 2 L^*. \\end{aligned}$$ . That is, with large number of data points, even 1-NN algorithm has risk that is within factor 2 of the optimal risk. Proof. Given X‚ÄÑ=‚ÄÑx, let X‚Ä≤(n) the closest data point to x amongst given n observations. Then due to ùí≥‚ÄÑ‚äÇ‚ÄÑ‚Ñùd (i.e.¬†complete, separable metric space), it can be argued that X‚Ä≤(n)‚ÄÑ‚Üí‚ÄÑx as n‚ÄÑ‚Üí‚ÄÑ‚àû with probability 1. Further, Œ∑ is continuous. Therefore, Œ∑(X‚Ä≤(n))‚ÄÑ‚Üí‚ÄÑŒ∑(x) as n‚ÄÑ‚Üí‚ÄÑ‚àû with probability 1. Let Y‚Ä≤(n) be the label observed associated X‚Ä≤(n). Then, $$\\begin{aligned} \\mathbb{P}(h_{1\\text{-NN}}(x) \\neq Y | X = x) &amp; = \\mathbb{P}(Y'(n) \\neq Y | X = x) \\\\ &amp; = \\mathbb{P}(Y'(n) = 1, Y = 0| X = x) + \\mathbb{P}(Y'(n) = 0, Y = 1| X = x) \\\\ &amp; \\stackrel{(a)}{=} \\mathbb{P}(Y'(n) = 1 | X = x) \\mathbb{P}( Y = 0 | X = x) + \\mathbb{P}(Y'(n) = 0 | X = x) \\mathbb{P}( Y = 1| X = x) \\\\ &amp; = \\eta(X'(n)) (1-\\eta(x)) + (1-\\eta(X'(n)) \\eta(x) \\\\ &amp; \\to 2 \\eta(x) (1-\\eta(x)) \\\\ &amp; \\stackrel{(b)}{=} 2 \\min\\{\\eta(x), 1-\\eta(x)\\} \\max\\{\\eta(x), 1-\\eta(x)\\} \\\\ &amp; \\stackrel{(c)}{\\leq} 2 \\min\\{\\eta(x), 1-\\eta(x)\\}. \\end{aligned}$$ In above, (a) follows from the fact that Y‚Ä≤(n) and Y are generated independently per our generative model; (b) from that the fact for Œ±,‚ÄÜŒ≤‚ÄÑ‚àà‚ÄÑ‚Ñù we have Œ±Œ≤‚ÄÑ=‚ÄÑmin‚ÄÜ{Œ±,‚ÄÜŒ≤}max‚ÄÜ{Œ±,‚ÄÜŒ≤}; and (c) from the fact that Œ∑(x)‚ÄÑ‚àà‚ÄÑ[0,1] as it is probability. Then, the claim of theorem follows by recalling that the Bayes risk L*‚ÄÑ=‚ÄÑùîº[min{Œ∑(X),1‚àíŒ∑(X)}].¬†‚óª . Theorem 2 provides asymptotic guarantee for 1-NN algorithm. But in practice, we have only finite amount of data. A refined analysis (cf.¬†see [@chen-shah Chapter 3]) suggests that if ùí≥‚ÄÑ‚äÇ‚ÄÑ‚Ñùd, Œ∑ is Œ±‚ÄÑ‚â•‚ÄÑ1-Holder continuous (see [@chen-shah] for precise definition, for example), then for any Œµ‚ÄÑ‚àà‚ÄÑ(0,1), for k-NN with k‚ÄÑ=‚ÄÑŒò(Œµ‚àí2) and $n = \\Theta(\\varepsilon^{-\\frac{d}{\\alpha} + 3} \\log \\varepsilon^{-1})$, the algorithm find approximation of Œ∑ that is within Œµ error on average. This leads to good approximation of Bayes risk. The nearest neighbor approach is powerful in that it works for any (reasonable) setting. For that reason, it is called non-parametric method. However, it comes at the cost of not utilizing the potentially simpler, a priori known structure in the data. For that reason, it suffers from requiring large amount of data (there is no ‚Äòfree lunch‚Äô). However, if we have prior knowledge about the model class, it may make sense to incorporate it. And this is where the framework of Empirical Risk Minimization through incorporating ‚Äúinductive bias‚Äù comes handy and we describe that next. ",
    "url": "/supervised/classification_foundation/#nn-and-bayes",
    
    "relUrl": "/supervised/classification_foundation/#nn-and-bayes"
  },"23": {
    "doc": "Classification Foundation",
    "title": "Empirical Risk Minimization",
    "content": "ERM makes the following trade-off: we accept that we do not know the full distribution (unlike the Bayes Classifier), but we do get to see the training data and we are not ignoring the training distribution (unlike NN classifiers). By not throwing away the training distribution, we can at least measure the error incurred by a classifier on the training data, aka the training error or empirical risk: $$L_{S}(h) :=\\frac{1}{N} \\big|\\left\\{i \\in[N] | h\\left(x_{i}\\right) \\neq y_{i}\\right\\} \\big|.$$ As the name suggests, the paradigm of Empirical Risk Minimization (ERM) seeks a predictor that minimizes LS(h). In other words, it uses LS(h) as a proxy for the true risk. Notice how this implies that while we do not throw away any ‚Äúknowledge‚Äù gained from the training data, we are implicitly assuming the distribution of the training data is ‚Äúrepresentative‚Äù of the true underlying distribution. ",
    "url": "/supervised/classification_foundation/#empirical-risk-minimization",
    
    "relUrl": "/supervised/classification_foundation/#empirical-risk-minimization"
  },"24": {
    "doc": "Classification Foundation",
    "title": "Over-fitting",
    "content": "Of course, greedily minimizing the empirical loss can lead to overfitting. If we get ‚Äúunlucky‚Äù where the training data is not true example of the actual distribution, then over-fitting would lead to high error on unseen data, which is obviously problematic. Food for thought: What is over-fitting? Essentially memorizing the training data. But is it necessarily bad? In itself not necessarily a bad idea; some argue that deep neural nets are essentially memorizing, yet they generalize well. More on this later today and also in Lecture 7. ",
    "url": "/supervised/classification_foundation/#sub:over_fitting",
    
    "relUrl": "/supervised/classification_foundation/#sub:over_fitting"
  },"25": {
    "doc": "Classification Foundation",
    "title": "Inductive bias",
    "content": "Since it is so easy to overfit when minimizing empirical risk, we search for settings where overfitting can be alleviated. One idea is to introduce the so-called ‚Äúinductive bias,‚Äù which restricts the family of classifiers that we search over; this choice is made in advance before having seen any training data. Common examples of inductive-bias include linear models, neural networks, random forests, etc. The reason this is called a bias is because we are limiting ourselves to a ‚Äúpre-determined‚Äù hypothesis class that we chose (i.e.¬†our ‚Äúbias‚Äù is present). And by doing so, it may not have any classifier that perfectly fits the data ‚Äì that is, overfitting is avoided. Formally, let ERM‚Ñã uses ERM to learn h‚ÄÑ:‚ÄÑùí≥‚ÄÑ‚Üí‚ÄÑùí¥ over $h \\in {\\cal H}$ by using the training data S: $$\\operatorname{ERM}_{\\mathcal{H}}(S) \\in \\underset{h \\in \\mathcal{H}}{\\operatorname{argmin}} L_{S}(h).$$ In other words, we are now minimizing a constrained empirical risk. Ideally, the choice of ‚Ñã should be governed by knowledge of data. But even ‚Äúsimple‚Äù choices of ‚Ñã can overfit if we are not careful. Of course, overly strong inductive bias can lead to underfitting. How to balance between the overfit and underfit is precisely the job of learning designers. ",
    "url": "/supervised/classification_foundation/#sub:inductive_bias",
    
    "relUrl": "/supervised/classification_foundation/#sub:inductive_bias"
  },"26": {
    "doc": "Classification Foundation",
    "title": "Loss function, ERM setup",
    "content": "Now let us write the ERM as a mathematical problem. Recall, risk is defined as L(h)‚ÄÑ=‚ÄÑ‚Ñô(h(X)‚â†Y). Here we introduce a generic framework that encapsulates variety of ‚Äúrisks‚Äù. To that end, consider a loss function ‚Ñì‚ÄÑ:‚ÄÑ‚Ñã‚ÄÖ√ó‚ÄÖùí≥‚ÄÖ√ó‚ÄÖùí¥‚ÄÑ‚Üí‚ÄÑ‚Ñù+. Then, the risk can then be written as the expected loss upon using h‚ÄÑ‚àà‚ÄÑ‚Ñã with respect to the data distribution ‚Ñô. Thus, L(h)‚ÄÑ:=‚ÄÑùîº[‚Ñì(h,X,Y)]. Similarly, the empirical risk is using N data points is $$\\label{eq:5} L_{S}(h) :=\\frac{1}{N} \\sum_{i=1}^{N} \\ell\\left(h, x_{i}, y_{i}\\right).$$ The classification risk we have considered thus far corresponds to 0/1-loss: $$\\ell_{0 / 1}(h,x, y) :=\\left\\{\\begin{array}{ll}{1,} &amp; {h(x) \\neq y} \\\\ {0,} &amp; {h(x)=y},\\end{array}\\right.$$ which incurs a loss of 1 if the current data is mis-classified. When we shall discuss regression, we will introduce different (squared) loss. Under the 0/1-loss, the task of ERM reduces to the computational question: $$\\min _{h \\in \\mathcal{H}} \\frac{1}{N} \\sum_{i=1}^{N} \\ell_{0 / 1}\\left(h, x_{i}, y_{i}\\right)=\\frac{1}{N} \\sum_{i=1}^{N}\\left\\llbracket h\\left(x_{i}\\right) \\neq y_{i}\\right\\rrbracket.\\label{eq:6}$$ This empirical risk minimization is typically computationally hard. One way to get around the computational hardness is to utilize surrogate for 0/1-loss that leads to computational simplicity. For example, support vector machine that shall be covered in future lecture, the computationally feasible surrogate of 0/1-loss turns out to be the so called ‚Äúhinge loss‚Äù. ",
    "url": "/supervised/classification_foundation/#loss-function-erm-setup",
    
    "relUrl": "/supervised/classification_foundation/#loss-function-erm-setup"
  },"27": {
    "doc": "Classification Foundation",
    "title": "ERM: Theory",
    "content": "The ERM theory asks this question: when does ERM work? In other words, if we minimize the empirical risk LS(h), what bearing does that have on the population risk L(h)? The goal of learning theory is to study this (and such) question(s). Informally, if for all h‚ÄÑ‚àà‚ÄÑ‚Ñã, the empirical risk LS(h) is a good approximation to L(h), then ERM will also return a good hypothesis within ${\\cal H}$, and we may be able to establish a bound of the form $$\\label{eq4} L_{\\mathbb{P}}\\left(\\operatorname{ERM}_{\\mathcal{H}}(S) \\right) \\leq \\min _{h \\in \\mathcal{H}} L_{\\mathbb{P}}(h) + \\varepsilon(N, {\\cal H}).$$ where recall that ERM‚Ñã(S) is the classifier learned using ERM; both risks are taken over the data distribution as well as randomly generated S per the data distribution. If such is that case, then empirical risk of ERM‚Ñã(S) will provide a good proxy of the best population risk achievable by ${\\cal H}$. Naturally, larger ${\\cal H}$ we have better the best population risk minimization is achieved over ${\\cal H}$; so what stops us from choosing very large ${\\cal H}$? . ",
    "url": "/supervised/classification_foundation/#erm-theory",
    
    "relUrl": "/supervised/classification_foundation/#erm-theory"
  },"28": {
    "doc": "Classification Foundation",
    "title": "ERM: Bias-complexity tradeoff",
    "content": "At the highest level, typically the error $\\varepsilon(N, {\\cal H})$ in [eq4] increases with ${\\cal H}$ for a fixed N. And minh‚ÄÑ‚àà‚ÄÑ‚ÑãL‚Ñô(h) decreases as ${\\cal H}$ increases. That is, if we increase ‚Äúcomplexity‚Äù of ${\\cal H}$, then the ‚Äòbias‚Äô, represented as minh‚ÄÑ‚àà‚ÄÑ‚ÑãL‚Ñô(h) decreases. But ‚Äòvariance‚Äô, represented as $\\varepsilon(N, {\\cal H})$ in [eq4] increases. The ultimate goal is to achieve the right trade-off between these two for a given number of data points, N. Classical training vs test curve. (Figure taken from: [@belkin2019reconciling].) This bias-variance tradeoff or tension is captured by Figure 1. This is the classical view point. In the recent times, practitioners have found ‚Äúrich‚Äù models e.g.¬†over parametrized neural networks and empirically observed that once passed a certain complexity (of the model class ‚Ñã), beyond the so-called interpolation threshold point, the data can be fit perfectly as well as the generalization continues to improve. This is shown by the double-descent curve in Figure 2. It is believed that this behavior is similar to that of non-parametric method like the nearest neighbor that generalizes well for all setting with enough observations even through the corresponding model class is extremely rich. Modern training vs test curve. (Figure taken from: [@belkin2019reconciling].) Extra reading: all sections are from [@sss]: . | Definitions: ¬ß2.1, . | ERM and inductive bias: ¬ß2.2, ¬ß2.3, . | Loss function: ¬ß12.3, ¬ß3.2.2, . | Nearest neighbor ¬ß19.0, ¬ß19.1. | . | These exercises are beyond those covered in recitation; they supplement the lecture material and notes, and it will be valuable for the reader to think about them carefully.‚Ü©Ô∏é . | . ",
    "url": "/supervised/classification_foundation/#erm-bias-complexity-tradeoff",
    
    "relUrl": "/supervised/classification_foundation/#erm-bias-complexity-tradeoff"
  },"29": {
    "doc": "Classification Foundation",
    "title": "Classification Foundation",
    "content": " ",
    "url": "/supervised/classification_foundation/",
    
    "relUrl": "/supervised/classification_foundation/"
  },"30": {
    "doc": "Credit",
    "title": "Credit",
    "content": "Special thanks to: . | Leslie Kaelbling | 6.867 Fall 2019 teaching staff | . This site uses: . | Pandoc | Hypothesis | Jekyll | Just the Docs | Just the Class | Last Modified At | . ",
    "url": "/credit/",
    
    "relUrl": "/credit/"
  },"31": {
    "doc": "Deep Reinforcement Learning",
    "title": "Deep Reinforcement Learning",
    "content": " ",
    "url": "/reinforcement/deepRL/",
    
    "relUrl": "/reinforcement/deepRL/"
  },"32": {
    "doc": "Graphical Models",
    "title": "Graphical Models",
    "content": " ",
    "url": "/unsupervised/graphical/",
    
    "relUrl": "/unsupervised/graphical/"
  },"33": {
    "doc": "About",
    "title": "The site is under\nconstruction",
    "content": " ",
    "url": "/#the-site-is-under-construction",
    
    "relUrl": "/#the-site-is-under-construction"
  },"34": {
    "doc": "About",
    "title": "Course Overview",
    "content": ". | Graduate level; offered regularly in the fall | 12 units (3-0-9) | Prerequisites: . | Linear algebra (at the level of 18.06) | and Probability (at the level of 6.3700, 6.3800, or 18.600) | . | Brief description: Principles, techniques, and algorithms in machine learning from the point of view of statistical inference; representation, generalization, and model selection; and methods such as linear/additive models, active learning, boosting, support vector machines, non-parametric Bayesian methods, hidden Markov models, Bayesian networks, and convolutional and recurrent neural networks. Recommended prerequisite: 6.3900 or other previous experience in machine learning. Enrollment may be limited. | . ",
    "url": "/#course-overview",
    
    "relUrl": "/#course-overview"
  },"35": {
    "doc": "About",
    "title": "Staff",
    "content": ". | Instructors 6.7900-instructors@mit.edu . | Pulkit Agrawal | Tommi Jaakkola | Shen Shen | . | TAs . | Eric Zhang | Rupert Li | Abhijatmedhi ‚ÄòEarth‚Äô Chotrattanapituk | Amit Schechter | Nishant Abhangi | Elizaveta Tremsina | Frederick ‚ÄòFreddie‚Äô Zhao | . | . ",
    "url": "/#staff",
    
    "relUrl": "/#staff"
  },"36": {
    "doc": "About",
    "title": "Weekly Schedule",
    "content": ". | Lectures: Tuesday and Thursday, 2:30pm-4pm, in 32-123 | Recitations (four options to choose one from): . | Friday, 10am, in 4-237 | Friday, 11am, in 4-237 | Friday, 1pm, in 3-270 | Friday, 2pm, in 3-270 | . | Office hours: . | Instructor office hours: TBD | TA office hours: TBD | . | . ",
    "url": "/#weekly-schedule",
    
    "relUrl": "/#weekly-schedule"
  },"37": {
    "doc": "About",
    "title": "Grading",
    "content": ". | Homework | Exams | . ",
    "url": "/#grading",
    
    "relUrl": "/#grading"
  },"38": {
    "doc": "About",
    "title": "Recommended Reading",
    "content": "All freely accessible with an MIT IP: . | [B] Pattern Recognition and Machine Learning, Bishop; Springer, 2007. | [HTF] The Elements of Statistical Learning, Hastie, Tibshirani, Friedman, 2009. | [SB]/[SSS] Understanding Machine Learning: From Theory to Algorithms, Shalev-Shwartz and Ben-David, 2014. | [SB] Reinforcement Learning: An Introduction, Sutton and Barton, 2018. | [JWHT] An Introduction to Statistical Learning, James, Witten, Hastie, Tibshirani; Springer, 2013. | [EH] Computer Age Statistical Inference, Efron and Hastie; Cambridge University Press, 2016. | . ",
    "url": "/#recommended-reading",
    
    "relUrl": "/#recommended-reading"
  },"39": {
    "doc": "About",
    "title": "About",
    "content": " ",
    "url": "/",
    
    "relUrl": "/"
  },"40": {
    "doc": "Introduction",
    "title": "Background",
    "content": "The term ‚ÄúMachine Learning‚Äù was coined by MIT alumnus Arthur Samuel1 in 1959. It evolved from many fields including Statistical Learning, Pattern Recognition and so on. The goal of machine learning is to make computers ‚Äúlearn‚Äù from ‚Äúdata‚Äù2. From an end user‚Äôs perspective, it is about understanding your data, make predictions and decisions. Intellectually, it is a collection of models, methods and algorithms that have evolved over more than a half-century now. Historically both disciplines evolved from different perspectives, but with similar end goals. For example, Machine Learning focused on ‚Äúprediction‚Äù and ‚Äúdecisions‚Äù. It relied on ‚Äúpatterns‚Äù or ‚Äúmodel‚Äù learnt in the process to achieve it. Computation has played key role in its evolution. In contrast, Statistics, founded by statisticians such as Pearson and Fisher, focused on ‚Äúmodel learning‚Äù. To understand and explain ‚Äúwhy‚Äù behind a phenomenon. Probability has played key role in development of the field. As a concrete example, recall the ideal gas law PV‚ÄÑ=‚ÄÑnRT for Physics. Historically, machine learning only cared about ability to predict P by knowing V and T, did not matter how; on the other hand, Statistics did care about the precise form of the relationship between P,‚ÄÜV and T, in particular it being linear. Having said that, in current day and age, both disciplines are getting closer and closer, day-by-day, and this class is such an amalgamation. Artificial Intelligence‚Äôs stated goal is to mimic human behavior in an intelligent manner, and to do what humans can do but really well, which includes artificial ‚Äúcreativity‚Äù and driving cars, playing games, responding to consumer questions, etc. Traditionally, the main tools to achieve these goals are ‚Äúrules‚Äù and ‚Äúdecision trees‚Äù. In that sense, Artificial intelligence seeks to create muscle and mind of humans, and mind requires learning from data, i.e.¬†Machine Learning. However, Machine Learning helps learn from data beyond mimicking humans. Having said that, again the boundaries between AI and ML are getting blurry day-by-day. ",
    "url": "/intro/#background",
    
    "relUrl": "/intro/#background"
  },"41": {
    "doc": "Introduction",
    "title": "Course Structure",
    "content": "The course contains four parts: . | Part I. Supervised Learning. Learning from data to predict. | Part II. Unsupervised Learning. Understanding the structure within the data. | Part III. Probabilistic Modeling. Probabilistic view to model complex scenarios. | Part IV. Decision Making. Using data to make decisions. | . ",
    "url": "/intro/#course-structure",
    
    "relUrl": "/intro/#course-structure"
  },"42": {
    "doc": "Introduction",
    "title": "Supervised Learning",
    "content": "The goal of supervised learning is to predict target using input / features, and a model is learned to do so. This can be sufficiently summarized as $$\\text{\\textit{target}} = f ( \\text{\\textit{features}} )$$ For classification tasks, the target is categorical or takes discrete values (e.g.¬†hot or cold). For regression tasks, the target takes any real value (e.g.¬†temperature). The model type reflects our belief about the reality and different model leads to different algorithm. The philosophy of supervised learning is: future of the past equals future of the future. Examples of classification include: identify handwritten digits, email spam filtering, detecting malicious network connection based on network log information or predicting whether a client will default on her/his credit based on the client‚Äôs features. For example, suppose we have access to a client‚Äôs features or attributes in terms of the (credit card) balance and income. Consider Figure 1. It plots available data with X axis representing (credit card) balance and Y axis representing income. The color of the point is blue if no default and brown if default. Pictorially, the classifier is trying to learn a boundary as shown in Figure 1 which separate no default from default. Classification of default and no default Formally, the data are labeled observations of the following form: (x1,y1),‚ÄÜ‚Ä¶,‚ÄÜ(xN,yN). The goal is to learn a model that maps attribute (or feature) x to label (or target) y so that given attribute x, we can predict corresponding unknown (discrete) label y. That is, to learn a function f such that y‚ÄÑ=‚ÄÑf(x) (and sometimes also what‚Äôs the confidence). Various approaches for learning f can be categorized as . | Linear: Logistic regression, Support Vector Machine (SVM), Linear Discriminant Analysis (LDA), Perceptron . | Non-linear (parametric): Quadratic Discriminant Analysis (QDA), Polynomial, Neural Networks . | Non-parametric: Kernels, Nearest Neighbors . | . How to find f? Among all possible choices of f, choose the one that fits the data the best. That is, solve optimization: *empirical risk minimization (ERM): $$\\text{Minimize } \\sum_{i=1}^N \\mathrm{loss}\\left(y_i,f(x_i)\\right) \\text{ over all possible } f.$$ Stochastic Gradient Descent (SGD) is a method to solve this optimization problem. This is where Optimization meets Machine Learning. 6.390 (or equivalent undergraduate class) discusses the ‚ÄúHow‚Äù or ‚Äúmechanics‚Äù of such approaches. In this class, we expect that you know the ‚ÄúHow‚Äù for much of supervised learning and decision making. That is, more than 60% of this class. So, what will we do in 6.790 (since ‚ÄÑ&gt;‚ÄÑ60% is already done!)? . To start with, we will learn ‚ÄúWhy‚Äù behind the ‚ÄúHow‚Äù. We will utilize Probability as our formal language. We will discuss estimators and theoretical guarantees, and generalization: does a good model fit on historical data lead to ability to predict future? Finally, we will have 40% of the course discusses unsupervised learning / probabilistic modeling to understand the structure within the data. To understand ‚ÄúWhy‚Äù, effectively we need to ‚Äúlogically deduce‚Äù what we do starting with appropriate goals and axioms. The axioms that are relevant are that of Probability. In particular, to reason about what we do in Machine Learning, we will utilize the language of probability. And probability is entirely based on the three key axioms. Formally, there is a probability space Œ©, events ‚Ñ± in it, and a probability function ‚Ñô‚ÄÑ:‚ÄÑ‚Ñ±‚ÄÑ‚Üí‚ÄÑ[0,1]. | Axiom 1. ‚Ñô(A)‚ÄÑ‚â•‚ÄÑ0, for all A‚ÄÑ‚àà‚ÄÑ‚Ñ±. | Axiom 2. ‚Ñô(Œ©)‚ÄÑ=‚ÄÑ1. | Axiom 3. $\\mathbb{P}\\left(\\cup_{i=1}^{\\infty} E_i\\right)=\\sum_{i=1}^{\\infty}\\mathbb{P}\\left( E_i\\right)$ . | . The above exercise is a simple example of logical deduction starting from the axioms of probability. In a sense, this is what we will do to explain ‚Äúwhy‚Äù. Before proceeding further, it is important to wonder ‚Äì ‚ÄúIs it possible to have a different set of probability axioms?‚Äù This is a question hotly debated in the first half of last century. At the end of the day, All roads lead to Rome: All sorts of reasonable hypothesis about beliefs / decision making lead to axioms of probability3. In the language of probability, both attributes X and labels Y are random variables. Especially, Y is discrete-valued random variable. The conditional distribution ‚Ñô(Y|X) is of interest. Suppose labels take value 1 (e.g.¬†default) or ‚ÄÖ‚àí‚ÄÖ1 (e.g.¬†no default), given attribute X‚ÄÑ=‚ÄÑx. An ideal classier, also known as Bayes classifier, which in the context of binary classification, predicts $$ \\begin{equation} \\hat{Y}(x)= \\begin{cases}1 &amp; \\text { if } \\mathbb{P}(Y=1 \\mid X=x) \\geq 1 / 2 \\\\ 0 &amp; \\text { otherwise }\\end{cases} \\end{equation} $$ . The performance metric of interest is mis-classification probability, i.e.¬†‚Ñô(YÃÇ(X)‚â†Y). Probabilistic view will help us understand how to choose the loss function and how well our model generalizes. In terms of generalization and overfitting, you should trust your data, but only so much. Consider the following example: We have observations (xi,yi), i‚ÄÑ=‚ÄÑ1,‚ÄÜ‚Ä¶,‚ÄÜn. Here attributes xi are points distributed uniformly in the unit square. The label is generated according to the following rule: As sketched in the figure below, yi‚ÄÑ=‚ÄÑ0 when the corresponding xi lies in the shaded square and yi‚ÄÑ=‚ÄÑ1 otherwise. The area of the shaded square is 1/2. image Pretend we do not know the true label rule and would like to to find a model to approximate it based on the observations. The function fit, $$f({x})=\\begin{cases} y_i, &amp; \\text{if }{x}={x}_i, \\\\ 0, &amp; \\text{otherwise}, \\end{cases}$$ which assigns every observed points to the correct label yi and assign all unseen points to 0, is a perfect fit for the observation. However, since the possibility we encounter the same points in the set {(xi,yi),‚ÄÜi‚ÄÑ=‚ÄÑ1,‚ÄÜ‚Ä¶,‚ÄÜn} in the future is zero, we will most certainly assign all future points to 0 and this function is simply as bad as ‚Äúrandom‚Äù function! This is overfitting. In order to prevent overfitting, empirically, we use cross-validation ‚Äì split data into three parts: train, (validate) and test, or/and K-fold cross-validation. To explain why this the right thing to do, we shall discuss the notion of generalization that utilizes the view that data is generated per an unknown underlying probability distribution. Methodically, we shall use regularization and again probabilistic formalism will help explain why (or why not) it works well. Probabilistic view, again will come to our rescue to explain the implicit regularization that is implemented by modern methods (e.g. ‚Äòdropout‚Äô) of neural networks. Some examples of regression include predict wage given age, year, and education level. Formally, the data are labeled observations of the following form: (x1,y1),‚ÄÜ‚Ä¶,‚ÄÜ(xN,yN). The goal is to learn a model that maps attribute (or feature) x to label (or target) y so that given attribute x, we can predict corresponding unknown (continuous) label y. That is, to learn a function f such that y‚ÄÑ=‚ÄÑf(x) (and sometimes also what is the confidence interval). In the language of probability, both attributes X and labels Y are still random variables. Now, Y is continuous-valued random variable. The conditional distribution ‚Ñô(Y|X) is of interest. Given attribute X‚ÄÑ=‚ÄÑx, we estimate YÃÇ(x) to minimize estimation error. One the most common estimation error is ùîº[(Y‚àíYÃÇ(x))2|X=x], which is minimized by YÃÇ(x)‚ÄÑ=‚ÄÑùîº[Y|X=x]. Finally, we should determine predictive distribution. ùîº[Y|X=x] is unknown. The model fit for regression means to find the best fit for f(x)‚ÄÑ‚âà‚ÄÑùîº[Y|X=x] using observed data. ",
    "url": "/intro/#supervised-learning",
    
    "relUrl": "/intro/#supervised-learning"
  },"43": {
    "doc": "Introduction",
    "title": "Unsupervised Learning",
    "content": "In unsupervised learning, there is no target. Only input / features are given. The goal is to learn the data distribution. In this course, we are going to cover topics such as dimensionality reduction, matrix estimation, clustering and mixture distribution, and feature extraction (topic model and deep generative model) from unstructured data such as text, audio or image, or for complexity reduction. Examples of unsupervised learning: Finding the principal component of DNA data (dimensionality reduction)¬†4, movie recommendation (matrix estimation), analyzing topics in documents (feature extraction: topic model), generating fake faces of celebrities (feature extraction: deep generative model). ",
    "url": "/intro/#unsupervised-learning",
    
    "relUrl": "/intro/#unsupervised-learning"
  },"44": {
    "doc": "Introduction",
    "title": "Probabilistic Modeling",
    "content": "Two important topics in probabilistic modeling is incorporating prior knowledge from Bayesian perspective and sampling from distribution when probabilistic model is complex. Most of the key tasks in machine learning are inference tasks. For example, in prediction we need to infer ‚Ñô(Y|X), in model learning, we need to infer ‚Ñô(parameters|data). The Bayes‚Äô rule states that $$\\underset{\\text{posterior}}{\\mathbb{P}(\\text{parameters}|\\text{data})}\\propto \\underset{\\text{likelihood}}{\\mathbb{P}(\\text{data}|\\text{parameters})}\\times \\underset{\\text{prior}}{\\mathbb{P}(\\text{parameters})}$$ The key question is how to select prior? This is the prior knowledge of the world. One of the classical priors is Gaussian distribution, which for example, leads to ridge regularization in regression. A probability distribution can be complex. It may have succinct representation but no closed form formula, and hence difficult to evaluate. For example, we may know $$\\mathbb{P}(X=x)\\propto \\exp(f(x))=\\frac{1}{Z}\\exp(f(x)),$$ where Z‚ÄÑ=‚ÄÑ‚à´exp‚ÄÜ(f(x))dx. This integration can be very hard to evaluate for a general f(x). The key algorithm to evaluate on such complex distributions is Markov Chain Monte Carlo (MCMC)5 It has specific forms such as Gibbs sampling and Metropolis-Hastings. MCMC works for generic form of distribution. ",
    "url": "/intro/#probabilistic-modeling",
    
    "relUrl": "/intro/#probabilistic-modeling"
  },"45": {
    "doc": "Introduction",
    "title": "Decision Making",
    "content": "In data driven decision making (in presence of uncertainty), we need to learn the model of uncertainty, given observations. The goal is to make ‚Äúoptimal‚Äù decision with respect to a long-term objective. The decision vs information timescale are critically important. The following diagram summarizes the framework of decision making, . typical setup for decision making problems The two key timescales are state or environment dynamics, and information dynamics. Depending on the two timescales, there are methods / approaches including optimizing given model of uncertainty, Markov decision process, and reinforcement learning. | | State Dynamics | Information Dynamics | . | Optimizing Given Model of Uncertainty | No change (or extremely slow) | Lots of historical information | . | Markov Decision Process | High | Lots of historical information | . | Reinforcement Learning | High | Minimal information, learn as you go | . The fundamental challenge in reinforcement learning is explore vs exploit. An important application of reinforcement learning is automated game player. We‚Äôll do a case study on AlphaGoZero. ",
    "url": "/intro/#decision-making",
    
    "relUrl": "/intro/#decision-making"
  },"46": {
    "doc": "Introduction",
    "title": "And then, What Is\nNot Cover, But Of Interest",
    "content": "We may not be able to cover the following interesting topics in machine learning: . | Active Learning, actively obtain data as each data point is expensive. | Transfer Learning, transfer data collected for one task to other learning task. | Semi-supervised Learning, supervised setting with (additional) unsupervised data. | Causal inference, Hypothesis testing, ... | . But hopefully, things you‚Äôll learn this in course will provide systematic foundations to approach these topics. | See https://g.co/kgs/Lj3v3k to read more about Arthur Samuel.‚Ü©Ô∏é . | What is learning? Some food for thought: https://goo.gl/5R1m4S.‚Ü©Ô∏é . | A good set of readings include [@Cox1946], [@Savage2012] and [@de2017]‚Ü©Ô∏é . | MCMC is one of the top 10 algorithms of all time¬†[@top10]. Other algorithms include quicksort and fast Fourier transform.‚Ü©Ô∏é . | John Novembre, Toby Johnson, Katarzyna Bryc, Zolt¬¥an Kutalik, Adam R Boyko, Adam Auton, Amit Indap, Karen S King, Sven Bergmann, Matthew R Nelson, Matthew Stephens, and Carlos D Bustamante. Genes mirror geography within Europe. Nature, 456:98, Aug 2008. doi:10.1038/nature07331.‚Ü©Ô∏é . | . ",
    "url": "/intro/#and-then-what-is-not-cover-but-of-interest",
    
    "relUrl": "/intro/#and-then-what-is-not-cover-but-of-interest"
  },"47": {
    "doc": "Introduction",
    "title": "Introduction",
    "content": " ",
    "url": "/intro/",
    
    "relUrl": "/intro/"
  },"48": {
    "doc": "Learnability and VC Dimension",
    "title": "Overview",
    "content": "In this lecture, we will discuss learnability and show when the Empirical Risk Minimization methods that we‚Äôve introduced will succeed. We will start our discussion from simple settings under strong assumptions, realizability and infinite hypothesis class. We will define a formal PAC-Learnability and generalize our conclusion to settings where realizability does not hold and hypothesis class is infinite. The concept of VC-dimension will be introduced to quantify the power of infinite hypothesis classes. Finally, we will reach the fundamental theorem of statistical learning. ",
    "url": "/supervised/learnability_and_vc/#overview",
    
    "relUrl": "/supervised/learnability_and_vc/#overview"
  },"49": {
    "doc": "Learnability and VC Dimension",
    "title": "Motivation",
    "content": "In our previous discussion, we introduced the empirical risk minimization (ERM) method as a approximation to the true risk minimization and we also introduced different ways to restrict our hypothesis class so that our ERM found classifiers can have better performance on unseen data. We saw a theoretic analysis on Logistic Regression and Naive Bayes last lecture and in this lecture, we want to generalize the discussion to the entire ERM methods and try to answer some fundamental questions related to the concept of learning. First of all, we haven‚Äôt had a clear definition of what does it mean to be able to learn. We also want to ask, what hypothesis class should we choose and what limitations do different hypothesis class have. Further more, given a hypothesis class, we would like to discuss and determine what kinds of learning rules should we use, and how many data points do we need to learn a good model. With these question in head, we will start our discussion from simple settings and then try to generalize our conclusions. ",
    "url": "/supervised/learnability_and_vc/#motivation",
    
    "relUrl": "/supervised/learnability_and_vc/#motivation"
  },"50": {
    "doc": "Learnability and VC Dimension",
    "title": "Learnability",
    "content": " ",
    "url": "/supervised/learnability_and_vc/#learnability",
    
    "relUrl": "/supervised/learnability_and_vc/#learnability"
  },"51": {
    "doc": "Learnability and VC Dimension",
    "title": "Realizability",
    "content": "We start our discussion with a simplifying assumption, realizability. Formally, realizability means that there exists an optimal hypothesis h*‚ÄÑ‚àà‚ÄÑ‚Ñã such that the true risk L‚Ñô(h*)‚ÄÑ=‚ÄÑ0. This is a strong assumption and it implies that with probability 1, over random samples S‚ÄÑ‚àº‚ÄÑ‚Ñô, LS(h*)‚ÄÑ=‚ÄÑ0 . However, this strong assumption only implies the existence of such a hypothesis that can give 0 error, it is not guaranteed that the ERM found hypothesis hS is the optimal hypothesis h*. The realizability assumption makes sure the richness of our hypothesis class, so that we don‚Äôt need to worry about underfitting, but we can still be overfitting by only minimizing the empirical error. Thus, we want to further discuss that under this assumption, what is the risk of the ERM hypothesis hS on the unseen data and can this risk be bounded such that we are guaranteed to find a good hypothesis? . ",
    "url": "/supervised/learnability_and_vc/#realizability",
    
    "relUrl": "/supervised/learnability_and_vc/#realizability"
  },"52": {
    "doc": "Learnability and VC Dimension",
    "title": "œµ - Œ¥ Parameters",
    "content": "To quantitively measure how good our hypothesis is, we introduce the œµ and Œ¥ parameters for our discussion. The œµ parameter is called accuracy paramter and is used to quantify the quality of the prediction. Concretely, we interpret the event L‚Ñô(hs)‚ÄÑ&gt;‚ÄÑœµ as a failure of the learner, while if L‚Ñô(hS)‚ÄÑ‚â§‚ÄÑœµ, we view the output of the ERM as an approximately correct hypothesis. However, this single parameter is not enough because hS depends on the training set S, and the training set is picked by a random process so that there is randomness in the result of the ERM. It is not realistic to expect that with full certainty S will suffice to direct the learner toward a good hypothesis, as there is always some probability that the sampled training data happens to be very non-representative of the underlying distribution ‚Ñô. We therefore denote the probability of getting a non-representative sample by Œ¥, and call (1‚àíŒ¥) the confidence parameter of our prediction. ",
    "url": "/supervised/learnability_and_vc/#epsilon---delta-parameters",
    
    "relUrl": "/supervised/learnability_and_vc/#epsilon---delta-parameters"
  },"53": {
    "doc": "Learnability and VC Dimension",
    "title": "Finite Hypothesis Class",
    "content": "To bound the error of the ERM hypothesis hS, we further introduce some restrictions on the hypothesis class ‚Ñã so that we can prevent overfitting. The simplest type of restriction on a class is imposing an upper bound on its size, that is, the hypothesis class ‚Ñã has a finite cardinality. With this additional assumption, we can show that the ERM hypothesis will not overfit, i.e., have a bounded error on unseen data. Theorem 1 (). Let ‚Ñã be finite. Let Œ¥‚ÄÑ‚àà‚ÄÑ(0,1), œµ‚ÄÑ&gt;‚ÄÑ0 and $N \\geq \\frac{\\log(|\\mathcal{H}|/\\delta)}{\\epsilon}$. Then, for any distribution ‚Ñô for which realizability holds, we probability at least 1‚ÄÖ‚àí‚ÄÖŒ¥ over the choice of dataset S of size N, every ERM hypothesis hS satisfies L‚Ñô‚ÄÑ‚â§‚ÄÑœµ . Proof. Let ‚ÑãB be the set of ‚Äòfailed‚Äô hypotheses, that is ‚ÑãB‚ÄÑ=‚ÄÑ{h‚ÄÑ‚àà‚ÄÑ‚Ñã‚ÄÑ:‚ÄÑL‚Ñô(h)‚ÄÑ&gt;‚ÄÑœµ} In addition, let M be the set of misleading samples, that is M‚ÄÑ=‚ÄÑ{S‚ÄÑ:‚ÄÑ‚àÉh‚ÄÑ‚àà‚ÄÑ‚ÑãB,‚ÄÜLS(h)‚ÄÑ=‚ÄÑ0} Namely, for every S‚ÄÑ‚àà‚ÄÑM, there is a ‚Äòfailed‚Äô hypothesis, h‚ÄÑ‚àà‚ÄÑ‚Ñ¨, that looks like a ‚Äògood‚Äô hypothesis on S. Now, recall that we would like to bound the probability of the event L‚Ñô(hS)‚ÄÑ&gt;‚ÄÑœµ. Since the realizability implies that LS(hS)‚ÄÑ=‚ÄÑ0, it follows that the event L‚Ñô(hS)‚ÄÑ&gt;‚ÄÑœµ can only happen if for some h‚ÄÑ‚àà‚ÄÑ‚ÑãB, we have LS(h)‚ÄÑ=‚ÄÑ0. In other words, the failure will only happen if our training data is in the set of misleading samples Set M. Formally, we have {S‚ÄÑ:‚ÄÑL‚Ñô(hS)‚ÄÑ&gt;‚ÄÑœµ}‚ÄÑ‚äÜ‚ÄÑM As we can write M as M‚ÄÑ=‚ÄÑ‚à™h‚ÄÑ‚àà‚ÄÑ‚ÑãB{S‚ÄÑ:‚ÄÑLS(h)‚ÄÑ=‚ÄÑ0} Hence, P({S:L‚Ñô(hS)&gt;œµ})‚ÄÑ‚â§‚ÄÑP(‚à™h‚ÄÑ‚àà‚ÄÑ‚ÑãB{S:LS(h)=0}) Applying the union bound to the right-hand side yields P({S:L‚Ñô(hS))‚ÄÑ‚â§‚ÄÑ‚àëh‚ÄÑ‚àà‚ÄÑ‚ÑãBP({S:LS(h)=0}¬†) Next, we can bound each summand of the right-hand side. Fix some ‚Äòfailed‚Äô hypothesis h‚ÄÑ‚àà‚ÄÑ‚ÑãB, the event LS(h)‚ÄÑ=‚ÄÑ0 is equivalent to the event that in the training set, ‚àÄi, h(xi)‚ÄÑ=‚ÄÑyi. Since the training data are i.i.d. sampled, we have $${P} \\left(\\{S : L_{S}(h)=0\\}\\ \\right) = \\prod_{i=1}^N {P} \\left( \\{x_i: h(x_i) = y_i\\} \\right)$$ For each individual sampling of an element of the training set, we have P({xi:h(xi)=yi})‚ÄÑ=‚ÄÑ1‚ÄÖ‚àí‚ÄÖL‚Ñô(h)‚ÄÑ‚â§‚ÄÑ1‚ÄÖ‚àí‚ÄÖœµ where the last inequality follows from the fact that h‚ÄÑ‚àà‚ÄÑ‚ÑãB. Using the inequality 1‚ÄÖ‚àí‚ÄÖœµ‚ÄÑ‚â§‚ÄÑe‚àíœµ, we have for every h‚ÄÑ‚àà‚ÄÑ‚ÑãB, P(S:LS(h)=0)‚ÄÑ‚â§‚ÄÑ(1‚àíœµ)N‚ÄÑ‚â§‚ÄÑe‚àíœµN Therefore, we have P(S:L‚Ñô(hS)&gt;œµ)‚ÄÑ‚â§‚ÄÑ|‚ÑãB|e‚àíœµN‚ÄÑ‚â§‚ÄÑ|‚Ñã|e‚àíœµN Let Œ¥‚ÄÑ=‚ÄÑP(S:L‚Ñô(hS)&gt;œµ), we will reach the desired conclusion that with probability at least 1‚ÄÖ‚àí‚ÄÖŒ¥, and having $N \\geq \\frac{\\log (|\\mathcal{H}|/\\delta)}{\\epsilon}$, L‚Ñô(hS)‚ÄÑ‚â§‚ÄÑœµ¬†‚óª . A weaker result can be proved without realizability, see Exercise 2 for details. ",
    "url": "/supervised/learnability_and_vc/#finite-hypothesis-class",
    
    "relUrl": "/supervised/learnability_and_vc/#finite-hypothesis-class"
  },"54": {
    "doc": "Learnability and VC Dimension",
    "title": "PAC Learnability",
    "content": "We see that the finite hypothesis class makes it possible to bound the unseen error of ERM hypothesis. In order the generalize this result, we first give a formal name of such hypothesis classes. As we are using the œµ and Œ¥ parameters which implies the conclusion is both approximate and not determined, we use the name Probably approximately correct learnablity, also known as PAC-Learnability. A formal definition is as follows, . Definition 2 (PAC-Learnablity). Assuming realizability, a hypothesis class ‚Ñã is PAC-learnable if there exists a function N‚Ñã(œµ,Œ¥) and a learning algorithm with the following property: For every œµ,‚ÄÜŒ¥‚ÄÑ‚àà‚ÄÑ(0,1) and every distribution ‚Ñô, training using N‚ÄÑ‚â•‚ÄÑN‚Ñã(œµ,Œ¥) i.i.d. samples generated from ‚Ñô, the learning algorithm returns a hypothesis h such that L‚Ñô‚ÄÑ‚â§‚ÄÑœµ with confidence (1‚àíŒ¥) over choice of samples. Informally, PAC-learnability of class ‚Ñã means that enough number of random examples drawn from the data distribution will allow approximate risk minimization, i.e., ensure L‚Ñô(h)‚ÄÑ‚â§‚ÄÑœµ with probability ‚ÄÑ‚â•‚ÄÑ1‚ÄÖ‚àí‚ÄÖŒ¥, where the number of samples needed depends on the desired tolerances (œµ,Œ¥). Note here œµ and Œ¥ are inevitable. Œ¥ arises due to the randomness of training data S drawn from ‚Ñô and œµ arises due to the actual hypothesis picked by the learner on the finite data S. With this formal concept of PAC-learnable defined, we can discuss the situations when our two assumptions on realizability and finite hypothesis class do not hold. Concretely, is the hypothesis class still learnable if realizability does not hold? And on the other hand, what about infinite hypothesis classes? Are they PAC-learnable? . ",
    "url": "/supervised/learnability_and_vc/#pac-learnability",
    
    "relUrl": "/supervised/learnability_and_vc/#pac-learnability"
  },"55": {
    "doc": "Learnability and VC Dimension",
    "title": "Agnostic PAC-Learnability",
    "content": "We first release the realizability assumption. By No-Free-Lunch (NFL) theorem, we know that no learner is guaranteed to match the Bayes classifier in general, as there‚Äôs always an adversarial distribution that can be constructed on which our learner fails while another may succeed. Thus, if the realizability does not hold, we don‚Äôt have the hope of satisfying L‚Ñô‚ÄÑ‚â§‚ÄÑœµ. We now can only weaken our aim, and see if we can at least come œµ‚àí close to the best possible classifier within our hypothesis class with high probability, i.e. L‚Ñô(hS)‚ÄÑ‚â§‚ÄÑinfh‚Ä≤‚ÄÑ‚àà‚ÄÑ‚Ñã‚Ñô(h‚Ä≤)‚ÄÖ+‚ÄÖœµ In this setting, the hypothesis class ‚Ñã may be bad, but we can still try to be approximately as good as the best possible hypothesis within this class. This weaker property is known as agnostic PAC-Learnability. Definition 3 (Agnostic PAC-Learnability). A hypothesis class ‚Ñã is agnostic PAC learnable if there exist a function N‚Ñã‚ÄÑ:‚ÄÑ(0,1)2‚ÄÑ‚Üí‚ÄÑN and a learning algorithm with the following property: For every œµ,‚ÄÜŒ¥‚ÄÑ‚àà‚ÄÑ(0,1) and for every distribution ‚Ñô over ùí≥‚ÄÖ√ó‚ÄÖùí¥, when running the learning algorithm on N‚ÄÑ&gt;‚ÄÑN‚Ñã(œµ,Œ¥) i.i.d. samples generated by ‚Ñô, the algorithm returns a hypothesis h such that, with probability of at least 1‚ÄÖ‚àí‚ÄÖŒ¥ over the choice of the N training samples, L‚Ñô(h)‚ÄÑ‚â§‚ÄÑinfh‚Ä≤‚ÄÑ‚àà‚ÄÑ‚Ñã‚Ñô(h‚Ä≤)‚ÄÖ+‚ÄÖœµ . Clearly, if the realizability assumption holds, agnostic PAC-Learnability provides the same guarantee as PAC-Learnability. In that sense, agnostic PAC-Learnability generalizes the definition of PAC-Learnability. When the realizability assumption does not hold, no learner can guarantee an arbitrarily small error. Nevertheless, under the definition of agnostic PAC learning, a learner can still declare success if its error is not much larger than the best error achievable by a predictor from the class ‚Ñã. This is in contrast to PAC learning, in which the learner is required to achieve a small error in absolute terms and not relative to the best error achievable by the hypothesis class. Recall from lecture 2 where we decompose the error into the approximation error term and the estimation error term, where L‚Ñô(hS)‚ÄÑ=‚ÄÑœµapx‚ÄÖ+‚ÄÖœµest œµapx‚ÄÑ:=‚ÄÑminh‚ÄÑ‚àà‚ÄÑ‚ÑãL(h) œµest‚ÄÑ:=‚ÄÑL‚Ñô(hS)‚ÄÖ‚àí‚ÄÖœµapx As the approximation error depends on the fit of our prior knowledge via the inductive bias to the unknown underlying distribution, so it won‚Äôt be minimized further more after we‚Äôve chosen the hypothesis class ‚Ñã. The agnostic PAC-Learnability loses the bound on this term but bound the estimation error uniformly over all distributions for a given hypothesis class. ",
    "url": "/supervised/learnability_and_vc/#agnostic-pac-learnability",
    
    "relUrl": "/supervised/learnability_and_vc/#agnostic-pac-learnability"
  },"56": {
    "doc": "Learnability and VC Dimension",
    "title": "Uniform\nConvergence implies agnostic PAC Learnability",
    "content": "How can we make sure the ERM solution is close the true risk? One strong assumption one can make is that LS(h) for all h‚ÄÑ‚àà‚ÄÑ‚Ñã is close to the true risk L‚Ñô(h), then the ERM solution hS will also have small true risk L‚Ñô(hS). Hence, we introduce the notion of an œµ‚àírepresentative data sample . Definition 4 (œµ-representative). A dataset S is called œµ-representative if ‚àÄhin‚Ñã,‚Ää‚ÄÅ|LS(h)‚àíL‚Ñô(h)|‚ÄÑ‚â§‚ÄÑœµ . The next simple conclusion we can make is that whenever the sample is œµ/2-representative, the ERM learning rule is guaranteed to return a good hypothesis. Theorem 5. Assume S is œµ/2 - representative. Then, any ERM solution hS‚ÄÑ‚àà‚ÄÑargminh‚ÄÑ‚àà‚ÄÑ‚ÑãLS(h) satisfies L‚Ñô‚ÄÑ‚â§‚ÄÑminh‚ÄÑ‚àà‚ÄÑ‚ÑçL‚Ñô‚ÄÖ+‚ÄÖœµ . Proof. For every h‚ÄÑ‚àà‚ÄÑ‚Ñã, $$\\begin{aligned} L_{\\mathbb{P}} &amp; \\leq L_S(h_S) + \\epsilon/2 \\\\ &amp; \\leq L_S(h) + \\epsilon/2 \\\\ &amp; \\leq L_{\\mathbb{P}}(h) + \\epsilon/2 + \\epsilon/2 \\\\ &amp; = L_{\\mathbb{P}}(h) + \\epsilon \\end{aligned}$$¬†‚óª . The simple theorem implies that to ensure that the ERM rule is agnostic PAC-Learnable, it suffices to show that with probability of at least 1?Œ¥ over the random choice of a training set, it will be an œµ-representative training set. The following uniform convergence condition formalizes this requirement. Definition 6 (Uniform Convergence). A hypothesis class ‚Ñã has the uniform convergence property w.r.t a domain Z and a loss function ‚Ñì, if there exists a function N‚ÑãUC‚ÄÑ:‚ÄÑ(0,1)2‚ÄÑ‚Üí‚ÄÑ‚Ñï such that for every œµ,‚ÄÜŒ¥‚ÄÑ‚àà‚ÄÑ(0,1) and for every probability distribution ‚Ñô over Z, if S is a sample of N‚ÄÑ‚â•‚ÄÑN‚ÑãUC(œµ,Œ¥) i.i.d. examples drawn from ‚Ñô, then, with probability of at least 1‚ÄÖ‚àí‚ÄÖŒ¥, S is œµ-representative. Similar to the definition of sample complexity for PAC learning, the function N‚ÑãUC measures the minimal sample complexity of obtaining the uniform convergence property, namely, how many examples we need to ensure that with probability of at least 1‚ÄÖ‚àí‚ÄÖŒ¥ the sample would be œµ-representative. The term uniform here refers to having a fixed sample size that works for all members of ‚Ñã and over all possible probability distributions over the domain. The following corollary follows directly from the previous theorem and the definition of uniform convergence. Corollary 7. If a class ‚Ñã has the uniform convergence property with a function N‚ÑãUC then the class is agnostically PAC learnable with the sample complexity N‚Ñã(œµ,Œ¥)‚ÄÑ‚â§‚ÄÑN‚ÑãUC(œµ/2,Œ¥). ",
    "url": "/supervised/learnability_and_vc/#uniform-convergence-implies-agnostic-pac-learnability",
    
    "relUrl": "/supervised/learnability_and_vc/#uniform-convergence-implies-agnostic-pac-learnability"
  },"57": {
    "doc": "Learnability and VC Dimension",
    "title": "VC-Dimesion",
    "content": "Now, let‚Äôs move to the situation of infinite hypothesis class. Clearly, we don‚Äôt have a measurement for the size of the hypothesis class any more, but it is still possible to quantitively measure complexity of the model. For learnability In classification problems, what really matters is not the literal size of the hypothesis class, but the maximum number of data points that can be classified exactly. Take the simple situation in Figure 1 for example, the hypothesis class of 1-dimensional linear classifier has a infinite size, but this doesn‚Äôt mean this class is a very complex class. As shown in Figure 1 (a), two points with whatever labels can be classified correctly by a linear classifier, but in Figure 1 (b), we can see that this no longer holds for 3 points, as the last example in (b) cannot be classified correctly by any hypothesis in the linear classifier class. This inspires us that in order to measure the richness of our hypothesis class, we can try to construct a subset C of the data domain for which our classifier fails or succeeds. To understand the power of our hypothesis class, we just focus on its behavior on C and try to check how many different possible classification decisions on C can our hypothesis class capture. Then, if the hypothesis class can explain all decisions possible on C, then one can construct a ‚Äòmisleading data distribution‚Äô so that we maintain realizability on C but can be totally wrong on the part outside of C and thus suffer large risk. This implies that to achieve learnability, we need to restrict the size of C. Linear classifiers in 1D can shatter 2 points as in (a), but cannot classifier the last case correctly in (b). Thus the VC-Dimension of 1-D linear classifiers is 2. To be more formal, here we introduce the definition of restriction of ‚Ñã to C and the following definition of shattering and VC-Dimension . Definition 8. Let ‚Ñã be a class of functions from ùí≥ to {0,‚ÄÜ1} and let C‚ÄÑ=‚ÄÑ{c1,‚ÄÜ...,‚ÄÜcm}‚ÄÑ‚äÇ‚ÄÑùí≥. The restriction of ‚Ñã to C is the set of functions from C to {0,‚ÄÜ1} that can be derived from ‚Ñã. That is, ‚ÑãC‚ÄÑ=‚ÄÑ{(h(c1),...,h(cm))‚ÄÑ:‚ÄÑh‚ÄÑ‚àà‚ÄÑ‚Ñã} where we present each function from C to {0,‚ÄÜ1} as a vector in {0,‚ÄÜ1}|C|. If the restriction of H to C is the set of all functions from C to {0,‚ÄÜ1}, then we say ‚Ñã shatters the set C, formally . Definition 9 (Shattering). A hypothesis class ‚Ñã shatters finite set C‚ÄÑ‚äÇ‚ÄÑùí≥ if the restriction of ‚Ñã to C is the set of all functions from C to {0,‚ÄÜ1}. That is, |‚ÑãC|‚ÄÑ=‚ÄÑ2|C|. Definition 10 (VC-dimension). The VC-dimension of a hypothesis class ‚Ñã, denoted VCdim(‚Ñã), is the maximal size of a set C‚ÄÑ‚äÇ‚ÄÑùí≥ that can be shattered by ‚Ñã. If ‚Ñã can shatter sets of arbitrarily large size, we say that ‚Ñã has infinite VC-dimension. Here we give another example on 2-D linear classifiers, as shown in Figure 2. In (a), we can see that the linear classifier class can shatter 3 points in 2 dimensional space, however in (b), it cannot shatter 4 points as there exists a case where no linear classifier can correctly classifier the 4 points with the particular labelling as in the right figure in (b). This shows that the VC-Dimension of 2-D linear classifiers is 3. Linear classifiers in 2D can classifier 3 points with arbitrary labelling as shown in (a), but cannot classifier 4 points correctly as in (b). Thus the VC-Dimension of 2-D linear classifiers is 3. From this kind of observation, we can see that to show that VC‚ÄÖ‚àí‚ÄÖdim(‚Ñã)‚ÄÑ=‚ÄÑd, we need to prove two things: . | There exists a set C of size d that is shattered by ‚Ñã, this proves VC‚ÄÖ‚àí‚ÄÖdim(‚Ñã)‚ÄÑ‚â•‚ÄÑd . | No set of size d‚ÄÖ+‚ÄÖ1 is shattered by ‚Ñã, this proves VC‚ÄÖ‚àí‚ÄÖdim(‚Ñã)‚ÄÑ&lt;‚ÄÑd‚ÄÖ+‚ÄÖ1. Thus VC‚ÄÖ‚àí‚ÄÖdim(‚Ñã)‚ÄÑ=‚ÄÑd. | . Though we showed the VC-Dimension of d-dimensional linear classifier is d‚ÄÖ+‚ÄÖ1, most of the time, we can only have lower/upper bound of VC dimension, but not an exact computable number. Thus, it is important to understand the meaning of the lower and upper bound of VC-Dimension. ",
    "url": "/supervised/learnability_and_vc/#vc_dimension",
    
    "relUrl": "/supervised/learnability_and_vc/#vc_dimension"
  },"58": {
    "doc": "Learnability and VC Dimension",
    "title": "Fundamental Theorem of\nLearnability",
    "content": "Theorem 11 (The Fundamental Theorem of Statistical Learning). Let ‚Ñã be a hypothesis class of functions from a domain ùí≥ to {0,‚ÄÜ1} and let the loss function be the 0‚ÄÖ‚àí‚ÄÖ1 loss. Then the following are equivalent: . | ‚Ñã has the uniform convergence property. | Any ERM rule is a successful agnostic PAC learner for ‚Ñã. | ‚Ñã is agnostic PAC learnable. | ‚Ñã is PAC learnable. | ‚Ñã Any ERM rule is a successful PAC learner for ‚Ñã. | ‚Ñã has a finite VC-dimension. | . In our previous discussion, we saw 1‚ÄÑ‚Üí‚ÄÑ2. 2‚ÄÑ‚Üí‚ÄÑ3, 3‚ÄÑ‚Üí‚ÄÑ4 and 2‚ÄÑ‚Üí‚ÄÑ5 are all trivial. For 4‚ÄÑ‚Üí‚ÄÑ6 and 5‚ÄÑ‚Üí‚ÄÑ6, there is detailed proof in [SSS] through the no-free-lunch theorem. Here, we take a closer look at 6‚ÄÑ‚Üí‚ÄÑ1, that a finite VC-dimension implies the uniform convergence property, and therefore is PAC-learnable. The detailed proof can be found in chapter 6 of [SSS], here we provide a high level sketch of the proof. The two main parts of the proof are . | If VC‚ÄÖ‚àí‚ÄÖdim(‚Ñã)‚ÄÑ=‚ÄÑd, when restricting to a finite subset C of the data domain, its effective size |‚ÑãC| is only O|C|d, instead of exponential in |C| . | Finite hypothesis class can be proved to have the uniform convergence property by a direct application of Hoeffiding inequality plus the union bound theorem. Similarly, the uniform convergence holds whenever the \"effective size\" is small. | . To define the term \"effective size\", we introduce the definition of Growth Function, . Definition 12 (Growth Function). Let ‚Ñã be a hypothesis class. Then the growth function of ‚Ñã, denoted œÑ‚Ñã‚ÄÑ:‚ÄÑ‚Ñï‚ÄÑ‚Üí‚ÄÑ‚Ñï, is defined as œÑ‚Ñã(N)‚ÄÑ=‚ÄÑmaxC‚ÄÑ‚äÇ‚ÄÑùí≥‚ÄÑ:‚ÄÑ|C|‚ÄÑ=‚ÄÑN|‚ÑãC| . In words, œÑ‚Ñã(N) is the number of different functions from a set C of size N to {0,‚ÄÜ1} that can be obtained by restricting ‚Ñã to C. We then can prove the Sauer‚Äôs lemma that can bound this growth function . Lemma 13. Let ‚Ñã be a hypothesis class with VC‚ÄÖ‚àí‚ÄÖDim(‚Ñã)‚ÄÑ‚â§‚ÄÑd‚ÄÑ&lt;‚ÄÑ‚àû. Then for all N, $\\tau_\\mathcal{H}(N) \\leq \\sum_{i=0}^d \\begin{pmatrix} N\\\\i \\end{pmatrix}$. In particular, if N‚ÄÑ&gt;‚ÄÑd‚ÄÖ+‚ÄÖ1 then œÑ‚Ñã(N)‚ÄÑ‚â§‚ÄÑ(eN)d . Thus, finite VC-dimension implies polynomial growth, while infinite VC-dim means exponential growth. Intuitively, for any C as a subset of ùí≥, let B be a subset of C such that ‚Ñã shatters B. Then, |‚ÑãC|‚ÄÑ‚â§‚ÄÑ#{B‚ÄÑ‚äÇ‚ÄÑC‚ÄÑ:‚ÄÑ‚Ñã shatters B}. That is, if ùíû is the collection of subsets of C that are shattered by ‚Ñã, then |‚ÑãC| is upper-bounded by the cardinality of ùíû. Then we can show the ERM error is bounded using the growth function . Theorem 14. Let ‚Ñã be a class and œÑ‚Ñã its growth function. Then for every distribution ‚Ñô(X,Y) and every Œ¥‚ÄÑ‚àà‚ÄÑ(0,1), with probability at least 1‚ÄÖ‚àí‚ÄÖŒ¥ over the choices of S‚ÄÑ‚àº‚ÄÑ‚Ñô, we have $$|L_S(h) - L_\\mathbb{P}(h) | \\leq \\frac{4+\\sqrt{\\log \\tau_{\\mathcal{H}}(2N)}}{\\delta \\sqrt{2N}}$$ . And it follows from here that if VC-Dim(‚Ñã) is finite, then the uniform convergence property holds, and indeed, $$N_{\\mathcal{H}}^{UC}(\\epsilon, \\delta) \\leq O(\\frac{d}{(\\delta \\epsilon)^2})$$ suffices for the uniform convergence property to hold. A more quantitative version of this theorem is as follows, and the proof can be found in chapter 28 of [SSS]. Theorem 15. Let ‚Ñã be a hypothesis class of functions from a domain ùí≥ to {0,‚ÄÜ1} and let the loss function be the 0‚ÄÖ‚àí‚ÄÖ1 loss. Assume that VC‚ÄÖ‚àí‚ÄÖDim(‚Ñã)‚ÄÑ=‚ÄÑd‚ÄÑ&lt;‚ÄÑ‚àû. Then, there are absolute constants C1, C2 such that: . | ‚Ñã has the uniform convergence property with sample complexity $$C_1\\frac{d+\\log(1/\\delta)}{\\epsilon^2} \\leq N_\\mathcal{H}^{UC}(\\epsilon,\\delta) \\leq C_2 \\frac{d+\\log(1/\\delta)}{\\epsilon^2}$$ . | ‚Ñã is agnostic PAC learnable with sample complexity $$C_1\\frac{d+\\log(1/\\delta)}{\\epsilon^2} \\leq N_\\mathcal{H}(\\epsilon,\\delta) \\leq C_2 \\frac{d+\\log(1/\\delta)}{\\epsilon^2}$$ . | ‚Ñã is PAC learnable with sample complexity $$C_1\\frac{d+\\log(1/\\delta)}{\\epsilon} \\leq N_\\mathcal{H}(\\epsilon,\\delta) \\leq C_2 \\frac{d\\log (1/\\epsilon)+\\log(1/\\delta)}{\\epsilon}$$ . | . ",
    "url": "/supervised/learnability_and_vc/#fundamental-theorem-of-learnability",
    
    "relUrl": "/supervised/learnability_and_vc/#fundamental-theorem-of-learnability"
  },"59": {
    "doc": "Learnability and VC Dimension",
    "title": "Learnability and VC Dimension",
    "content": " ",
    "url": "/supervised/learnability_and_vc/",
    
    "relUrl": "/supervised/learnability_and_vc/"
  },"60": {
    "doc": "Linear Regression",
    "title": "Linear Regression",
    "content": " ",
    "url": "/supervised/linearRegression/",
    
    "relUrl": "/supervised/linearRegression/"
  },"61": {
    "doc": "Markov Decision Process",
    "title": "Markov Decision Process",
    "content": " ",
    "url": "/reinforcement/mdp/",
    
    "relUrl": "/reinforcement/mdp/"
  },"62": {
    "doc": "Policy Gradient",
    "title": "Policy Gradient",
    "content": " ",
    "url": "/reinforcement/policy_gradient/",
    
    "relUrl": "/reinforcement/policy_gradient/"
  },"63": {
    "doc": "Reinforcement Learning",
    "title": "Reinforcement Learning",
    "content": " ",
    "url": "/reinforcement/",
    
    "relUrl": "/reinforcement/"
  },"64": {
    "doc": "Background/Review",
    "title": "Backrgound and Review",
    "content": " ",
    "url": "/review/#backrgound-and-review",
    
    "relUrl": "/review/#backrgound-and-review"
  },"65": {
    "doc": "Background/Review",
    "title": "Notations",
    "content": ". | Data matrix is of the size (n,d) where n is the number of data points, and d is the dimension of the features | Vectors are denoted with a small-case letter; matrices capital letters | The default norm of a vector is the l2 norm | . ",
    "url": "/review/#notations",
    
    "relUrl": "/review/#notations"
  },"66": {
    "doc": "Background/Review",
    "title": "Linear Algebra,\nCalculus, and Optimization",
    "content": ". | Gradient vector | Positive semi-definite (PSD) and positive definiteness (PD) | Convexity, strong convexity | Optimal solutions, uniqueness | . ",
    "url": "/review/#linear-algebra-calculus-and-optimization",
    
    "relUrl": "/review/#linear-algebra-calculus-and-optimization"
  },"67": {
    "doc": "Background/Review",
    "title": "Probability and Statistics",
    "content": "Concepts related to a single distribution . | Basics and formulas | Fundamental distributions . | 1d normal | Multi-variate normal | . | Jensen‚Äôs inequality | Max likelihood and Max log likelihood | . Example: MLE for Gaussian . We have data x1,‚ÄÜ‚Ä¶,‚ÄÜxN sampled from a distribution. The goal is to learn the distribution. The assumption is that the data is generated from a Gaussian distribution ùí©(Œº,œÉ2). Then the refined goal is to learn the mean and variance. How to learn (parameters, mean and variance)? . A common method is maximum likelihood (ML), that is, choose the parameters that maximize ‚Ñô(data|parameters). In this problem, to choose mean, variance from samples, the likelihood is $$\\begin{aligned} \\mathbb{P}\\left(x_1,\\ldots,x_N|\\mu,\\sigma^2\\right)=&amp;\\prod_{i=1}^N\\mathbb{P}\\left(x_i|\\mu,\\sigma^2\\right) \\\\ =&amp;\\prod_{i=1}^N \\frac{1}{(2\\pi\\sigma^2)^{1/2}}\\exp\\left(-\\frac{(x_i-\\mu)^2}{2\\sigma^2}\\right). \\end{aligned}$$ Maximizing likelihood is same as maximizing logarithm of likelihood. This leads to maxŒº,‚ÄÜœÉ2g(Œº,œÉ2), where $$g(\\mu,\\sigma^2)=-\\frac{1}{2\\sigma^2}\\sum_{i=1}^N(x_i-\\mu)^2-N\\ln\\sigma -N\\ln\\sqrt{2\\pi}.$$ This is an optimization problem and its solution is what we desire. For such reasons, optimization is an integral part of Machine Learning. The ML estimation for variance (and standard deviation) is biased. This leads to the Bessel correction for variance: $$\\tilde{\\sigma}^2_{\\rm ML}=\\frac{1}{N-1}\\sum_{i=1}^N (x_i-\\mu_{\\rm ML})^2.$$ . Concepts involving multiple distributions . | Marginal independence . | Joint probability is the product | Entropy of the joint distribution is the sum of individual entropies | . | Bayes‚Äô rule | Conditional independence . | Comparison with marginal independence | . | Importance sampling | KL divergence | . ",
    "url": "/review/#probability-and-statistics",
    
    "relUrl": "/review/#probability-and-statistics"
  },"68": {
    "doc": "Background/Review",
    "title": "Background/Review",
    "content": " ",
    "url": "/review/",
    
    "relUrl": "/review/"
  },"69": {
    "doc": "Weekly Schedule",
    "title": "Weekly Schedule",
    "content": ". | 9:00 AM | 9:30 AM | 10:00 AM | 10:30 AM | 11:00 AM | 11:30 AM | 12:00 PM | 12:30 PM | 1:00 PM | 1:30 PM | 2:00 PM | 2:30 PM | 3:00 PM | 3:30 PM | 4:00 PM | 4:30 PM | 5:00 PM | 5:30 PM | . | ",
    "url": "/schedule/",
    
    "relUrl": "/schedule/"
  },"70": {
    "doc": "Weekly Schedule",
    "title": "\n",
    "content": "| ",
    "url": "/schedule/",
    
    "relUrl": "/schedule/"
  },"71": {
    "doc": "Weekly Schedule",
    "title": "\nMonday\n",
    "content": ". | Office Hours 1:00 PM‚Äì2:00 PM 26-322? | . | ",
    "url": "/schedule/",
    
    "relUrl": "/schedule/"
  },"72": {
    "doc": "Weekly Schedule",
    "title": "\nTuesday\n",
    "content": ". | Lecture 2:30 PM‚Äì4:00 PM 32-123 | . | ",
    "url": "/schedule/",
    
    "relUrl": "/schedule/"
  },"73": {
    "doc": "Weekly Schedule",
    "title": "\nWednesday\n",
    "content": "| ",
    "url": "/schedule/",
    
    "relUrl": "/schedule/"
  },"74": {
    "doc": "Weekly Schedule",
    "title": "\nThursday\n",
    "content": ". | Lecture 2:30 PM‚Äì4:00 PM 32-123 | . | ",
    "url": "/schedule/",
    
    "relUrl": "/schedule/"
  },"75": {
    "doc": "Weekly Schedule",
    "title": "\nFriday\n",
    "content": ". | Recitation 10:00 AM‚Äì11:00 AM 4-237 | Recitation 11:00 AM‚Äì12:00 PM 4-237 | Recitation 1:00 PM‚Äì2:00 PM 3-270 | Recitation 2:00 PM‚Äì3:00 PM 3-270 | . | . ",
    "url": "/schedule/",
    
    "relUrl": "/schedule/"
  },"76": {
    "doc": "Supervised Learning",
    "title": "Supervised Learning",
    "content": " ",
    "url": "/supervised/",
    
    "relUrl": "/supervised/"
  },"77": {
    "doc": "Unsupervised Learning",
    "title": "Unsupervised Learning",
    "content": " ",
    "url": "/unsupervised/",
    
    "relUrl": "/unsupervised/"
  },"78": {
    "doc": "Value functions and Bellman",
    "title": "Value functions and Bellman",
    "content": " ",
    "url": "/reinforcement/value_bellman/",
    
    "relUrl": "/reinforcement/value_bellman/"
  }
}
