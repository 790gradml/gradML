<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>Discriminative vs Generative Classification | 6.790 Machine Learning</title> <meta name="generator" content="Jekyll v4.3.2" /> <meta property="og:title" content="Discriminative vs Generative Classification" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="Graduate Machine Learning course, MIT EECS, fall 2023 term." /> <meta property="og:description" content="Graduate Machine Learning course, MIT EECS, fall 2023 term." /> <link rel="canonical" href="https://gradml.mit.edu/supervised/DiscriminativeGenerative/" /> <meta property="og:url" content="https://gradml.mit.edu/supervised/DiscriminativeGenerative/" /> <meta property="og:site_name" content="6.790 Machine Learning" /> <meta property="og:type" content="website" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="Discriminative vs Generative Classification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","dateModified":"2023-06-16T05:04:41-04:00","description":"Graduate Machine Learning course, MIT EECS, fall 2023 term.","headline":"Discriminative vs Generative Classification","url":"https://gradml.mit.edu/supervised/DiscriminativeGenerative/"}</script> <!-- End Jekyll SEO tag --> <!-- Copied from https://katex.org/docs/browser.html#starter-template --> <link rel="stylesheet" href="/assets/js/katex.min.css"> <!-- The loading of KaTeX is deferred to speed up page rendering --> <script defer src="/assets/js/katex.min.js"> </script> <!-- Automatically display code inside script tags with type=math/tex using KaTeX --> <script defer src="/assets/js/mathtex-script-type.js"> </script> <!-- To automatically render math in text elements, include the auto-render extension: --> <script defer src="/assets/js/auto-render.min.js" onload="renderMathInElement(document.body, { globalGroup: true, trust: true, strict: false, throwOnError: false, });"></script> <!-- The KaTeX default is 1.21em, see https://katex.org/docs/font.html#font-size-and-lengths --> <style> .katex { font-size: 1.21em; } </style> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"> 6.790 <br> Machine Learning </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"> <use xlink:href="#svg-menu"></use> </svg> </a> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><a href="/" class="nav-list-link">About</a></li><li class="nav-list-item"><a href="/calendar/" class="nav-list-link">Syllabus / Calendar</a></li><li class="nav-list-item"><a href="/schedule/" class="nav-list-link">Weekly Schedule</a></li><li class="nav-list-item"><a href="/intro/" class="nav-list-link">Introduction</a></li><li class="nav-list-item"><a href="/review/" class="nav-list-link">Background/Review</a></li><li class="nav-list-item active"><button class="nav-list-expander btn-reset" aria-label="toggle items in Supervised Learning category" aria-pressed="true"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/supervised/" class="nav-list-link">Supervised Learning</a><ul class="nav-list"><li class="nav-list-item "><a href="/supervised/classification_foundation/" class="nav-list-link">Classification Foundation</a></li><li class="nav-list-item active"><a href="/supervised/DiscriminativeGenerative/" class="nav-list-link active">Discriminative vs Generative Classification</a></li><li class="nav-list-item "><a href="/supervised/learnability_and_vc/" class="nav-list-link">Learnability and VC Dimension</a></li><li class="nav-list-item "><a href="/supervised/linearRegression/" class="nav-list-link">Linear Regression</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Unsupervised Learning category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/unsupervised/" class="nav-list-link">Unsupervised Learning</a><ul class="nav-list"><li class="nav-list-item "><button class="nav-list-expander btn-reset" aria-label="toggle items in Graphical Models category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/unsupervised/graphical/" class="nav-list-link">Graphical Models</a><ul class="nav-list"></ul></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Reinforcement Learning category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/reinforcement/" class="nav-list-link">Reinforcement Learning</a><ul class="nav-list"><li class="nav-list-item "><a href="/reinforcement/mdp/" class="nav-list-link">Markov Decision Process</a></li><li class="nav-list-item "><a href="/reinforcement/value_bellman/" class="nav-list-link">Value functions and Bellman</a></li><li class="nav-list-item "><a href="/reinforcement/bandit/" class="nav-list-link">Bandits</a></li><li class="nav-list-item "><a href="/reinforcement/policy_gradient/" class="nav-list-link">Policy Gradient</a></li><li class="nav-list-item "><a href="/reinforcement/deepRL/" class="nav-list-link">Deep Reinforcement Learning</a></li></ul></li></ul> </nav> <footer class="site-footer"> <a href="/credit">Acknowledgement</a> </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search 6.790 Machine Learning" aria-label="Search 6.790 Machine Learning" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> <nav aria-label="Auxiliary" class="aux-nav"> <ul class="aux-nav-list"> <li class="aux-nav-list-item"> <a href="https://canvas.mit.edu/" class="site-button" target="_blank" rel="noopener noreferrer" > Canvas </a> </li> <li class="aux-nav-list-item"> <a href="https://piazza.com" class="site-button" target="_blank" rel="noopener noreferrer" > Piazza </a> </li> </ul> </nav> </div> <div id="main-content-wrap" class="main-content-wrap"> <nav aria-label="Breadcrumb" class="breadcrumb-nav"> <ol class="breadcrumb-nav-list"> <li class="breadcrumb-nav-list-item"><a href="/supervised/">Supervised Learning</a></li> <li class="breadcrumb-nav-list-item"><span>Discriminative vs Generative Classification</span></li> </ol> </nav> <div id="main-content" class="main-content"> <main> <h1 id="overview"> <a href="#overview" class="anchor-heading" aria-labelledby="overview"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> Overview </h1> <p>In this lecture, we will mainly discuss two different approaches to build classifiers, the generative approach and the discriminative approach. As concrete examples, we will look at the Naive Bayes classifier for the generative approach and compare it the logistic regression, as an example of discriminative approach. We will show under center conditions, the Naive Bayes is a linear classifier, just as Logistic Regression, but it assumes stronger assumptions and therefore is a more biased classifier.</p> <h1 id="four-approaches-to-build-classifiers"> <a href="#four-approaches-to-build-classifiers" class="anchor-heading" aria-labelledby="four-approaches-to-build-classifiers"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> Four Approaches to Build Classifiers </h1> <h2 id="review-bayes-classifier"> <a href="#review-bayes-classifier" class="anchor-heading" aria-labelledby="review-bayes-classifier"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> Review: Bayes Classifier </h2> <p>We started our discussion of classification from the Bayes Classifier in Lecture 2. Recall the Bayes classifier <span class="math inline"><em>h</em><sup>*</sup>(<em>x</em>)</span> is defined by the rule: <span class="math display">$$h^{*}(x) := \begin{cases} 1, &amp;\text{if}\ \eta(x)=\mathbb{P}(Y=1 | X=x)&gt;\frac{1}{2}\\ 0, &amp;\text{otherwise}. \end{cases}$$</span> The classifier predicts label <span class="math inline">1</span> if the conditional probability of being in class <span class="math inline">1</span> is bigger than half. We also showed this classifier is actually the optimal possible classifier, as the underlying distribution <span class="math inline">ℙ</span> is assumed known.</p> <h2 id="when-bayes-classifier-is-not-feasible"> <a href="#when-bayes-classifier-is-not-feasible" class="anchor-heading" aria-labelledby="when-bayes-classifier-is-not-feasible"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> When Bayes Classifier is not feasible </h2> <p>However, we know this idealized situation is seldom the case in reality, as we usually do not have access to <span class="math inline">ℙ</span>. Therefore, we introduced two different approaches, distance based classification (e.g. Nearest Neighbors) and Empirical Risk Minimization (e.g. SVM). Here, we give a formal summary of four possible methods to learn a classifier.</p> <ul> <li><p>Distance based method</p></li> <li><p>Empirical Risk Minimization</p></li> <li><p>Discriminative Approach: Fit a model <span class="math inline">ℙ̂(<em>Y</em>|<em>X</em>)</span> to approximate the conditional distribution <span class="math inline">ℙ(<em>Y</em>|<em>X</em>)</span>, add classify using: <span class="math display"><em>h</em>(<em>x</em>) = <em>a</em><em>r</em><em>g</em><em>m</em><em>a</em><em>x</em><sub><em>y</em></sub>ℙ̂(<em>Y</em>=<em>y</em>|<em>X</em>=<em>x</em>)</span></p></li> <li><p>Generative Approach: Fit a model <span class="math inline">ℙ̂(<em>X</em>,<em>Y</em>)</span> to approximate the joint distribution <span class="math inline">ℙ(<em>X</em>,<em>Y</em>)</span>, add classify using: <span class="math display"><em>h</em>(<em>x</em>) = <em>a</em><em>r</em><em>g</em><em>m</em><em>a</em><em>x</em><sub><em>y</em></sub><em>P</em>(<em>X</em>=<em>x</em>,<em>Y</em>=<em>y</em>) = <em>P</em>(<em>X</em>=<em>x</em>|<em>Y</em>=<em>y</em>)<em>P</em>(<em>Y</em>=<em>y</em>)</span></p></li> </ul> <h1 id="discriminative-vs.-generative"> <a href="#discriminative-vs.-generative" class="anchor-heading" aria-labelledby="discriminative-vs.-generative"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> Discriminative vs. Generative </h1> <p>The discriminative and the generative methods are two approaches to approximate the unknown underlying true distribution, and they are related by the Bayes rule <span class="math display"><em>P</em>(<em>X</em>,<em>Y</em>) = <em>P</em>(<em>X</em>|<em>Y</em>)<em>P</em>(<em>Y</em>) = <em>P</em>(<em>Y</em>|<em>X</em>)<em>P</em>(<em>X</em>)</span>. Concretely, the generative approach learn what the individual classes looks like and models the data distribution <span class="math inline"><em>P</em>(<em>X</em>)</span>. It is a potentially harder problem and computationally more challenging. But the advantage of this approach is that it can be used to sample new data.</p> <p>On the other hand, the discriminative approach learn the boundary between classes and models <span class="math inline"><em>P</em>(<em>Y</em>|<em>X</em>)</span>, the conditional distribution and ignores <span class="math inline"><em>P</em>(<em>X</em>)</span>. It solves a potentially easier problem and computationally simpler. However, it cannot be used to sample new data.</p> <div class="center"> </div> <h1 id="discrimitive-classifiers-logistic-regression"> <a href="#discrimitive-classifiers-logistic-regression" class="anchor-heading" aria-labelledby="discrimitive-classifiers-logistic-regression"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> Discrimitive Classifiers: Logistic Regression </h1> <p>Logistic Regression can be viewed as an approach of fitting a discriminative model. It assumes a parametric form of the conditional distribution <span class="math inline"><em>P</em>(<em>Y</em>|<em>X</em>)</span> as <span class="math display">$$P(Y=1|X=x;w) = \frac{e^{w^Tx}}{1+e^{w^Tx}} = \sigma (w^Tx)$$</span> where <span class="math inline">$\sigma(z) = \frac{1}{1+e^{-z}}$</span> and is often called the Sigmoid function. Therefore, <span class="math display">$$P(Y=0|X=x;w) = 1- \frac{e^{w^Tx}}{1+e^{w^Tx}} = 1- \sigma (w^Tx)$$</span> In the training process, we are given a training data set <span class="math inline"><em>S</em> = {(<em>x</em><sub>1</sub>,<em>y</em><sub>1</sub>), ..., (<em>x</em><sub><em>N</em></sub>,<em>y</em><sub><em>N</em></sub>)}</span> to estimate the weights <span class="math inline"><em>w</em></span>. Naturally, we will apply the maximum likelihood method where we can write the likelihood of the training data, assuming i.i.d., as <span class="math display">$$\begin{aligned} P(S|w) &amp; = \prod_{i=1}^N P(y_i|x_i;w) \\ &amp; = \prod_{i=1}^N ( \sigma (w^Tx_i))^{y_i} (1-\sigma (w^Tx_i) ) ^{1-y_i} \end{aligned}$$</span> Equivalent to maximize <span class="math inline"><em>P</em>(<em>S</em>|<em>w</em>)</span>, we can minimize the negative log-likelihood of <span class="math inline"><em>S</em></span>, which is <span class="math display">$$\begin{aligned} L(w) &amp;= - \sum_{i=1}^N \log [(\sigma(w^Tx_i))^{y_i}(1-\sigma(w^Tx_i))^{1-y_i}] \\ &amp; = - \sum_{i=1}^N [y_i \log \sigma(w^Tx_i) + (1-y_i) \log (1-\sigma (w^Tx_i))] \end{aligned}$$</span> This is often called the cross-entropy error.</p> <div class="center"> </div> <h1 id="generative-classifiers-naive-bayes"> <a href="#generative-classifiers-naive-bayes" class="anchor-heading" aria-labelledby="generative-classifiers-naive-bayes"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> Generative Classifiers: Naive Bayes </h1> <p>Now we take a closer look at the generative approach. As said before, in the generative approach, we fit a model of the joint distribution <span class="math inline"><em>P</em>(<em>X</em>,<em>Y</em>)</span> and derived our classifier using the Bayes rule: <span class="math display">$$P(Y|X) = \frac{P(X|Y)P(Y)}{P(X)}$$</span> where <span class="math inline"><em>P</em>(<em>X</em>) = ∑<sub><em>Y</em></sub><em>P</em>(<em>X</em>|<em>Y</em>)<em>P</em>(<em>Y</em>)</span>. For classification, we have <span class="math display">$$\begin{aligned} h(x) &amp; = argmax_y P(Y=y|X=x)\\ &amp; = argmax_y \frac{P(X=x|Y=y)P(Y=y)}{P(X=x)} \\ &amp; = argmax_y P(X=x|Y=y)P(Y=y) \end{aligned}$$</span> The denominator <span class="math inline"><em>P</em>(<em>X</em>=<em>x</em>)</span> is only a normalization constant and thus can be ignored for deriving <span class="math inline"><em>a</em><em>r</em><em>g</em><em>m</em><em>a</em><em>x</em><sub><em>y</em></sub><em>P</em>(<em>Y</em>=<em>y</em>|<em>X</em>=<em>x</em>)</span>.</p> <p>Now let’s suppose both <span class="math inline"><em>X</em></span> and <span class="math inline"><em>Y</em></span> are discrete random variables, where <span class="math inline"><em>X</em> ∈ 𝒳<sup><em>d</em></sup></span> and <span class="math inline"><em>Y</em> ∈ 𝒴</span>. Then we have <span class="math display"><em>P</em>(<em>Y</em>=<em>y</em><sub><em>i</em></sub>|<em>X</em>=<em>x</em><sub><em>i</em></sub>) ∝ <em>P</em>(<em>X</em>=<em>x</em><sub><em>k</em></sub>|<em>Y</em>=<em>y</em><sub><em>i</em></sub>)<em>P</em>(<em>Y</em>=<em>y</em><sub><em>i</em></sub>)</span> Remember <span class="math inline"><em>X</em></span> is a <span class="math inline"><em>d</em></span> dimensional random variable, so to fully express this conditional distribution, we will approximately need <span class="math inline">|𝒳|<sup><em>d</em></sup>|𝒴| + |𝒴|</span> parameters. As a simple example, suppose <span class="math inline">𝒳 = 𝒴 = {0, 1}</span>, then <span class="math inline"><em>X</em> ∈ {0, 1}<sup><em>d</em></sup></span>. To specify <span class="math inline"><em>P</em>(<em>X</em>=<em>x</em><sub><em>k</em></sub>|<em>Y</em>=0)</span>, we need <span class="math inline">2<sup><em>d</em></sup></span> parameters. Unless <span class="math inline"><em>d</em></span> is sufficiently small, this full distribution is usually not computational tractable.</p> <h2 id="the-naive-bayes-assumption"> <a href="#the-naive-bayes-assumption" class="anchor-heading" aria-labelledby="the-naive-bayes-assumption"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> The Naive Bayes Assumption </h2> <p>To make the computation tractable and make the problem simpler, the Naive Bayes model make a strong assumption that the <span class="math inline"><em>d</em></span> features are conditional independent of each other given the class label <span class="math inline"><em>Y</em></span>. Therefore, <span class="math display">$$\begin{aligned} P(X|Y) &amp;= P(X_1,X_2,...,X_d|Y) \\ &amp;=P(X_1|X_2,...X_d|Y)P(X_2|X_3,...,X_d|Y)...P(X_d|Y) \\ &amp;=P(X_1|Y)P(X_2|Y)...P(X_d|Y) \end{aligned}$$</span> where the second ‘<span class="math inline">=</span>’ used the conditional independence. Therefore, our classifier becomes <span class="math display">$$argmax_y P(Y=y|X_1,...,X_d) \propto P(Y=y)\prod_{j=1}^d P(X_j|Y=y)$$</span></p> <div class="center"> </div> <p>Now, using maximum likelihood on training data <span class="math inline"><em>S</em></span>, we can estimate the parameters for the Naive Bayes classifier, assuming a specific distribution for <span class="math inline"><em>P</em>(<em>X</em><sub><em>j</em></sub>|<em>Y</em>)</span>, <span class="math inline">1 ≤ <em>j</em> ≤ <em>d</em></span>. We can show for a multinomial distribution of <span class="math inline"><em>P</em>(<em>X</em><sub><em>j</em></sub>|<em>Y</em>)</span>, the parameters of Naive Bayes are <span class="math display">$$\begin{aligned} P(Y=y) &amp;= \frac{\# \text{Example with }Y = y}{N} \\ P(X_i=x|Y=y) &amp;= \frac{\# \text{Example with} X_i = x \text{ and } Y=y}{\# \text{Example with }Y = y} \end{aligned}$$</span></p> <div class="center"> </div> <h2 id="naive-bayes-can-be-a-linear-classifier"> <a href="#naive-bayes-can-be-a-linear-classifier" class="anchor-heading" aria-labelledby="naive-bayes-can-be-a-linear-classifier"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> Naive Bayes can be a linear classifier </h2> <p>We can show that under some common assumptions of <span class="math inline"><em>P</em>(<em>X</em><sub><em>j</em></sub>|<em>Y</em>)</span>, the Naive Bayes classifier is actually a linear classifier. Here we provide a proof for <span class="math inline"><em>X</em><sub><em>j</em></sub> ∈ 0, 1</span> and <span class="math inline"><em>P</em>(<em>X</em><sub><em>j</em></sub>|<em>Y</em>)</span> is a Bernoulli distribution. (See Exercise 2 Problem 2 for other more general situations.)</p> <div id="thm:nb_linear" class="theorem"> <p><strong>Theorem 1</strong> (). <em>Let <span class="math inline"><em>X</em> ∈ {0, 1}<sup><em>d</em></sup></span>, and <span class="math inline"><em>P</em>(<em>X</em><sub><em>j</em></sub>|<em>Y</em>)</span>,<span class="math inline">1 ≤ <em>j</em> ≤ <em>d</em></span> is a Bernoulli distribution. The Naive Bayes classifier is defined by <span class="math display"><em>h</em>(<em>x</em>) = <em>s</em><em>g</em><em>n</em>(<em>w</em><sup><em>T</em></sup><em>x</em>+<em>w</em><sub>0</sub>)</span> for a suitable choice of <span class="math inline"><em>w</em></span>,<span class="math inline"><em>w</em><sub>0</sub></span>.</em></p> </div> <div class="proof"> <p><em>Proof.</em> As <span class="math inline"><em>X</em><sub><em>j</em></sub> ∈ {0, 1}</span>, the Bernoulli distribution is therefore <span class="math display">$$\begin{aligned} P(X_j|Y=1) &amp; = a_j^{X_j} (1-a_j)^{(1-X_j)} \\ P(X_j|Y=0) &amp; = b_j^{X_j} (1-b_j)^{(1-X_j)} \end{aligned}$$</span> where <span class="math inline"><em>a</em><sub><em>j</em></sub></span> and <span class="math inline"><em>b</em><sub><em>j</em></sub></span> are parameters for the <span class="math inline"><em>j</em></span>th dimension of <span class="math inline"><em>X</em></span>.</p> <p>With the conditional independence, we have <span class="math display">$$\begin{aligned} P(Y=1|X) &amp; =\frac{P(X|Y=1)P(Y=1)}{P(X|Y=1)P(Y=1)+P(X|Y=0)P(Y=0)} \\ &amp; =\frac{1}{1+\frac{P(X|Y=0)P(Y=0)}{P(X|Y=1)P(Y=1)}} \\ &amp; = \frac{1}{1+\exp (- \log \frac{P(X|Y=1)P(Y=1)}{P(X|Y=0)P(Y=0)})} \\ &amp; =\sigma \left( \sum_j^d \log \frac{P(X_j|Y=1)}{P(X_j|Y=0)} + \log \frac{P(Y=1)}{P(Y=0)} \right) \end{aligned}$$</span> and therefore, <span class="math display">$$\begin{aligned} P(Y=1|X) &amp; = \sigma \left( \sum_j^d \log \frac{a_j^{X_j}(1-a_j)^{(1-X_j)}}{b_j^{X_j}(1-b_j)^{(1-X_j)}} + \log \frac{p}{1-p} \right) \\ &amp; = \sigma \left( \sum_j^d \left(X_j \log \frac{a_j(1-b_j)}{b_j(1-a_j)}\right) + \log \left(\frac{p}{1-p} \prod_j^d \frac{1-a_j}{1-b_j} \right) \right) \\ &amp; = \sigma \left( \sum_j^d w_j X_j + w_0\right) \end{aligned}$$</span> where <span class="math inline">$w_j = \log \frac{a_j(1-b_j)}{b_j(1-a_j)}$</span> and <span class="math inline">$w_0=\log \left(\frac{p}{1-p} \prod_j^d \frac{1-a_j}{1-b_j} \right)$</span>. Therefore, <span class="math display"><em>h</em>(<em>x</em>) = <em>s</em><em>g</em><em>n</em>(<em>w</em><sup><em>T</em></sup><em>x</em>+<em>w</em><sub>0</sub>)</span></p> <p>and this shows the Naive Bayes is a linear classifier. ◻</p> </div> <h1 id="naive-bayes-vs.-logistic-regression"> <a href="#naive-bayes-vs.-logistic-regression" class="anchor-heading" aria-labelledby="naive-bayes-vs.-logistic-regression"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> Naive Bayes vs. Logistic Regression </h1> <p>We saw in the last section that Naive Bayes can be a linear classifier under some assumptions, but some of the assumptions are quite strong, such as the conditional independence. Thus, the hypothesis class of Naive Bayes is not all the possible linear classifiers, but only a subset of them. We know the Logistic Regression is another common linear classifier, how does Naive Bayes compare to Logistic Regression? In this section, we will look at them more closely through a theoretic view.</p> <h2 id="asymptotic-regime"> <a href="#asymptotic-regime" class="anchor-heading" aria-labelledby="asymptotic-regime"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> Asymptotic Regime </h2> <p>We first define certain notations for our discussion, let <span class="math inline"><em>ϵ</em>(<em>h</em><sub><em>A</em></sub>,<em>N</em>) ≡ <em>L</em>(<em>h</em><sub><em>A</em></sub>,<em>S</em>)</span>, where <span class="math inline">|<em>S</em>| = <em>N</em></span>. Here, <span class="math inline"><em>ϵ</em>(<em>h</em><sub><em>A</em></sub>,<em>N</em>)</span> stands for the error of hypothesis <span class="math inline"><em>h</em></span> trained using algorithm <span class="math inline"><em>A</em></span> from <span class="math inline"><em>N</em></span> observations. We will first discuss in the asymptotic setting, which means the number of training data points is infinity.</p> <p>If the two classes is linear separable, which means both models are correct, then we have <span class="math display"><em>ϵ</em>(<em>h</em><sub><em>N</em><em>B</em></sub>,∞) = <em>ϵ</em>(<em>h</em><sub><em>L</em><em>R</em></sub>,∞)</span> which means, asymptotically, NB and LR produce the identical classifier.</p> <p>If the linear assumption does not hold, which is usually the case, we claim LR is expected to outperform NB, i.e. <span class="math display"><em>ϵ</em>(<em>h</em><sub><em>L</em><em>R</em></sub>,∞) ≤ <em>ϵ</em>(<em>h</em><sub><em>N</em><em>B</em></sub>,∞)</span> Intuitively, this can be shown by observing that, since Logistic Regression assumes no other assumption other than linear classification, <span class="math inline"><em>ϵ</em>(<em>h</em><sub><em>L</em><em>R</em></sub>,∞)</span> converges to <span class="math inline"><em>i</em><em>n</em><em>f</em><sub><em>h</em> ∈ ℋ</sub><em>L</em>(<em>h</em>)</span>, where <span class="math inline">ℋ</span> is the class of all linear classifiers, it must therefore be asymptotically no worse than the linear classifier picked by Naive Bayes, which assumes conditional independence between features.</p> <h2 id="non-asymptotic-regime"> <a href="#non-asymptotic-regime" class="anchor-heading" aria-labelledby="non-asymptotic-regime"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> Non-asymptotic Regime </h2> <p>In a more real setting, where we do not have infinite number of training data, we must talk about how fast (how many training sample needed) an estimator converges to its asymptotic limit. This ‘rate of convergence’ for Logistic Regression is as below</p> <div id="thm:LR_converge" class="theorem"> <p><strong>Theorem 2</strong> (). <em>Let <span class="math inline"><em>h</em><sub><em>L</em><em>R</em>, <em>N</em></sub></span> be a Logistic Regression model in <span class="math inline"><em>d</em></span> dimensions. Then, with high probability, <span class="math display">$$\epsilon(h_{LR,N} \leq \epsilon(h_{LR},\infty)) + O \left( \sqrt{\frac{d}{N} \log \frac{N}{d}} \right)$$</span> Thus, for <span class="math inline"><em>ϵ</em>(<em>h</em><sub><em>L</em><em>R</em></sub>,<em>N</em>) ≤ <em>ϵ</em>(<em>h</em><sub><em>L</em><em>R</em></sub>,∞) + <em>ϵ</em><sub>0</sub></span> to hold up for a fixed constant <span class="math inline"><em>ϵ</em><sub>0</sub></span>, it suffices to pick <span class="math inline"><em>N</em> = <em>Ω</em>(<em>d</em>)</span></em></p> </div> <p>The proof of theorem <a href="#thm:LR_converge" data-reference-type="ref" data-reference="thm:LR_converge">2</a> follows the application of the uniform convergence bounds to logistic regression, and using the fact that the <span class="math inline"><em>d</em></span> dimensional linear classifier <span class="math inline">ℋ</span> has a VC dimension of <span class="math inline"><em>d</em> + 1</span>. These concepts will be covered in Lecture 5.</p> <p>Now, we want to draw a similar picture for the Naive Bayes classifier and compare it the Logistic Regression. It turns out this is a more challenging task and we will break our analysis into two parts</p> <ul> <li><p>How fast do parameters of NB converge to their optimal values</p></li> <li><p>How fast do the risk if NB converge to the asymptotic risk</p></li> </ul> <p>For the first part, formally, we have the following theroem</p> <div id="thm:NB_convergence" class="theorem"> <strong>Theorem 3</strong> (). *Let any <span class="math inline"><em>ϵ</em><sub>1</sub></span>, <span class="math inline"><em>d</em><em>e</em><em>l</em><em>t</em><em>a</em> &gt; 0</span> and any <span class="math inline"><em>l</em> ≤ 0</span> be fixed. Assume that for some fixed <span class="math inline"><em>ρ</em><sub>0</sub> &gt; 0</span>, we hvae <span class="math inline"><em>ρ</em><sub>0</sub> ≤ <em>P</em>(<em>y</em>=1) ≤ 1 − <em>ρ</em><sub>0</sub></span>. Let <span class="math inline"><em>d</em> = <em>O</em>((1/<em>ϵ</em><sub>1</sub><sup>2</sup>)log(<em>d</em>/<em>δ</em>))</span>, then with probability at least <span class="math inline">1 − <em>δ</em></span>: $$ <span class="math display"><em>a</em><em>n</em><em>d</em></span> <p>$$ for all <span class="math inline"><em>j</em> = 1, ..., <em>d</em></span> and <span class="math inline"><em>b</em> ∈ 𝒴</span>*</p> </div> <p>This theorem states that with a number of samples that is only <em>logarithmic</em>, rather than linear, in <span class="math inline"><em>d</em></span>, the parameters of Naive Bayes are uniformly close to their asymptotic values in <span class="math inline"><em>h</em><sub><em>N</em><em>B</em>, ∞</sub></span>.</p> <p>Proof of this theorem is a straightforward application of the Hoeffding bound, here we provide a simple setting where <span class="math inline"><em>X</em> ∈ {0, 1}</span> and <span class="math inline"><em>P</em>(<em>X</em>=1) = <em>p</em></span>. Suppose we have <span class="math inline"><em>N</em></span> i.i.d. samples <span class="math inline">(<em>x</em><sub>1</sub>,...,<em>x</em><sub><em>N</em></sub>)</span>, then the maximum likelihood estimation for <span class="math inline"><em>p</em></span> is simply <span class="math display">$$\hat{p} = \frac{1}{N}\sum_i x_i$$</span> The Hoeffding bound for this case states, for all <span class="math inline"><em>p</em> ∈ [0,1]</span>, <span class="math inline"><em>ϵ</em> &gt; 0</span> <span class="math display"><em>P</em>(|<em>p</em>−<em>p̂</em>|&gt;<em>ϵ</em>) ≤ 2<em>e</em><sup>−2<em>N</em><em>ϵ</em><sup>2</sup></sup></span> Intuitively, this means that the probability of the empirical estimation being epsilon-far from the ground truth decays exponentially fast with the number of samples <span class="math inline"><em>N</em></span>. For a detailed proof of theorem <a href="#thm:NB_convergence" data-reference-type="ref" data-reference="thm:NB_convergence">3</a>, please look at paper <a href="#Ng" data-reference-type="ref" data-reference="Ng">[Ng]</a>.</p> <p>Now we know the parameters of Naive Bayes converge logarithmically to its optimal values, but this doesn’t directly imply the error of Naive Bayes also converges with this rate. To intuitively show why the error also converges, we can first show that the convergence of the parameters implies that <span class="math inline"><em>h</em><sub><em>N</em><em>B</em>, <em>N</em></sub></span> is very likely to make the same predictions as <span class="math inline"><em>h</em><sub><em>N</em><em>B</em>, ∞</sub></span>. Recall that <span class="math inline"><em>h</em><sub><em>N</em><em>B</em></sub>(<em>x</em>)</span> makes its predictions according to <span class="math display">$$\ell_{NB}(x)=\log \frac{\hat{P}(Y=1)\prod_j \hat{P}(x_j|Y=1)}{\hat{P}(Y=0)\prod_j \hat{P}(x_j|Y=0)} &gt; 0$$</span> For every example for which both <span class="math inline">ℓ<sub><em>N</em><em>B</em></sub>(<em>x</em>)</span> and <span class="math inline">ℓ<sub><em>N</em><em>B</em>, ∞</sub>(<em>x</em>)</span> have the same sign, <span class="math inline"><em>h</em><sub><em>N</em><em>B</em>, <em>N</em></sub></span> and <span class="math inline"><em>h</em><sub><em>N</em><em>B</em>, ∞</sub></span> will make the same prediction. If <span class="math inline"><em>h</em><sub><em>N</em><em>B</em>, ∞</sub> ≫ 0</span> or <span class="math inline"><em>h</em><sub><em>N</em><em>B</em>, ∞</sub> ≪ 0</span>, as <span class="math inline">ℓ<sub><em>N</em><em>B</em></sub>(<em>x</em>)</span> is only a small perturbation of <span class="math inline">ℓ<sub><em>N</em><em>B</em>, ∞</sub>(<em>x</em>)</span>, they will be on the same side of <span class="math inline">0</span> with high probability.</p> <p>With some steps of derivation, we will eventually reach the formal risk convergence of Naive Bayes</p> <div id="thm:NB_risk_convergence" class="theorem"> <p><strong>Theorem 4</strong> (). <em>Define <span class="math inline"><em>G</em>(<em>τ</em>) = <em>P</em><sub>(<em>x</em>,<em>y</em>) ∼ ℙ</sub>[(ℓ<sub><em>N</em><em>B</em>, ∞</sub>(<em>x</em>)∈[0,<em>τ</em><em>d</em>]∧<em>y</em>=1)∨(ℓ<sub><em>N</em><em>B</em>, ∞</sub>(<em>x</em>)∈[−<em>τ</em><em>d</em>,0]∧<em>y</em>=0)]</span>. Assume that for some fixed <span class="math inline"><em>ρ</em><sub>0</sub> &gt; 0</span>, we have <span class="math inline"><em>ρ</em><sub>0</sub> ≤ <em>P</em>(<em>y</em>=1) ≤ 1 − <em>ρ</em><sub>0</sub>)</span>, and that <span class="math inline"><em>ρ</em><sub>0</sub> ≤ <em>P</em>(<em>x</em><sub><em>j</em></sub>=1|<em>y</em>=<em>b</em>) ≤ 1 − <em>ρ</em><sub>0</sub>)</span> for all <span class="math inline"><em>j</em></span>,<span class="math inline"><em>b</em></span>, then with high probability, <span class="math display">$$\epsilon(h_{NB,N}) \leq \epsilon(h_{NB, \infty}) + G\left(O\left(\sqrt{\frac{1}{N}\log d}\right)\right)$$</span></em></p> </div> <p>Here, <span class="math inline"><em>G</em></span> defines the fraction of points that are very close to the decision boundary. Intuitively, if we can understand and control the event <span class="math inline"><em>G</em>(<em>τ</em>)</span>, then we can obtain a more precise control on the bound of the error.</p> <div id="thm:NB_risk_convergence_final" class="theorem"> <p><strong>Theorem 5</strong> (). <em>Let the conditions of Theorem <a href="#thm:NB_risk_convergence" data-reference-type="ref" data-reference="thm:NB_risk_convergence">4</a> hold, and suppose <span class="math inline"><em>G</em>(<em>τ</em>) ≤ <em>ϵ</em>/2 + <em>F</em>(<em>τ</em>)</span>, for some function <span class="math inline"><em>F</em>(<em>τ</em>)</span> that statisfies <span class="math inline"><em>F</em>(<em>τ</em>) → 0</span> as <span class="math inline"><em>τ</em> → 0</span>, and some fixed <span class="math inline"><em>ϵ</em><sub>0</sub> &gt; 0</span>. Then for <span class="math inline"><em>ϵ</em>(<em>h</em><sub><em>N</em><em>B</em>, <em>N</em></sub>) ≤ <em>ϵ</em>(<em>h</em><sub><em>N</em><em>B</em>, ∞</sub>) + <em>ϵ</em><sub>0</sub></span> to hold with high probability, it suffices to pick <span class="math inline"><em>N</em> = <em>Ω</em>(log<em>d</em>)</span></em></p> </div> <p>Thus, we can conclude that though the asymptotic error of Naive Bayes is greater than Logistic Regression, the convergence rate of NB is only <span class="math inline"><em>Ω</em>(log<em>d</em>)</span>, which is faster than <span class="math inline"><em>Ω</em>(<em>d</em>)</span> of Logistic Regression.</p> </main> <hr> <footer> <p><a href="#top" id="back-to-top">Back to top</a></p> <div class="d-flex mt-2"> <p class="text-small text-grey-dk-000 mb-0"> <a href="https://shenshen.mit.edu/git/shensquared/gradml/tree/main/supervised/DiscriminativeGenerative.md" id="edit-this-page">Edit this page on Git</a> | This page is last updated on 16-Jun-2023 </p> </div> </footer> </div> </div> <div class="search-overlay"></div> </div> </body> </html> <script src="https://hypothes.is/embed.js" async></script>
