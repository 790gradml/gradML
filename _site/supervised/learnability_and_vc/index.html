<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>Learnability and VC Dimension | 6.790 Machine Learning</title> <meta name="generator" content="Jekyll v4.3.2" /> <meta property="og:title" content="Learnability and VC Dimension" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="Graduate Machine Learning Course, MIT EECS, Fall 2023." /> <meta property="og:description" content="Graduate Machine Learning Course, MIT EECS, Fall 2023." /> <link rel="canonical" href="https://gradml.mit.edu/supervised/learnability_and_vc/" /> <meta property="og:url" content="https://gradml.mit.edu/supervised/learnability_and_vc/" /> <meta property="og:site_name" content="6.790 Machine Learning" /> <meta property="og:type" content="website" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="Learnability and VC Dimension" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","dateModified":"2023-06-16T05:04:41-04:00","description":"Graduate Machine Learning Course, MIT EECS, Fall 2023.","headline":"Learnability and VC Dimension","url":"https://gradml.mit.edu/supervised/learnability_and_vc/"}</script> <!-- End Jekyll SEO tag --> <!-- Copied from https://katex.org/docs/browser.html#starter-template --> <link rel="stylesheet" href="/assets/js/katex.min.css"> <!-- The loading of KaTeX is deferred to speed up page rendering --> <script defer src="/assets/js/katex.min.js"> </script> <!-- Automatically display code inside script tags with type=math/tex using KaTeX --> <script defer src="/assets/js/mathtex-script-type.js"> </script> <!-- To automatically render math in text elements, include the auto-render extension: --> <script defer src="/assets/js/auto-render.min.js" onload="renderMathInElement(document.body, { globalGroup: true, trust: true, strict: false, throwOnError: false, });"></script> <!-- The KaTeX default is 1.21em, see https://katex.org/docs/font.html#font-size-and-lengths --> <style> .katex { font-size: 1.21em; } </style> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"> 6.790 Machine Learning </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"> <use xlink:href="#svg-menu"></use> </svg> </a> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><a href="/" class="nav-list-link">About</a></li><li class="nav-list-item"><a href="/calendar/" class="nav-list-link">Syllabus / Calendar</a></li><li class="nav-list-item"><a href="/schedule/" class="nav-list-link">Weekly Schedule</a></li><li class="nav-list-item"><a href="/intro/" class="nav-list-link">Introduction</a></li><li class="nav-list-item"><a href="/review/" class="nav-list-link">Background/Review</a></li><li class="nav-list-item active"><button class="nav-list-expander btn-reset" aria-label="toggle items in Supervised Learning category" aria-pressed="true"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/supervised/" class="nav-list-link">Supervised Learning</a><ul class="nav-list"><li class="nav-list-item "><a href="/supervised/classification_foundation/" class="nav-list-link">Classification Foundation</a></li><li class="nav-list-item "><a href="/supervised/DiscriminativeGenerative/" class="nav-list-link">Discriminative vs Generative Classification</a></li><li class="nav-list-item active"><a href="/supervised/learnability_and_vc/" class="nav-list-link active">Learnability and VC Dimension</a></li><li class="nav-list-item "><a href="/supervised/linearRegression/" class="nav-list-link">Linear Regression</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Unsupervised Learning category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/unsupervised/" class="nav-list-link">Unsupervised Learning</a><ul class="nav-list"><li class="nav-list-item "><button class="nav-list-expander btn-reset" aria-label="toggle items in Graphical Models category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/unsupervised/graphical/" class="nav-list-link">Graphical Models</a><ul class="nav-list"></ul></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Reinforcement Learning category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/reinforcement/" class="nav-list-link">Reinforcement Learning</a><ul class="nav-list"><li class="nav-list-item "><a href="/reinforcement/mdp/" class="nav-list-link">Markov Decision Process</a></li><li class="nav-list-item "><a href="/reinforcement/value_bellman/" class="nav-list-link">Value functions and Bellman</a></li><li class="nav-list-item "><a href="/reinforcement/bandit/" class="nav-list-link">Bandits</a></li><li class="nav-list-item "><a href="/reinforcement/policy_gradient/" class="nav-list-link">Policy Gradient</a></li><li class="nav-list-item "><a href="/reinforcement/deepRL/" class="nav-list-link">Deep Reinforcement Learning</a></li></ul></li></ul> </nav> <footer class="site-footer"> <a href="/credit">Acknowledgement</a> </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search 6.790 Machine Learning" aria-label="Search 6.790 Machine Learning" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> <nav aria-label="Auxiliary" class="aux-nav"> <ul class="aux-nav-list"> <li class="aux-nav-list-item"> <a href="https://canvas.mit.edu/" class="site-button" target="_blank" rel="noopener noreferrer" > Canvas </a> </li> <li class="aux-nav-list-item"> <a href="https://piazza.com" class="site-button" target="_blank" rel="noopener noreferrer" > Piazza </a> </li> </ul> </nav> </div> <div id="main-content-wrap" class="main-content-wrap"> <nav aria-label="Breadcrumb" class="breadcrumb-nav"> <ol class="breadcrumb-nav-list"> <li class="breadcrumb-nav-list-item"><a href="/supervised/">Supervised Learning</a></li> <li class="breadcrumb-nav-list-item"><span>Learnability and VC Dimension</span></li> </ol> </nav> <div id="main-content" class="main-content"> <main> <h1 id="overview"> <a href="#overview" class="anchor-heading" aria-labelledby="overview"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> Overview </h1> <p>In this lecture, we will discuss learnability and show when the Empirical Risk Minimization methods that we’ve introduced will succeed. We will start our discussion from simple settings under strong assumptions, realizability and infinite hypothesis class. We will define a formal PAC-Learnability and generalize our conclusion to settings where realizability does not hold and hypothesis class is infinite. The concept of VC-dimension will be introduced to quantify the power of infinite hypothesis classes. Finally, we will reach the fundamental theorem of statistical learning.</p> <h1 id="motivation"> <a href="#motivation" class="anchor-heading" aria-labelledby="motivation"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> Motivation </h1> <p>In our previous discussion, we introduced the empirical risk minimization (ERM) method as a approximation to the true risk minimization and we also introduced different ways to restrict our hypothesis class so that our ERM found classifiers can have better performance on unseen data. We saw a theoretic analysis on Logistic Regression and Naive Bayes last lecture and in this lecture, we want to generalize the discussion to the entire ERM methods and try to answer some fundamental questions related to the concept of learning. First of all, we haven’t had a clear definition of what does it mean to be able to learn. We also want to ask, what hypothesis class should we choose and what limitations do different hypothesis class have. Further more, given a hypothesis class, we would like to discuss and determine what kinds of learning rules should we use, and how many data points do we need to learn a good model. With these question in head, we will start our discussion from simple settings and then try to generalize our conclusions.</p> <h1 id="learnability"> <a href="#learnability" class="anchor-heading" aria-labelledby="learnability"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> Learnability </h1> <h2 id="realizability"> <a href="#realizability" class="anchor-heading" aria-labelledby="realizability"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> Realizability </h2> <p>We start our discussion with a simplifying assumption, realizability. Formally, realizability means that there exists an optimal hypothesis <span class="math inline"><em>h</em><sup>*</sup> ∈ ℋ</span> such that the true risk <span class="math inline"><em>L</em><sub>ℙ</sub>(<em>h</em><sup>*</sup>) = 0</span>. This is a strong assumption and it implies that with probability <span class="math inline">1</span>, over random samples <span class="math inline"><em>S</em> ∼ ℙ</span>, <span class="math inline"><em>L</em><sub><em>S</em></sub>(<em>h</em><sup>*</sup>) = 0</span></p> <div class="center"> </div> <p>However, this strong assumption only implies the existence of such a hypothesis that can give <span class="math inline">0</span> error, it is not guaranteed that the ERM found hypothesis <span class="math inline"><em>h</em><sub><em>S</em></sub></span> is the optimal hypothesis <span class="math inline"><em>h</em><sup>*</sup></span>. The realizability assumption makes sure the richness of our hypothesis class, so that we don’t need to worry about underfitting, but we can still be overfitting by only minimizing the empirical error. Thus, we want to further discuss that under this assumption, what is the risk of the ERM hypothesis <span class="math inline"><em>h</em><sub><em>S</em></sub></span> on the unseen data and can this risk be bounded such that we are guaranteed to find a good hypothesis?</p> <h2 id="epsilon---delta-parameters"> <a href="#epsilon---delta-parameters" class="anchor-heading" aria-labelledby="epsilon---delta-parameters"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> <span class="math inline"><em>ϵ</em></span> - <span class="math inline"><em>δ</em></span> Parameters </h2> <p>To quantitively measure how good our hypothesis is, we introduce the <span class="math inline"><em>ϵ</em></span> and <span class="math inline"><em>δ</em></span> parameters for our discussion. The <span class="math inline"><em>ϵ</em></span> parameter is called <em>accuracy paramter</em> and is used to quantify the quality of the prediction. Concretely, we interpret the event <span class="math inline"><em>L</em><sub>ℙ</sub>(<em>h</em><sub><em>s</em></sub>) &gt; <em>ϵ</em></span> as a failure of the learner, while if <span class="math inline"><em>L</em><sub>ℙ</sub>(<em>h</em><sub><em>S</em></sub>) ≤ <em>ϵ</em></span>, we view the output of the ERM as an approximately correct hypothesis.</p> <p>However, this single parameter is not enough because <span class="math inline"><em>h</em><sub><em>S</em></sub></span> depends on the training set <span class="math inline"><em>S</em></span>, and the training set is picked by a random process so that there is randomness in the result of the ERM. It is not realistic to expect that with full certainty <span class="math inline"><em>S</em></span> will suffice to direct the learner toward a good hypothesis, as there is always some probability that the sampled training data happens to be very non-representative of the underlying distribution <span class="math inline">ℙ</span>. We therefore denote the probability of getting a non-representative sample by <span class="math inline"><em>δ</em></span>, and call <span class="math inline">(1−<em>δ</em>)</span> the <em>confidence parameter</em> of our prediction.</p> <h2 id="finite-hypothesis-class"> <a href="#finite-hypothesis-class" class="anchor-heading" aria-labelledby="finite-hypothesis-class"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> Finite Hypothesis Class </h2> <p>To bound the error of the ERM hypothesis <span class="math inline"><em>h</em><sub><em>S</em></sub></span>, we further introduce some restrictions on the hypothesis class <span class="math inline">ℋ</span> so that we can prevent overfitting. The simplest type of restriction on a class is imposing an upper bound on its size, that is, the hypothesis class <span class="math inline">ℋ</span> has a finite cardinality. With this additional assumption, we can show that the ERM hypothesis will not overfit, i.e., have a bounded error on unseen data.</p> <div id="finite_class" class="theorem"> <p><strong>Theorem 1</strong> (). <em>Let <span class="math inline">ℋ</span> be finite. Let <span class="math inline"><em>δ</em> ∈ (0,1)</span>, <span class="math inline"><em>ϵ</em> &gt; 0</span> and <span class="math inline">$N \geq \frac{\log(|\mathcal{H}|/\delta)}{\epsilon}$</span>. Then, for any distribution <span class="math inline">ℙ</span> for which realizability holds, we probability at least <span class="math inline">1 − <em>δ</em></span> over the choice of dataset <span class="math inline"><em>S</em></span> of size <span class="math inline"><em>N</em></span>, every ERM hypothesis <span class="math inline"><em>h</em><sub><em>S</em></sub></span> satisfies <span class="math inline"><em>L</em><sub>ℙ</sub> ≤ <em>ϵ</em></span></em></p> </div> <div class="proof"> <p><em>Proof.</em> Let <span class="math inline">ℋ<sub><em>B</em></sub></span> be the set of ‘failed’ hypotheses, that is <span class="math display">ℋ<sub><em>B</em></sub> = {<em>h</em> ∈ ℋ : <em>L</em><sub>ℙ</sub>(<em>h</em>) &gt; <em>ϵ</em>}</span> In addition, let <span class="math inline"><em>M</em></span> be the set of misleading samples, that is <span class="math display"><em>M</em> = {<em>S</em> : ∃<em>h</em> ∈ ℋ<sub><em>B</em></sub>, <em>L</em><sub><em>S</em></sub>(<em>h</em>) = 0}</span> Namely, for every <span class="math inline"><em>S</em> ∈ <em>M</em></span>, there is a ‘failed’ hypothesis, <span class="math inline"><em>h</em> ∈ ℬ</span>, that looks like a ‘good’ hypothesis on <span class="math inline"><em>S</em></span>. Now, recall that we would like to bound the probability of the event <span class="math inline"><em>L</em><sub>ℙ</sub>(<em>h</em><sub><em>S</em></sub>) &gt; <em>ϵ</em></span>. Since the realizability implies that <span class="math inline"><em>L</em><sub><em>S</em></sub>(<em>h</em><sub><em>S</em></sub>) = 0</span>, it follows that the event <span class="math inline"><em>L</em><sub>ℙ</sub>(<em>h</em><sub><em>S</em></sub>) &gt; <em>ϵ</em></span> can only happen if for some <span class="math inline"><em>h</em> ∈ ℋ<sub><em>B</em></sub></span>, we have <span class="math inline"><em>L</em><sub><em>S</em></sub>(<em>h</em>) = 0</span>. In other words, the failure will only happen if our training data is in the set of misleading samples Set <span class="math inline"><em>M</em></span>. Formally, we have <span class="math display">{<em>S</em> : <em>L</em><sub>ℙ</sub>(<em>h</em><sub><em>S</em></sub>) &gt; <em>ϵ</em>} ⊆ <em>M</em></span> As we can write <span class="math inline"><em>M</em></span> as <span class="math display"><em>M</em> = ∪<sub><em>h</em> ∈ ℋ<sub><em>B</em></sub></sub>{<em>S</em> : <em>L</em><sub><em>S</em></sub>(<em>h</em>) = 0}</span> Hence, <span class="math display"><em>P</em>({<em>S</em>:<em>L</em><sub>ℙ</sub>(<em>h</em><sub><em>S</em></sub>)&gt;<em>ϵ</em>}) ≤ <em>P</em>(∪<sub><em>h</em> ∈ ℋ<sub><em>B</em></sub></sub>{<em>S</em>:<em>L</em><sub><em>S</em></sub>(<em>h</em>)=0})</span> Applying the union bound to the right-hand side yields <span class="math display"><em>P</em>({<em>S</em>:<em>L</em><sub>ℙ</sub>(<em>h</em><sub><em>S</em></sub>)) ≤ ∑<sub><em>h</em> ∈ ℋ<sub><em>B</em></sub></sub><em>P</em>({<em>S</em>:<em>L</em><sub><em>S</em></sub>(<em>h</em>)=0} )</span> Next, we can bound each summand of the right-hand side. Fix some ‘failed’ hypothesis <span class="math inline"><em>h</em> ∈ ℋ<sub><em>B</em></sub></span>, the event <span class="math inline"><em>L</em><sub><em>S</em></sub>(<em>h</em>) = 0</span> is equivalent to the event that in the training set, <span class="math inline">∀<em>i</em></span>, <span class="math inline"><em>h</em>(<em>x</em><sub><em>i</em></sub>) = <em>y</em><sub><em>i</em></sub></span>. Since the training data are i.i.d. sampled, we have <span class="math display">$${P} \left(\{S : L_{S}(h)=0\}\ \right) = \prod_{i=1}^N {P} \left( \{x_i: h(x_i) = y_i\} \right)$$</span> For each individual sampling of an element of the training set, we have <span class="math display"><em>P</em>({<em>x</em><sub><em>i</em></sub>:<em>h</em>(<em>x</em><sub><em>i</em></sub>)=<em>y</em><sub><em>i</em></sub>}) = 1 − <em>L</em><sub>ℙ</sub>(<em>h</em>) ≤ 1 − <em>ϵ</em></span> where the last inequality follows from the fact that <span class="math inline"><em>h</em> ∈ ℋ<sub><em>B</em></sub></span>. Using the inequality <span class="math inline">1 − <em>ϵ</em> ≤ <em>e</em><sup>−<em>ϵ</em></sup></span>, we have for every <span class="math inline"><em>h</em> ∈ ℋ<sub><em>B</em></sub></span>, <span class="math display"><em>P</em>(<em>S</em>:<em>L</em><sub><em>S</em></sub>(<em>h</em>)=0) ≤ (1−<em>ϵ</em>)<sup><em>N</em></sup> ≤ <em>e</em><sup>−<em>ϵ</em><em>N</em></sup></span> Therefore, we have <span class="math display"><em>P</em>(<em>S</em>:<em>L</em><sub>ℙ</sub>(<em>h</em><sub><em>S</em></sub>)&gt;<em>ϵ</em>) ≤ |ℋ<sub><em>B</em></sub>|<em>e</em><sup>−<em>ϵ</em><em>N</em></sup> ≤ |ℋ|<em>e</em><sup>−<em>ϵ</em><em>N</em></sup></span> Let <span class="math inline"><em>δ</em> = <em>P</em>(<em>S</em>:<em>L</em><sub>ℙ</sub>(<em>h</em><sub><em>S</em></sub>)&gt;<em>ϵ</em>)</span>, we will reach the desired conclusion that with probability at least <span class="math inline">1 − <em>δ</em></span>, and having <span class="math inline">$N \geq \frac{\log (|\mathcal{H}|/\delta)}{\epsilon}$</span>, <span class="math display"><em>L</em><sub>ℙ</sub>(<em>h</em><sub><em>S</em></sub>) ≤ <em>ϵ</em></span> ◻</p> </div> <p>A weaker result can be proved without realizability, see Exercise 2 for details.</p> <h2 id="pac-learnability"> <a href="#pac-learnability" class="anchor-heading" aria-labelledby="pac-learnability"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> PAC Learnability </h2> <p>We see that the finite hypothesis class makes it possible to bound the unseen error of ERM hypothesis. In order the generalize this result, we first give a formal name of such hypothesis classes. As we are using the <span class="math inline"><em>ϵ</em></span> and <span class="math inline"><em>δ</em></span> parameters which implies the conclusion is both approximate and not determined, we use the name <em>Probably approximately correct learnablity</em>, also known as <em>PAC-Learnability</em>. A formal definition is as follows,</p> <div class="definition"> <p><strong>Definition 2</strong> (PAC-Learnablity). <em>Assuming realizability, a hypothesis class <span class="math inline">ℋ</span> is PAC-learnable if there exists a function <span class="math inline"><em>N</em><sub>ℋ</sub>(<em>ϵ</em>,<em>δ</em>)</span> and a learning algorithm with the following property: For every <span class="math inline"><em>ϵ</em>, <em>δ</em> ∈ (0,1)</span> and every distribution <span class="math inline">ℙ</span>, training using <span class="math inline"><em>N</em> ≥ <em>N</em><sub>ℋ</sub>(<em>ϵ</em>,<em>δ</em>)</span> i.i.d. samples generated from <span class="math inline">ℙ</span>, the learning algorithm returns a hypothesis <span class="math inline"><em>h</em></span> such that <span class="math inline"><em>L</em><sub>ℙ</sub> ≤ <em>ϵ</em></span> with confidence <span class="math inline">(1−<em>δ</em>)</span> over choice of samples.</em></p> </div> <p>Informally, PAC-learnability of class <span class="math inline">ℋ</span> means that enough number of random examples drawn from the data distribution will allow approximate risk minimization, i.e., ensure <span class="math inline"><em>L</em><sub>ℙ</sub>(<em>h</em>) ≤ <em>ϵ</em></span> with probability <span class="math inline"> ≥ 1 − <em>δ</em></span>, where the number of samples needed depends on the desired tolerances <span class="math inline">(<em>ϵ</em>,<em>δ</em>)</span>.</p> <p>Note here <span class="math inline"><em>ϵ</em></span> and <span class="math inline"><em>δ</em></span> are inevitable. <span class="math inline"><em>δ</em></span> arises due to the randomness of training data <span class="math inline"><em>S</em></span> drawn from <span class="math inline">ℙ</span> and <span class="math inline"><em>ϵ</em></span> arises due to the actual hypothesis picked by the learner on the finite data <span class="math inline"><em>S</em></span>.</p> <p>With this formal concept of PAC-learnable defined, we can discuss the situations when our two assumptions on realizability and finite hypothesis class do not hold. Concretely, is the hypothesis class still learnable if realizability does not hold? And on the other hand, what about infinite hypothesis classes? Are they PAC-learnable?</p> <h2 id="agnostic-pac-learnability"> <a href="#agnostic-pac-learnability" class="anchor-heading" aria-labelledby="agnostic-pac-learnability"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> Agnostic PAC-Learnability </h2> <p>We first release the realizability assumption. By No-Free-Lunch (NFL) theorem, we know that no learner is guaranteed to match the Bayes classifier in general, as there’s always an adversarial distribution that can be constructed on which our learner fails while another may succeed. Thus, if the realizability does not hold, we don’t have the hope of satisfying <span class="math inline"><em>L</em><sub>ℙ</sub> ≤ <em>ϵ</em></span>. We now can only weaken our aim, and see if we can at least come <span class="math inline"><em>ϵ</em>−</span> close to the best possible classifier within our hypothesis class with high probability, i.e. <span class="math display"><em>L</em><sub>ℙ</sub>(<em>h</em><sub><em>S</em></sub>) ≤ inf<sub><em>h</em>′ ∈ ℋ</sub>ℙ(<em>h</em>′) + <em>ϵ</em></span> In this setting, the hypothesis class <span class="math inline">ℋ</span> may be bad, but we can still try to be approximately as good as the best possible hypothesis within this class. This weaker property is known as agnostic PAC-Learnability.</p> <div class="definition"> <p><strong>Definition 3</strong> (Agnostic PAC-Learnability). <em>A hypothesis class <span class="math inline">ℋ</span> is agnostic PAC learnable if there exist a function <span class="math inline"><em>N</em><sub>ℋ</sub> : (0,1)<sup>2</sup> → <em>N</em></span> and a learning algorithm with the following property: For every <span class="math inline"><em>ϵ</em>, <em>δ</em> ∈ (0,1)</span> and for every distribution <span class="math inline">ℙ</span> over <span class="math inline">𝒳 × 𝒴</span>, when running the learning algorithm on <span class="math inline"><em>N</em> &gt; <em>N</em><sub>ℋ</sub>(<em>ϵ</em>,<em>δ</em>)</span> i.i.d. samples generated by <span class="math inline">ℙ</span>, the algorithm returns a hypothesis <span class="math inline"><em>h</em></span> such that, with probability of at least <span class="math inline">1 − <em>δ</em></span> over the choice of the <span class="math inline"><em>N</em></span> training samples, <span class="math display"><em>L</em><sub>ℙ</sub>(<em>h</em>) ≤ inf<sub><em>h</em>′ ∈ ℋ</sub>ℙ(<em>h</em>′) + <em>ϵ</em></span></em></p> </div> <p>Clearly, if the realizability assumption holds, agnostic PAC-Learnability provides the same guarantee as PAC-Learnability. In that sense, agnostic PAC-Learnability generalizes the definition of PAC-Learnability. When the realizability assumption does not hold, no learner can guarantee an arbitrarily small error. Nevertheless, under the definition of agnostic PAC learning, a learner can still declare success if its error is not much larger than the best error achievable by a predictor from the class <span class="math inline">ℋ</span>. This is in contrast to PAC learning, in which the learner is required to achieve a small error in absolute terms and not relative to the best error achievable by the hypothesis class.</p> <p>Recall from lecture 2 where we decompose the error into the approximation error term and the estimation error term, where <span class="math display"><em>L</em><sub>ℙ</sub>(<em>h</em><sub><em>S</em></sub>) = <em>ϵ</em><sub><em>a</em><em>p</em><em>x</em></sub> + <em>ϵ</em><sub><em>e</em><em>s</em><em>t</em></sub></span> <span class="math display"><em>ϵ</em><sub><em>a</em><em>p</em><em>x</em></sub> := min<sub><em>h</em> ∈ ℋ</sub><em>L</em>(<em>h</em>)</span> <span class="math display"><em>ϵ</em><sub><em>e</em><em>s</em><em>t</em></sub> := <em>L</em><sub>ℙ</sub>(<em>h</em><sub><em>S</em></sub>) − <em>ϵ</em><sub><em>a</em><em>p</em><em>x</em></sub></span> As the approximation error depends on the fit of our prior knowledge via the inductive bias to the unknown underlying distribution, so it won’t be minimized further more after we’ve chosen the hypothesis class <span class="math inline">ℋ</span>. The agnostic PAC-Learnability loses the bound on this term but bound the estimation error uniformly over all distributions for a given hypothesis class.</p> <h2 id="uniform-convergence-implies-agnostic-pac-learnability"> <a href="#uniform-convergence-implies-agnostic-pac-learnability" class="anchor-heading" aria-labelledby="uniform-convergence-implies-agnostic-pac-learnability"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> Uniform Convergence implies agnostic PAC Learnability </h2> <p>How can we make sure the ERM solution is close the true risk? One strong assumption one can make is that <span class="math inline"><em>L</em><sub><em>S</em></sub>(<em>h</em>)</span> for all <span class="math inline"><em>h</em> ∈ ℋ</span> is close to the true risk <span class="math inline"><em>L</em><sub>ℙ</sub>(<em>h</em>)</span>, then the ERM solution <span class="math inline"><em>h</em><sub><em>S</em></sub></span> will also have small true risk <span class="math inline"><em>L</em><sub>ℙ</sub>(<em>h</em><sub><em>S</em></sub>)</span>. Hence, we introduce the notion of an <span class="math inline"><em>ϵ</em>−</span>representative data sample</p> <div class="definition"> <p><strong>Definition 4</strong> (<span class="math inline"><em>ϵ</em></span>-representative). <em>A dataset <span class="math inline"><em>S</em></span> is called <span class="math inline"><em>ϵ</em></span>-representative if <span class="math display">∀<em>h</em><em>i</em><em>n</em>ℋ,  |<em>L</em><sub><em>S</em></sub>(<em>h</em>)−<em>L</em><sub>ℙ</sub>(<em>h</em>)| ≤ <em>ϵ</em></span></em></p> </div> <p>The next simple conclusion we can make is that whenever the sample is <span class="math inline"><em>ϵ</em>/2</span>-representative, the ERM learning rule is guaranteed to return a good hypothesis.</p> <div id="uni_convergence" class="theorem"> <p><strong>Theorem 5</strong>. <em>Assume <span class="math inline"><em>S</em></span> is <span class="math inline"><em>ϵ</em>/2</span> - representative. Then, any ERM solution <span class="math inline"><em>h</em><sub><em>S</em></sub> ∈ <em>a</em><em>r</em><em>g</em><em>m</em><em>i</em><em>n</em><sub><em>h</em> ∈ ℋ</sub><em>L</em><sub><em>S</em></sub>(<em>h</em>)</span> satisfies <span class="math display"><em>L</em><sub>ℙ</sub> ≤ min<sub><em>h</em> ∈ ℍ</sub><em>L</em><sub>ℙ</sub> + <em>ϵ</em></span></em></p> </div> <div class="proof"> <p><em>Proof.</em> For every <span class="math inline"><em>h</em> ∈ ℋ</span>, <span class="math display">$$\begin{aligned} L_{\mathbb{P}} &amp; \leq L_S(h_S) + \epsilon/2 \\ &amp; \leq L_S(h) + \epsilon/2 \\ &amp; \leq L_{\mathbb{P}}(h) + \epsilon/2 + \epsilon/2 \\ &amp; = L_{\mathbb{P}}(h) + \epsilon \end{aligned}$$</span> ◻</p> </div> <p>The simple theorem implies that to ensure that the ERM rule is agnostic PAC-Learnable, it suffices to show that with probability of at least <span class="math inline">1?<em>δ</em></span> over the random choice of a training set, it will be an <span class="math inline"><em>ϵ</em></span>-representative training set. The following uniform convergence condition formalizes this requirement.</p> <div class="definition"> <p><strong>Definition 6</strong> (Uniform Convergence). <em>A hypothesis class <span class="math inline">ℋ</span> has the uniform convergence property w.r.t a domain <span class="math inline"><em>Z</em></span> and a loss function <span class="math inline">ℓ</span>, if there exists a function <span class="math inline"><em>N</em><sub>ℋ</sub><sup><em>U</em><em>C</em></sup> : (0,1)<sup>2</sup> → ℕ</span> such that for every <span class="math inline"><em>ϵ</em>, <em>δ</em> ∈ (0,1)</span> and for every probability distribution <span class="math inline">ℙ</span> over <span class="math inline"><em>Z</em></span>, if <span class="math inline"><em>S</em></span> is a sample of <span class="math inline"><em>N</em> ≥ <em>N</em><sub>ℋ</sub><sup><em>U</em><em>C</em></sup>(<em>ϵ</em>,<em>δ</em>)</span> i.i.d. examples drawn from <span class="math inline">ℙ</span>, then, with probability of at least <span class="math inline">1 − <em>δ</em></span>, <span class="math inline"><em>S</em></span> is <span class="math inline"><em>ϵ</em></span>-representative.</em></p> </div> <p>Similar to the definition of sample complexity for PAC learning, the function <span class="math inline"><em>N</em><sub>ℋ</sub><sup><em>U</em><em>C</em></sup></span> measures the minimal sample complexity of obtaining the uniform convergence property, namely, how many examples we need to ensure that with probability of at least <span class="math inline">1 − <em>δ</em></span> the sample would be <span class="math inline"><em>ϵ</em></span>-representative. The term uniform here refers to having a fixed sample size that works for all members of <span class="math inline">ℋ</span> and over all possible probability distributions over the domain. The following corollary follows directly from the previous theorem and the definition of uniform convergence.</p> <div class="corollary"> <p><strong>Corollary 7</strong>. <em>If a class <span class="math inline">ℋ</span> has the uniform convergence property with a function <span class="math inline"><em>N</em><sub>ℋ</sub><sup><em>U</em><em>C</em></sup></span> then the class is agnostically PAC learnable with the sample complexity <span class="math inline"><em>N</em><sub>ℋ</sub>(<em>ϵ</em>,<em>δ</em>) ≤ <em>N</em><sub>ℋ</sub><sup><em>U</em><em>C</em></sup>(<em>ϵ</em>/2,<em>δ</em>)</span>.</em></p> </div> <h1 id="vc_dimension"> <a href="#vc_dimension" class="anchor-heading" aria-labelledby="vc_dimension"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> VC-Dimesion </h1> <p>Now, let’s move to the situation of infinite hypothesis class. Clearly, we don’t have a measurement for the size of the hypothesis class any more, but it is still possible to quantitively measure complexity of the model. For learnability In classification problems, what really matters is not the literal size of the hypothesis class, but the maximum number of data points that can be classified exactly. Take the simple situation in Figure <a href="#Fig:vc1" data-reference-type="ref" data-reference="Fig:vc1">1</a> for example, the hypothesis class of 1-dimensional linear classifier has a infinite size, but this doesn’t mean this class is a very complex class. As shown in Figure <a href="#Fig:vc1" data-reference-type="ref" data-reference="Fig:vc1">1</a> (a), two points with whatever labels can be classified correctly by a linear classifier, but in Figure <a href="#Fig:vc1" data-reference-type="ref" data-reference="Fig:vc1">1</a> (b), we can see that this no longer holds for 3 points, as the last example in (b) cannot be classified correctly by any hypothesis in the linear classifier class. This inspires us that in order to measure the richness of our hypothesis class, we can try to construct a subset <span class="math inline"><em>C</em></span> of the data domain for which our classifier fails or succeeds. To understand the power of our hypothesis class, we just focus on its behavior on <span class="math inline"><em>C</em></span> and try to check how many different possible classification decisions on <span class="math inline"><em>C</em></span> can our hypothesis class capture. Then, if the hypothesis class can explain all decisions possible on <span class="math inline"><em>C</em></span>, then one can construct a ‘misleading data distribution’ so that we maintain realizability on <span class="math inline"><em>C</em></span> but can be totally wrong on the part outside of <span class="math inline"><em>C</em></span> and thus suffer large risk. This implies that to achieve learnability, we need to restrict the size of <span class="math inline"><em>C</em></span>.</p> <figure id="Fig:vc1"> <img src="./vc_1_d.png" alt="Linear classifiers in 1D can shatter 2 points as in (a), but cannot classifier the last case correctly in (b). Thus the VC-Dimension of 1-D linear classifiers is 2." /> <figcaption aria-hidden="true">Linear classifiers in 1D can shatter 2 points as in (a), but cannot classifier the last case correctly in (b). Thus the VC-Dimension of 1-D linear classifiers is 2.</figcaption> </figure> <p>To be more formal, here we introduce the definition of restriction of <span class="math inline">ℋ</span> to <span class="math inline"><em>C</em></span> and the following definition of <em>shattering</em> and <em>VC-Dimension</em></p> <div class="definition"> <p><strong>Definition 8</strong>. <em>Let <span class="math inline">ℋ</span> be a class of functions from <span class="math inline">𝒳</span> to <span class="math inline">{0, 1}</span> and let <span class="math inline"><em>C</em> = {<em>c</em><sub>1</sub>, ..., <em>c</em><sub><em>m</em></sub>} ⊂ 𝒳</span>. The restriction of <span class="math inline">ℋ</span> to <span class="math inline"><em>C</em></span> is the set of functions from <span class="math inline"><em>C</em></span> to <span class="math inline">{0, 1}</span> that can be derived from <span class="math inline">ℋ</span>. That is, <span class="math display">ℋ<sub><em>C</em></sub> = {(<em>h</em>(<em>c</em><sub>1</sub>),...,<em>h</em>(<em>c</em><sub><em>m</em></sub>)) : <em>h</em> ∈ ℋ}</span> where we present each function from <span class="math inline"><em>C</em></span> to <span class="math inline">{0, 1}</span> as a vector in <span class="math inline">{0, 1}<sup>|<em>C</em>|</sup></span>.</em></p> </div> <p>If the restriction of <span class="math inline"><em>H</em></span> to <span class="math inline"><em>C</em></span> is the set of all functions from <span class="math inline"><em>C</em></span> to <span class="math inline">{0, 1}</span>, then we say <span class="math inline">ℋ</span> shatters the set <span class="math inline"><em>C</em></span>, formally</p> <div class="definition"> <p><strong>Definition 9</strong> (Shattering). <em>A hypothesis class <span class="math inline">ℋ</span> shatters finite set <span class="math inline"><em>C</em> ⊂ 𝒳</span> if the restriction of <span class="math inline">ℋ</span> to <span class="math inline"><em>C</em></span> is the set of all functions from <span class="math inline"><em>C</em></span> to <span class="math inline">{0, 1}</span>. That is, <span class="math inline">|ℋ<sub><em>C</em></sub>| = 2<sup>|<em>C</em>|</sup></span>.</em></p> </div> <div class="definition"> <p><strong>Definition 10</strong> (VC-dimension). <em>The VC-dimension of a hypothesis class <span class="math inline">ℋ</span>, denoted <span class="math inline"><em>V</em><em>C</em><em>d</em><em>i</em><em>m</em>(ℋ)</span>, is the maximal size of a set <span class="math inline"><em>C</em> ⊂ 𝒳</span> that can be shattered by <span class="math inline">ℋ</span>. If <span class="math inline">ℋ</span> can shatter sets of arbitrarily large size, we say that <span class="math inline">ℋ</span> has infinite VC-dimension.</em></p> </div> <div class="center"> </div> <p>Here we give another example on 2-D linear classifiers, as shown in Figure <a href="#Fig:vc2" data-reference-type="ref" data-reference="Fig:vc2">2</a>. In (a), we can see that the linear classifier class can shatter 3 points in 2 dimensional space, however in (b), it cannot shatter 4 points as there exists a case where no linear classifier can correctly classifier the 4 points with the particular labelling as in the right figure in (b). This shows that the VC-Dimension of 2-D linear classifiers is 3.</p> <figure id="Fig:vc2"> <img src="./vc_2_d.png" alt="Linear classifiers in 2D can classifier 3 points with arbitrary labelling as shown in (a), but cannot classifier 4 points correctly as in (b). Thus the VC-Dimension of 2-D linear classifiers is 3." /> <figcaption aria-hidden="true">Linear classifiers in 2D can classifier 3 points with arbitrary labelling as shown in (a), but cannot classifier 4 points correctly as in (b). Thus the VC-Dimension of 2-D linear classifiers is 3.</figcaption> </figure> <p>From this kind of observation, we can see that to show that <span class="math inline"><em>V</em><em>C</em> − <em>d</em><em>i</em><em>m</em>(ℋ) = <em>d</em></span>, we need to prove two things:</p> <ol type="1"> <li><p>There exists a set <span class="math inline"><em>C</em></span> of size <span class="math inline"><em>d</em></span> that is shattered by <span class="math inline">ℋ</span>, this proves <span class="math inline"><em>V</em><em>C</em> − <em>d</em><em>i</em><em>m</em>(ℋ) ≥ <em>d</em></span></p></li> <li><p>No set of size <span class="math inline"><em>d</em> + 1</span> is shattered by <span class="math inline">ℋ</span>, this proves <span class="math inline"><em>V</em><em>C</em> − <em>d</em><em>i</em><em>m</em>(ℋ) &lt; <em>d</em> + 1</span>. Thus <span class="math inline"><em>V</em><em>C</em> − <em>d</em><em>i</em><em>m</em>(ℋ) = <em>d</em></span>.</p></li> </ol> <p>Though we showed the VC-Dimension of <span class="math inline"><em>d</em></span>-dimensional linear classifier is <span class="math inline"><em>d</em> + 1</span>, most of the time, we can only have lower/upper bound of VC dimension, but not an exact computable number. Thus, it is important to understand the meaning of the lower and upper bound of VC-Dimension.</p> <div class="center"> </div> <h1 id="fundamental-theorem-of-learnability"> <a href="#fundamental-theorem-of-learnability" class="anchor-heading" aria-labelledby="fundamental-theorem-of-learnability"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> Fundamental Theorem of Learnability </h1> <div id="fundamental" class="theorem"> <p><strong>Theorem 11</strong> (The Fundamental Theorem of Statistical Learning). <em>Let <span class="math inline">ℋ</span> be a hypothesis class of functions from a domain <span class="math inline">𝒳</span> to <span class="math inline">{0, 1}</span> and let the loss function be the <span class="math inline">0 − 1</span> loss. Then the following are equivalent:</em></p> <ol type="1"> <li><p><em><span class="math inline">ℋ</span> has the uniform convergence property.</em></p></li> <li><p><em>Any ERM rule is a successful agnostic PAC learner for <span class="math inline">ℋ</span>.</em></p></li> <li><p><em><span class="math inline">ℋ</span> is agnostic PAC learnable.</em></p></li> <li><p><em><span class="math inline">ℋ</span> is PAC learnable.</em></p></li> <li><p><em><span class="math inline">ℋ</span> Any ERM rule is a successful PAC learner for <span class="math inline">ℋ</span>.</em></p></li> <li><p><em><span class="math inline">ℋ</span> has a finite VC-dimension.</em></p></li> </ol> </div> <p>In our previous discussion, we saw <span class="math inline">1 → 2</span>. <span class="math inline">2 → 3</span>, <span class="math inline">3 → 4</span> and <span class="math inline">2 → 5</span> are all trivial. For <span class="math inline">4 → 6</span> and <span class="math inline">5 → 6</span>, there is detailed proof in [SSS] through the no-free-lunch theorem. Here, we take a closer look at <span class="math inline">6 → 1</span>, that a finite VC-dimension implies the uniform convergence property, and therefore is PAC-learnable. The detailed proof can be found in chapter 6 of [SSS], here we provide a high level sketch of the proof. The two main parts of the proof are</p> <ol type="1"> <li><p>If <span class="math inline"><em>V</em><em>C</em> − <em>d</em><em>i</em><em>m</em>(ℋ) = <em>d</em></span>, when restricting to a finite subset <span class="math inline"><em>C</em></span> of the data domain, its effective size <span class="math inline">|ℋ<sub><em>C</em></sub>|</span> is only <span class="math inline"><em>O</em>|<em>C</em>|<sup><em>d</em></sup></span>, instead of exponential in <span class="math inline">|<em>C</em>|</span></p></li> <li><p>Finite hypothesis class can be proved to have the uniform convergence property by a direct application of Hoeffiding inequality plus the union bound theorem. Similarly, the uniform convergence holds whenever the "effective size" is small.</p></li> </ol> <p>To define the term "effective size", we introduce the definition of Growth Function,</p> <div class="definition"> <p><strong>Definition 12</strong> (Growth Function). <em>Let <span class="math inline">ℋ</span> be a hypothesis class. Then the growth function of <span class="math inline">ℋ</span>, denoted <span class="math inline"><em>τ</em><sub>ℋ</sub> : ℕ → ℕ</span>, is defined as <span class="math display"><em>τ</em><sub>ℋ</sub>(<em>N</em>) = max<sub><em>C</em> ⊂ 𝒳 : |<em>C</em>| = <em>N</em></sub>|ℋ<sub><em>C</em></sub>|</span></em></p> </div> <p>In words, <span class="math inline"><em>τ</em><sub>ℋ</sub>(<em>N</em>)</span> is the number of different functions from a set <span class="math inline"><em>C</em></span> of size <span class="math inline"><em>N</em></span> to <span class="math inline">{0, 1}</span> that can be obtained by restricting <span class="math inline">ℋ</span> to <span class="math inline"><em>C</em></span>. We then can prove the Sauer’s lemma that can bound this growth function</p> <div class="lemma"> <p><strong>Lemma 13</strong>. <em>Let <span class="math inline">ℋ</span> be a hypothesis class with <span class="math inline"><em>V</em><em>C</em> − <em>D</em><em>i</em><em>m</em>(ℋ) ≤ <em>d</em> &lt; ∞</span>. Then for all <span class="math inline"><em>N</em></span>, <span class="math inline">$\tau_\mathcal{H}(N) \leq \sum_{i=0}^d \begin{pmatrix} N\\i \end{pmatrix}$</span>. In particular, if <span class="math inline"><em>N</em> &gt; <em>d</em> + 1</span> then <span class="math inline"><em>τ</em><sub>ℋ</sub>(<em>N</em>) ≤ (<em>e</em><em>N</em>)<sup><em>d</em></sup></span></em></p> </div> <p>Thus, finite VC-dimension implies polynomial growth, while infinite VC-dim means exponential growth. Intuitively, for any <span class="math inline"><em>C</em></span> as a subset of <span class="math inline">𝒳</span>, let <span class="math inline"><em>B</em></span> be a subset of <span class="math inline"><em>C</em></span> such that <span class="math inline">ℋ</span> shatters <span class="math inline"><em>B</em></span>. Then, <span class="math inline">|ℋ<sub><em>C</em></sub>| ≤ #{<em>B</em> ⊂ <em>C</em> : ℋ shatters <em>B</em>}</span>. That is, if <span class="math inline">𝒞</span> is the collection of subsets of <span class="math inline"><em>C</em></span> that are shattered by <span class="math inline">ℋ</span>, then <span class="math inline">|ℋ<sub><em>C</em></sub>|</span> is upper-bounded by the cardinality of <span class="math inline">𝒞</span>. Then we can show the ERM error is bounded using the growth function</p> <div class="theorem"> <p><strong>Theorem 14</strong>. <em>Let <span class="math inline">ℋ</span> be a class and <span class="math inline"><em>τ</em><sub>ℋ</sub></span> its growth function. Then for every distribution <span class="math inline">ℙ(<em>X</em>,<em>Y</em>)</span> and every <span class="math inline"><em>δ</em> ∈ (0,1)</span>, with probability at least <span class="math inline">1 − <em>δ</em></span> over the choices of <span class="math inline"><em>S</em> ∼ ℙ</span>, we have <span class="math display">$$|L_S(h) - L_\mathbb{P}(h) | \leq \frac{4+\sqrt{\log \tau_{\mathcal{H}}(2N)}}{\delta \sqrt{2N}}$$</span></em></p> </div> <p>And it follows from here that if VC-Dim(<span class="math inline">ℋ</span>) is finite, then the uniform convergence property holds, and indeed, <span class="math display">$$N_{\mathcal{H}}^{UC}(\epsilon, \delta) \leq O(\frac{d}{(\delta \epsilon)^2})$$</span> suffices for the uniform convergence property to hold.</p> <p>A more quantitative version of this theorem is as follows, and the proof can be found in chapter 28 of [SSS].</p> <div class="theorem"> <p><strong>Theorem 15</strong>. <em>Let <span class="math inline">ℋ</span> be a hypothesis class of functions from a domain <span class="math inline">𝒳</span> to <span class="math inline">{0, 1}</span> and let the loss function be the <span class="math inline">0 − 1</span> loss. Assume that <span class="math inline"><em>V</em><em>C</em> − <em>D</em><em>i</em><em>m</em>(ℋ) = <em>d</em> &lt; ∞</span>. Then, there are absolute constants <span class="math inline"><em>C</em><sub>1</sub></span>, <span class="math inline"><em>C</em><sub>2</sub></span> such that:</em></p> <ol type="1"> <li><p><em><span class="math inline">ℋ</span> has the uniform convergence property with sample complexity <span class="math display">$$C_1\frac{d+\log(1/\delta)}{\epsilon^2} \leq N_\mathcal{H}^{UC}(\epsilon,\delta) \leq C_2 \frac{d+\log(1/\delta)}{\epsilon^2}$$</span></em></p></li> <li><p><em><span class="math inline">ℋ</span> is agnostic PAC learnable with sample complexity <span class="math display">$$C_1\frac{d+\log(1/\delta)}{\epsilon^2} \leq N_\mathcal{H}(\epsilon,\delta) \leq C_2 \frac{d+\log(1/\delta)}{\epsilon^2}$$</span></em></p></li> <li><p><em><span class="math inline">ℋ</span> is PAC learnable with sample complexity <span class="math display">$$C_1\frac{d+\log(1/\delta)}{\epsilon} \leq N_\mathcal{H}(\epsilon,\delta) \leq C_2 \frac{d\log (1/\epsilon)+\log(1/\delta)}{\epsilon}$$</span></em></p></li> </ol> </div> </main> <hr> <footer> <p><a href="#top" id="back-to-top">Back to top</a></p> <div class="d-flex mt-2"> <p class="text-small text-grey-dk-000 mb-0"> <a href="https://shenshen.mit.edu/git/shensquared/gradml/tree/main/supervised/learnability_and_vc.md" id="edit-this-page">Edit this page on Git</a> | This page was last updated on 16-Jun-2023 </p> </div> </footer> </div> </div> <div class="search-overlay"></div> </div> </body> </html> <script src="https://hypothes.is/embed.js" async></script>
