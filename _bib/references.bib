
@book{quinonero-candela_dataset_2008,
	address = {Cambridge, MA},
	series = {Neural {Information} {Processing}},
	title = {Dataset {Shift} in {Machine} {Learning}},
	isbn = {978-0-262-17005-5},
	url = {https://mitpress.mit.edu/books/dataset-shift-machine-learning},
	abstract = {An overview of recent efforts in the machine learning community to deal with dataset and covariate shift, which occurs when test and training inputs and outputs have different distributions.},
	language = {en},
	number = {12},
	publisher = {MIT Press},
	author = {Quiñonero-Candela, Joaquin and Sugiyama, Masashi and Schwaighofer, Anton and Lawrence, Neil D.},
	month = dec,
	year = {2008}
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667},
	url = {https://www.mitpressjournals.org/doi/10.1162/neco.1997.9.8.1735},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	number = {8},
	urldate = {2019-10-13},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	pages = {1735--1780},
	file = {Snapshot:/home/matthew/.zotero/zotero/lzd3w98o.default/zotero/storage/CX3U6WWX/neco.1997.9.8.html:text/html}
}

@inproceedings{cho_learning_2014,
	address = {Doha, Qatar},
	title = {Learning {Phrase} {Representations} using {RNN} {Encoder}–{Decoder} for {Statistical} {Machine} {Translation}},
	doi = {10.3115/v1/D14-1179},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Cho, Kyunghyun and van Merriënboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
	month = oct,
	year = {2014},
	pages = {1724--1734},
	file = {Full Text PDF:/home/matthew/.zotero/zotero/lzd3w98o.default/zotero/storage/T4MKQIPQ/Cho et al. - 2014 - Learning Phrase Representations using RNN Encoder–.pdf:application/pdf}
}

@article{jordan_attractor_1986,
	title = {Attractor dynamics and parallelism in a connectionist sequential machine},
	url = {https://ci.nii.ac.jp/naid/10018634949/},
	urldate = {2019-10-13},
	journal = {Proc. of the Eighth Annual Conference of the Cognitive Science Society (Erlbaum, Hillsdale, NJ), 1986},
	author = {JORDAN, M.},
	year = {1986},
	file = {Attractor dynamics and parallelism in a connectionist sequential machine Snapshot:/home/matthew/.zotero/zotero/lzd3w98o.default/zotero/storage/9SHTAN48/10018634949.html:text/html}
}

@article{pearlmutter_learning_1989,
	title = {Learning {State} {Space} {Trajectories} in {Recurrent} {Neural} {Networks}},
	volume = {1},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1989.1.2.263},
	doi = {10.1162/neco.1989.1.2.263},
	abstract = {Many neural network learning procedures compute gradients of the errors on the output layer of units after they have settled to their final values. We describe a procedure for finding ∂E/∂wij, where E is an error functional of the temporal trajectory of the states of a continuous recurrent network and wij are the weights of that network. Computing these quantities allows one to perform gradient descent in the weights to minimize E. Simulations in which networks are taught to move through limit cycles are shown. This type of recurrent network seems particularly suited for temporally continuous domains, such as signal processing, control, and speech.},
	number = {2},
	urldate = {2019-10-13},
	journal = {Neural Computation},
	author = {Pearlmutter, Barak A.},
	month = jun,
	year = {1989},
	pages = {263--269},
	file = {Accepted Version:/home/matthew/.zotero/zotero/lzd3w98o.default/zotero/storage/97NXW723/Pearlmutter - 1989 - Learning State Space Trajectories in Recurrent Neu.pdf:application/pdf;Snapshot:/home/matthew/.zotero/zotero/lzd3w98o.default/zotero/storage/YUNW57B9/neco.1989.1.2.html:text/html}
}

@article{cleeremans_finite_1989,
	title = {Finite {State} {Automata} and {Simple} {Recurrent} {Networks}},
	volume = {1},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1989.1.3.372},
	doi = {10.1162/neco.1989.1.3.372},
	abstract = {We explore a network architecture introduced by Elman (1988) for predicting successive elements of a sequence. The network uses the pattern of activation over a set of hidden units from time-step t−1, together with element t, to predict element t + 1. When the network is trained with strings from a particular finite-state grammar, it can learn to be a perfect finite-state recognizer for the grammar. When the network has a minimal number of hidden units, patterns on the hidden units come to correspond to the nodes of the grammar, although this correspondence is not necessary for the network to act as a perfect finite-state recognizer. We explore the conditions under which the network can carry information about distant sequential contingencies across intervening elements. Such information is maintained with relative ease if it is relevant at each intermediate step; it tends to be lost when intervening elements do not depend on it. At first glance this may suggest that such networks are not relevant to natural language, in which dependencies may span indefinite distances. However, embeddings in natural language are not completely independent of earlier information. The final simulation shows that long distance sequential contingencies can be encoded by the network even if only subtle statistical properties of embedded strings depend on the early information.},
	number = {3},
	urldate = {2019-10-13},
	journal = {Neural Computation},
	author = {Cleeremans, Axel and Servan-Schreiber, David and McClelland, James L.},
	month = sep,
	year = {1989},
	pages = {372--381},
	file = {Snapshot:/home/matthew/.zotero/zotero/lzd3w98o.default/zotero/storage/8I8A8E95/neco.1989.1.3.html:text/html}
}

@article{lei_simple_2017,
	title = {Simple {Recurrent} {Units} for {Highly} {Parallelizable} {Recurrence}},
	url = {http://arxiv.org/abs/1709.02755},
	abstract = {Common recurrent neural architectures scale poorly due to the intrinsic difficulty in parallelizing their state computations. In this work, we propose the Simple Recurrent Unit (SRU), a light recurrent unit that balances model capacity and scalability. SRU is designed to provide expressive recurrence, enable highly parallelized implementation, and comes with careful initialization to facilitate training of deep models. We demonstrate the effectiveness of SRU on multiple NLP tasks. SRU achieves 5--9x speed-up over cuDNN-optimized LSTM on classification and question answering datasets, and delivers stronger results than LSTM and convolutional models. We also obtain an average of 0.7 BLEU improvement over the Transformer model on translation by incorporating SRU into the architecture.},
	urldate = {2019-10-14},
	journal = {arXiv:1709.02755 [cs]},
	author = {Lei, Tao and Zhang, Yu and Wang, Sida I. and Dai, Hui and Artzi, Yoav},
	month = sep,
	year = {2017},
	note = {arXiv: 1709.02755},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language},
	annote = {Comment: EMNLP},
	file = {arXiv\:1709.02755 PDF:/home/matthew/.zotero/zotero/lzd3w98o.default/zotero/storage/KD8ECE4L/Lei et al. - 2017 - Simple Recurrent Units for Highly Parallelizable R.pdf:application/pdf;arXiv.org Snapshot:/home/matthew/.zotero/zotero/lzd3w98o.default/zotero/storage/VI94CGHG/1709.html:text/html}
}

@article{miller_stable_2018,
	title = {Stable {Recurrent} {Models}},
	url = {http://arxiv.org/abs/1805.10369},
	abstract = {Stability is a fundamental property of dynamical systems, yet to this date it has had little bearing on the practice of recurrent neural networks. In this work, we conduct a thorough investigation of stable recurrent models. Theoretically, we prove stable recurrent neural networks are well approximated by feed-forward networks for the purpose of both inference and training by gradient descent. Empirically, we demonstrate stable recurrent models often perform as well as their unstable counterparts on benchmark sequence tasks. Taken together, these findings shed light on the effective power of recurrent networks and suggest much of sequence learning happens, or can be made to happen, in the stable regime. Moreover, our results help to explain why in many cases practitioners succeed in replacing recurrent models by feed-forward models.},
	urldate = {2019-10-14},
	journal = {arXiv:1805.10369 [cs, stat]},
	author = {Miller, John and Hardt, Moritz},
	month = may,
	year = {2018},
	note = {arXiv: 1805.10369},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	annote = {Comment: To appear in ICLR 2019. This paper was previously titled "When Recurrent Models Don't Need to Be Recurrent." The current version subsumes all previous versions},
	file = {arXiv\:1805.10369 PDF:/home/matthew/.zotero/zotero/lzd3w98o.default/zotero/storage/9TZ8E6VI/Miller and Hardt - 2018 - Stable Recurrent Models.pdf:application/pdf;arXiv.org Snapshot:/home/matthew/.zotero/zotero/lzd3w98o.default/zotero/storage/PP6DARVI/1805.html:text/html}
}

@misc{olah_understanding_2015,
	title = {Understanding {LSTM} {Networks} -- colah's blog},
	url = {https://colah.github.io/posts/2015-08-Understanding-LSTMs/},
	language = {en},
	urldate = {2019-10-14},
	journal = {colah's blog},
	author = {Olah, Cristopher},
	month = aug,
	year = {2015},
	file = {Understanding LSTM Networks -- colah's blog:/home/matthew/.zotero/zotero/lzd3w98o.default/zotero/storage/KBCHGJUT/2015-08-Understanding-LSTMs.html:text/html}
}

@article{olah_attention_2016,
	title = {Attention and {Augmented} {Recurrent} {Neural} {Networks}},
	volume = {1},
	issn = {2476-0757},
	url = {http://distill.pub/2016/augmented-rnns},
	doi = {10.23915/distill.00001},
	abstract = {A visual overview of neural attention, and the powerful extensions of neural networks being built on top of it.},
	language = {en},
	number = {9},
	urldate = {2019-10-14},
	journal = {Distill},
	author = {Olah, Chris and Carter, Shan},
	month = sep,
	year = {2016},
	pages = {e1},
	file = {Snapshot:/home/matthew/.zotero/zotero/lzd3w98o.default/zotero/storage/DW4KIAT2/augmented-rnns.html:text/html}
}

@article{madsen_visualizing_2019,
	title = {Visualizing memorization in {RNNs}},
	volume = {4},
	issn = {2476-0757},
	url = {https://distill.pub/2019/memorization-in-rnns},
	doi = {10.23915/distill.00016},
	abstract = {Inspecting gradient magnitudes in context can be a powerful tool to see when recurrent units use short-term or long-term contextual understanding.},
	language = {en},
	number = {3},
	urldate = {2019-10-14},
	journal = {Distill},
	author = {Madsen, Andreas},
	month = mar,
	year = {2019},
	pages = {e16},
	file = {Snapshot:/home/matthew/.zotero/zotero/lzd3w98o.default/zotero/storage/6APH5JKH/memorization-in-rnns.html:text/html}
}

@inproceedings{rogers_too_2017,
	address = {Vancouver, Canada},
	title = {The (too {Many}) {Problems} of {Analogical} {Reasoning} with {Word} {Vectors}},
	url = {https://www.aclweb.org/anthology/S17-1017},
	doi = {10.18653/v1/S17-1017},
	abstract = {This paper explores the possibilities of analogical reasoning with vector space models. Given two pairs of words with the same relation (e.g. man:woman :: king:queen), it was proposed that the offset between one pair of the corresponding word vectors can be used to identify the unknown member of the other pair (king - man + woman = queen). We argue against such “linguistic regularities” as a model for linguistic relations in vector space models and as a benchmark, and we show that the vector offset (as well as two other, better-performing methods) suffers from dependence on vector similarity.},
	urldate = {2019-10-14},
	booktitle = {Proceedings of the 6th {Joint} {Conference} on {Lexical} and {Computational} {Semantics} (*{SEM} 2017)},
	publisher = {Association for Computational Linguistics},
	author = {Rogers, Anna and Drozd, Aleksandr and Li, Bofang},
	month = aug,
	year = {2017},
	pages = {135--148},
	file = {Full Text PDF:/home/matthew/.zotero/zotero/lzd3w98o.default/zotero/storage/BNBV7UZ5/Rogers et al. - 2017 - The (too Many) Problems of Analogical Reasoning wi.pdf:application/pdf}
}

@incollection{mikolov_distributed_2013,
	title = {Distributed {Representations} of {Words} and {Phrases} and their {Compositionality}},
	url = {http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf},
	urldate = {2019-10-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 26},
	publisher = {Curran Associates, Inc.},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
	editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
	year = {2013},
	pages = {3111--3119},
	file = {NIPS Full Text PDF:/home/matthew/.zotero/zotero/lzd3w98o.default/zotero/storage/S6ZV2DX8/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases a.pdf:application/pdf;NIPS Snapshot:/home/matthew/.zotero/zotero/lzd3w98o.default/zotero/storage/6R94U2DT/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.html:text/html}
}

@inproceedings{zeiler_visualizing_2014,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Visualizing and {Understanding} {Convolutional} {Networks}},
	isbn = {978-3-319-10590-1},
	abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2014},
	publisher = {Springer International Publishing},
	author = {Zeiler, Matthew D. and Fergus, Rob},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	year = {2014},
	keywords = {Convolutional Neural Network, Input Image, Pixel Space, Stochastic Gradient Descent, Training Image},
	pages = {818--833},
	file = {Springer Full Text PDF:/home/matthew/.zotero/zotero/lzd3w98o.default/zotero/storage/WX9W6CQG/Zeiler and Fergus - 2014 - Visualizing and Understanding Convolutional Networ.pdf:application/pdf}
}

@inproceedings{peters_deep_2018,
	address = {New Orleans, Louisiana},
	title = {Deep {Contextualized} {Word} {Representations}},
	url = {https://www.aclweb.org/anthology/N18-1202},
	doi = {10.18653/v1/N18-1202},
	abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
	urldate = {2019-10-14},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Peters, Matthew and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
	month = jun,
	year = {2018},
	pages = {2227--2237},
	file = {Full Text PDF:/home/matthew/.zotero/zotero/lzd3w98o.default/zotero/storage/MEDV5HGS/Peters et al. - 2018 - Deep Contextualized Word Representations.pdf:application/pdf}
}

@incollection{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	url = {http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf},
	urldate = {2019-10-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
	pages = {5998--6008},
	file = {NIPS Full Text PDF:/home/matthew/.zotero/zotero/lzd3w98o.default/zotero/storage/BAEF4ZX3/Vaswani et al. - 2017 - Attention is All you Need.pdf:application/pdf;NIPS Snapshot:/home/matthew/.zotero/zotero/lzd3w98o.default/zotero/storage/ASV9G8FQ/7181-attention-is-all-you-need.html:text/html}
}

@inproceedings{devlin_bert:_2019,
	address = {Minneapolis, Minnesota},
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {https://www.aclweb.org/anthology/N19-1423},
	doi = {10.18653/v1/N19-1423},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2019-10-14},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = jun,
	year = {2019},
	pages = {4171--4186},
	file = {Full Text PDF:/home/matthew/.zotero/zotero/lzd3w98o.default/zotero/storage/FJPAG3DI/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf}
}

@misc{alammar_illustrated_2018,
	title = {The {Illustrated} {Transformer}},
	url = {http://jalammar.github.io/illustrated-transformer/},
	language = {en},
	journal = {Visualizing machine learning one concept at a time  Blog About},
	author = {Alammar, Jay},
	month = jun,
	year = {2018}
}

@misc{alammar_illustrated_2019,
	title = {The {Illustrated} {GPT}-2 ({Visualizing} {Transformer} {Language} {Models})},
	url = {http://jalammar.github.io/illustrated-gpt2/},
	language = {en},
	urldate = {2019-10-14},
	journal = {Visualizing machine learning one concept at a time},
	author = {Alammar, Jay},
	month = aug,
	year = {2019}
}

@article{taylor_``cloze_1953,
	title = {``{Cloze} {Procedure}'': {A} {New} {Tool} for {Measuring} {Readability}},
	volume = {30},
	issn = {0197-2448},
	shorttitle = {“{Cloze} {Procedure}”},
	url = {https://doi.org/10.1177/107769905303000401},
	doi = {10.1177/107769905303000401},
	abstract = {Here is the first comprehensive statement of a research method and its theory which were introduced briefly during a workshop at the 1953 AEJ convention. Included are findings from three pilot studies and two experiments in which “cloze procedure” results are compared with those of two readability formulas.},
	language = {en},
	number = {4},
	urldate = {2019-10-15},
	journal = {Journalism Bulletin},
	author = {Taylor, Wilson L.},
	month = sep,
	year = {1953},
	pages = {415--433}
}

@article{pappas_beyond_2018,
	title = {Beyond {Weight} {Tying}: {Learning} {Joint} {Input}-{Output} {Embeddings} for {Neural} {Machine} {Translation}},
	shorttitle = {Beyond {Weight} {Tying}},
	url = {http://arxiv.org/abs/1808.10681},
	abstract = {Tying the weights of the target word embeddings with the target word classifiers of neural machine translation models leads to faster training and often to better translation quality. Given the success of this parameter sharing, we investigate other forms of sharing in between no sharing and hard equality of parameters. In particular, we propose a structure-aware output layer which captures the semantic structure of the output space of words within a joint input-output embedding. The model is a generalized form of weight tying which shares parameters but allows learning a more flexible relationship with input word embeddings and allows the effective capacity of the output layer to be controlled. In addition, the model shares weights across output classifiers and translation contexts which allows it to better leverage prior knowledge about them. Our evaluation on English-to-Finnish and English-to-German datasets shows the effectiveness of the method against strong encoder-decoder baselines trained with or without weight tying.},
	urldate = {2019-10-27},
	journal = {arXiv:1808.10681 [cs]},
	author = {Pappas, Nikolaos and Werlen, Lesly Miculicich and Henderson, James},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.10681},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: To appear at WMT 2018},
	file = {arXiv Fulltext PDF:/home/matthew/.zotero/zotero/lzd3w98o.default/zotero/storage/VXHKA547/Pappas et al. - 2018 - Beyond Weight Tying Learning Joint Input-Output E.pdf:application/pdf;arXiv.org Snapshot:/home/matthew/.zotero/zotero/lzd3w98o.default/zotero/storage/Q9LHQQZY/1808.html:text/html}
}