<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>Discriminative vs Generative Classification | 6.790 Machine Learning</title> <meta name="generator" content="Jekyll v4.3.2" /> <meta property="og:title" content="Discriminative vs Generative Classification" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="Graduate Machine Learning Course, MIT EECS, Fall 2023." /> <meta property="og:description" content="Graduate Machine Learning Course, MIT EECS, Fall 2023." /> <link rel="canonical" href="https://gradml.mit.edu/supervised/DiscriminativeGenerative/" /> <meta property="og:url" content="https://gradml.mit.edu/supervised/DiscriminativeGenerative/" /> <meta property="og:site_name" content="6.790 Machine Learning" /> <meta property="og:type" content="website" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="Discriminative vs Generative Classification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","dateModified":"2023-06-18T19:25:55-04:00","description":"Graduate Machine Learning Course, MIT EECS, Fall 2023.","headline":"Discriminative vs Generative Classification","url":"https://gradml.mit.edu/supervised/DiscriminativeGenerative/"}</script> <!-- End Jekyll SEO tag --> <link rel="shortcut icon" type="image/png" href="/assets/images/favicon.png"> <!-- CSS --> <link rel="stylesheet" href="/assets/js/katex.min.css" /> <!-- JavaScript --> <script defer src="/assets/js/katex.min.js"></script> <script defer src="/assets/js/auto-render.min.js" onload="renderMathInElement(document.body,{ delimiters: [ { left: '$$', right: '$$', display: true }, { left: '$', right: '$', display: false }, { left: '\\[', right: '\\]', display: true }, { left: '\\(', right: '\\)', display: false } ] } );"> </script> <script src="/assets/js/hypothesis.js" async></script> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"> 6.790 Machine Learning </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"> <use xlink:href="#svg-menu"></use> </svg> </a> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Info category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/" class="nav-list-link">Info</a><ul class="nav-list"><li class="nav-list-item "><a href="/info/calendar/" class="nav-list-link">Syllabus / Calendar</a></li><li class="nav-list-item "><a href="/info/staff/" class="nav-list-link">Staff</a></li><li class="nav-list-item "><a href="/info/schedule/" class="nav-list-link">Weekly Schedule</a></li><li class="nav-list-item "><a href="/info/grading/" class="nav-list-link">Grading and Late Policy</a></li></ul></li><li class="nav-list-item"><a href="/intro/" class="nav-list-link">Introduction</a></li><li class="nav-list-item"><a href="/review/" class="nav-list-link">Background/Review</a></li><li class="nav-list-item active"><button class="nav-list-expander btn-reset" aria-label="toggle items in Supervised Learning category" aria-pressed="true"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/supervised/" class="nav-list-link">Supervised Learning</a><ul class="nav-list"><li class="nav-list-item "><a href="/supervised/classification_foundation/" class="nav-list-link">Classification Fundamentals</a></li><li class="nav-list-item active"><a href="/supervised/DiscriminativeGenerative/" class="nav-list-link active">Discriminative vs Generative Classification</a></li><li class="nav-list-item "><a href="/supervised/learnability_and_vc/" class="nav-list-link">Learnability and VC Dimension</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Unsupervised Learning category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/unsupervised/" class="nav-list-link">Unsupervised Learning</a><ul class="nav-list"><li class="nav-list-item "><button class="nav-list-expander btn-reset" aria-label="toggle items in Graphical Models category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/unsupervised/graphical/" class="nav-list-link">Graphical Models</a><ul class="nav-list"></ul></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Reinforcement Learning category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/reinforcement/" class="nav-list-link">Reinforcement Learning</a><ul class="nav-list"><li class="nav-list-item "><a href="/reinforcement/mdp/" class="nav-list-link">Markov Decision Process</a></li><li class="nav-list-item "><a href="/reinforcement/value_bellman/" class="nav-list-link">Value functions and Bellman</a></li><li class="nav-list-item "><a href="/reinforcement/bandit/" class="nav-list-link">Bandits</a></li><li class="nav-list-item "><a href="/reinforcement/policy_gradient/" class="nav-list-link">Policy Gradient</a></li><li class="nav-list-item "><a href="/reinforcement/deepRL/" class="nav-list-link">Deep Reinforcement Learning</a></li></ul></li></ul> </nav> <footer class="site-footer"> <a href="/credit">Acknowledgement</a> </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search 6.790 Machine Learning" aria-label="Search 6.790 Machine Learning" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> <nav aria-label="Auxiliary" class="aux-nav"> <ul class="aux-nav-list"> <li class="aux-nav-list-item"> <a href="https://canvas.mit.edu/" class="site-button" target="_blank" rel="noopener noreferrer" > Canvas </a> </li> <li class="aux-nav-list-item"> <a href="https://piazza.com" class="site-button" target="_blank" rel="noopener noreferrer" > Piazza </a> </li> </ul> </nav> </div> <div id="main-content-wrap" class="main-content-wrap"> <nav aria-label="Breadcrumb" class="breadcrumb-nav"> <ol class="breadcrumb-nav-list"> <li class="breadcrumb-nav-list-item"><a href="/supervised/">Supervised Learning</a></li> <li class="breadcrumb-nav-list-item"><span>Discriminative vs Generative Classification</span></li> </ol> </nav> <div id="main-content" class="main-content"> <p class="warning"> The site is under construction. </p> <main> <h2 class="no_toc no_toc text-delta" id="table-of-contents"> <a href="#table-of-contents" class="anchor-heading" aria-labelledby="table-of-contents"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> Table of contents </h2> <ol id="markdown-toc"> <li><a href="#overview" id="markdown-toc-overview">Overview</a></li> <li><a href="#four-approaches-to-build-classifiers" id="markdown-toc-four-approaches-to-build-classifiers">Four Approaches to Build Classifiers</a> <ol> <li><a href="#review-bayes-classifier" id="markdown-toc-review-bayes-classifier">Review: Bayes Classifier</a></li> <li><a href="#when-bayes-classifier-is-not-feasible" id="markdown-toc-when-bayes-classifier-is-not-feasible">When Bayes Classifier is not feasible</a></li> </ol> </li> <li><a href="#discriminative-vs-generative" id="markdown-toc-discriminative-vs-generative">Discriminative vs. Generative</a></li> <li><a href="#discrimitive-classifiers-logistic-regression" id="markdown-toc-discrimitive-classifiers-logistic-regression">Discrimitive Classifiers: Logistic Regression</a></li> <li><a href="#generative-classifiers-naive-bayes" id="markdown-toc-generative-classifiers-naive-bayes">Generative Classifiers: Naive Bayes</a> <ol> <li><a href="#the-naive-bayes-assumption" id="markdown-toc-the-naive-bayes-assumption">The Naive Bayes Assumption</a></li> <li><a href="#naive-bayes-can-be-a-linear-classifier" id="markdown-toc-naive-bayes-can-be-a-linear-classifier">Naive Bayes can be a linear classifier</a></li> </ol> </li> <li><a href="#naive-bayes-vs-logistic-regression" id="markdown-toc-naive-bayes-vs-logistic-regression">Naive Bayes vs. Logistic Regression</a> <ol> <li><a href="#asymptotic-regime" id="markdown-toc-asymptotic-regime">Asymptotic Regime</a></li> <li><a href="#non-asymptotic-regime" id="markdown-toc-non-asymptotic-regime">Non-asymptotic Regime</a></li> </ol> </li> </ol> <h1 id="overview"> <a href="#overview" class="anchor-heading" aria-labelledby="overview"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> Overview </h1> <p>In this lecture, we will mainly discuss two different approaches to build classifiers, the generative approach and the discriminative approach. As concrete examples, we will look at the Naive Bayes classifier for the generative approach and compare it the logistic regression, as an example of discriminative approach. We will show under center conditions, the Naive Bayes is a linear classifier, just as Logistic Regression, but it assumes stronger assumptions and therefore is a more biased classifier.</p> <h1 id="four-approaches-to-build-classifiers"> <a href="#four-approaches-to-build-classifiers" class="anchor-heading" aria-labelledby="four-approaches-to-build-classifiers"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> Four Approaches to Build Classifiers </h1> <h2 id="review-bayes-classifier"> <a href="#review-bayes-classifier" class="anchor-heading" aria-labelledby="review-bayes-classifier"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> Review: Bayes Classifier </h2> <p>We started our discussion of classification from the Bayes Classifier in Lecture 2. Recall the Bayes classifier $h^{*}(x)$ is defined by the rule:</p> \[h^{*}(x) := \begin{cases} 1, &amp;\text{if}\ \eta(x)=\mathbb{P}(Y=1 | X=x)&gt;\frac{1}{2}\\ 0, &amp;\text{otherwise}. \end{cases}\] <p>The classifier predicts label $1$ if the conditional probability of being in class $1$ is bigger than half. We also showed this classifier is actually the optimal possible classifier, as the underlying distribution $\mathbb{P}$ is assumed known.</p> <h2 id="when-bayes-classifier-is-not-feasible"> <a href="#when-bayes-classifier-is-not-feasible" class="anchor-heading" aria-labelledby="when-bayes-classifier-is-not-feasible"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> When Bayes Classifier is not feasible </h2> <p>However, we know this idealized situation is seldom the case in reality, as we usually do not have access to $\mathbb{P}$. Therefore, we introduced two different approaches, distance based classification (e.g. Nearest Neighbors) and Empirical Risk Minimization (e.g. SVM). Here, we give a formal summary of four possible methods to learn a classifier.</p> <ul> <li> <p>Distance based method</p> </li> <li> <p>Empirical Risk Minimization</p> </li> <li> <p>Discriminative Approach: Fit a model $\mathbb{\hat{P}}(Y|X)$ to approximate the conditional distribution $\mathbb{P}(Y|X)$, add classify using: $h(x)=\argmax_{y}\mathbb{\hat{P}}(Y=y|X=x)$</p> </li> <li> <p>Generative Approach: Fit a model $\mathbb{\hat{P}}(X, Y)$ to approximate the joint distribution $\mathbb{P}(X, Y)$, add classify using: \(h(x)=\\argmax_{y}P(X=x,Y=y) = P(X=x | Y=y)P(Y=y)\)</p> </li> </ul> <h1 id="discriminative-vs-generative"> <a href="#discriminative-vs-generative" class="anchor-heading" aria-labelledby="discriminative-vs-generative"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> Discriminative vs. Generative </h1> <p>The discriminative and the generative methods are two approaches to approximate the unknown underlying true distribution, and they are related by the Bayes rule \(P(X,Y)=P(X|Y)P(Y)=P(Y|X)P(X)\). Concretely, the generative approach learn what the individual classes looks like and models the data distribution $P(X)$. It is a potentially harder problem and computationally more challenging. But the advantage of this approach is that it can be used to sample new data.</p> <p>On the other hand, the discriminative approach learn the boundary between classes and models $P(Y|X)$, the conditional distribution and ignores $P(X)$. It solves a potentially easier problem and computationally simpler. However, it cannot be used to sample new data.</p> <h1 id="discrimitive-classifiers-logistic-regression"> <a href="#discrimitive-classifiers-logistic-regression" class="anchor-heading" aria-labelledby="discrimitive-classifiers-logistic-regression"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> Discrimitive Classifiers: Logistic Regression </h1> <p>Logistic Regression can be viewed as an approach of fitting a discriminative model. It assumes a parametric form of the conditional distribution $P(Y|X)$ as \(P(Y=1|X=x;w) = \frac{e^{w^Tx}}{1+e^{w^Tx}} = \sigma (w^Tx)\) where $\sigma(z) = \frac{1}{1+e^{-z}}$ and is often called the Sigmoid function.</p> <p>Therefore, \(P(Y=0|X=x;w) = 1- \frac{e^{w^Tx}}{1+e^{w^Tx}} = 1- \sigma (w^Tx)\) In the training process, we are given a training data set $S={(x_1,y_1), … , (x_N,y_N)}$ to estimate the weights $w$. Naturally, we will apply the maximum likelihood method where we can write the likelihood of the training data, assuming i.i.d., as</p> \[\begin{aligned} P(S|w) &amp; = \prod_{i=1}^N P(y_i|x_i;w) \\ &amp; = \prod_{i=1}^N ( \sigma (w^Tx_i))^{y_i} (1-\sigma (w^Tx_i) ) ^{1-y_i} \end{aligned}\] <p>Equivalent to maximize $P(S|w)$, we can minimize the negative log-likelihood of $S$, which is</p> \[\begin{aligned} L(w) &amp;= - \sum_{i=1}^N \log [(\sigma(w^Tx_i))^{y_i}(1-\sigma(w^Tx_i))^{1-y_i}] \\ &amp; = - \sum_{i=1}^N [y_i \log \sigma(w^Tx_i) + (1-y_i) \log (1-\sigma (w^Tx_i))] \end{aligned}\] <p>This is often called the cross-entropy error.</p> <h1 id="generative-classifiers-naive-bayes"> <a href="#generative-classifiers-naive-bayes" class="anchor-heading" aria-labelledby="generative-classifiers-naive-bayes"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> Generative Classifiers: Naive Bayes </h1> <p>Now we take a closer look at the generative approach. As said before, in the generative approach, we fit a model of the joint distribution $P(X,Y)$ and derived our classifier using the Bayes rule: \(P(Y|X) = \frac{P(X|Y)P(Y)}{P(X)}\) where $P(X)=\sum_Y P(X|Y)P(Y)$.</p> <p>For classification, we have</p> \[\begin{aligned} h(x) &amp; = \argmax_y P(Y=y|X=x)\\ &amp; = \argmax_y \frac{P(X=x|Y=y)P(Y=y)}{P(X=x)} \\ &amp; = \argmax_y P(X=x|Y=y)P(Y=y) \end{aligned}\] <p>The denominator $P(X=x)$ is only a normalization constant and thus can be ignored for deriving $\argmax_y P(Y=y|X=x)$.</p> <p>Now let’s suppose both $X$ and $Y$ are discrete random variables, where $X \in \mathcal{X}^d$ and $Y \in \mathcal{Y}$. Then we have \(P(Y=y_i|X=x_i) \propto P(X=x_k|Y=y_i)P(Y=y_i)\) Remember $X$ is a $d$ dimensional random variable, so to fully express this conditional distribution, we will approximately need $|\mathcal{X}|^d |\mathcal{Y}| + |\mathcal{Y}|$ parameters. As a simple example, suppose $\mathcal{X}=\mathcal{Y}={0,1}$, then $X \in {0,1}^d$. To specify $P(X=x_k|Y=0)$, we need $2^d$ parameters. Unless $d$ is sufficiently small, this full distribution is usually not computational tractable.</p> <h2 id="the-naive-bayes-assumption"> <a href="#the-naive-bayes-assumption" class="anchor-heading" aria-labelledby="the-naive-bayes-assumption"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> The Naive Bayes Assumption </h2> <p>To make the computation tractable and make the problem simpler, the Naive Bayes model make a strong assumption that the $d$ features are conditional independent of each other given the class label $Y$. Therefore, \(\begin{aligned} P(X|Y) &amp;= P(X_1,X_2,...,X_d|Y) \\ &amp;=P(X_1|X_2,...X_d|Y)P(X_2|X_3,...,X_d|Y)...P(X_d|Y) \\ &amp;=P(X_1|Y)P(X_2|Y)...P(X_d|Y) \end{aligned}\) where the second ’$=$’ used the conditional independence. Therefore, our classifier becomes \(\argmax_y P(Y=y|X_1,...,X_d) \propto P(Y=y)\prod_{j=1}^d P(X_j|Y=y)\)</p> <p>Now, using maximum likelihood on training data $S$, we can estimate the parameters for the Naive Bayes classifier, assuming a specific distribution for $P(X_j|Y)$, $1\leq j \leq d$. We can show for a multinomial distribution of $P(X_j|Y)$, the parameters of Naive Bayes are \(\begin{aligned} P(Y=y) &amp;= \frac{\# \text{Example with }Y = y}{N} \\ P(X_i=x|Y=y) &amp;= \frac{\# \text{Example with} X_i = x \text{ and } Y=y}{\# \text{Example with }Y = y} \end{aligned}\)</p> <h2 id="naive-bayes-can-be-a-linear-classifier"> <a href="#naive-bayes-can-be-a-linear-classifier" class="anchor-heading" aria-labelledby="naive-bayes-can-be-a-linear-classifier"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> Naive Bayes can be a linear classifier </h2> <p>We can show that under some common assumptions of $P(X_j|Y)$, the Naive Bayes classifier is actually a linear classifier. Here we provide a proof for $X_j \in {0,1}$ and $P(X_j|Y)$ is a Bernoulli distribution. (See Exercise 2 Problem 2 for other more general situations.)</p> <div id="thm:nb_linear" class="theorem"> **Theorem 1** (). *Let $X \in \{0,1\}^d$, and $P(X_j|Y)$,$1\leq j\leq d$ is a Bernoulli distribution. The Naive Bayes classifier is defined by $$h(x) = sgn (w^Tx+w_0)$$ for a suitable choice of $w$,$w_0$.* </div> <div class="proof"> *Proof.* As $X_j \in \{0,1 \}$, the Bernoulli distribution is therefore $$\begin{aligned} P(X_j|Y=1) &amp; = a_j^{X_j} (1-a_j)^{(1-X_j)} \\ P(X_j|Y=0) &amp; = b_j^{X_j} (1-b_j)^{(1-X_j)} \end{aligned}$$ where $a_j$ and $b_j$ are parameters for the $j$th dimension of $X$. With the conditional independence, we have $$\begin{aligned} P(Y=1|X) &amp; =\frac{P(X|Y=1)P(Y=1)}{P(X|Y=1)P(Y=1)+P(X|Y=0)P(Y=0)} \\ &amp; =\frac{1}{1+\frac{P(X|Y=0)P(Y=0)}{P(X|Y=1)P(Y=1)}} \\ &amp; = \frac{1}{1+\exp (- \log \frac{P(X|Y=1)P(Y=1)}{P(X|Y=0)P(Y=0)})} \\ &amp; =\sigma \left( \sum_j^d \log \frac{P(X_j|Y=1)}{P(X_j|Y=0)} + \log \frac{P(Y=1)}{P(Y=0)} \right) \end{aligned}$$ and therefore, $$\begin{aligned} P(Y=1|X) &amp; = \sigma \left( \sum_j^d \log \frac{a_j^{X_j}(1-a_j)^{(1-X_j)}}{b_j^{X_j}(1-b_j)^{(1-X_j)}} + \log \frac{p}{1-p} \right) \\ &amp; = \sigma \left( \sum_j^d \left(X_j \log \frac{a_j(1-b_j)}{b_j(1-a_j)}\right) + \log \left(\frac{p}{1-p} \prod_j^d \frac{1-a_j}{1-b_j} \right) \right) \\ &amp; = \sigma \left( \sum_j^d w_j X_j + w_0\right) \end{aligned}$$ where $w_j = \log \frac{a_j(1-b_j)}{b_j(1-a_j)}$ and $w_0=\log \left(\frac{p}{1-p} \prod_j^d \frac{1-a_j}{1-b_j} \right)$. Therefore, $$h(x) = sgn (w^Tx+w_0)$$ and this shows the Naive Bayes is a linear classifier. ◻ </div> <h1 id="naive-bayes-vs-logistic-regression"> <a href="#naive-bayes-vs-logistic-regression" class="anchor-heading" aria-labelledby="naive-bayes-vs-logistic-regression"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> Naive Bayes vs. Logistic Regression </h1> <p>We saw in the last section that Naive Bayes can be a linear classifier under some assumptions, but some of the assumptions are quite strong, such as the conditional independence. Thus, the hypothesis class of Naive Bayes is not all the possible linear classifiers, but only a subset of them. We know the Logistic Regression is another common linear classifier, how does Naive Bayes compare to Logistic Regression? In this section, we will look at them more closely through a theoretic view.</p> <h2 id="asymptotic-regime"> <a href="#asymptotic-regime" class="anchor-heading" aria-labelledby="asymptotic-regime"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> Asymptotic Regime </h2> <p>We first define certain notations for our discussion, let $\epsilon(h_A,N) \equiv L(h_A, S)$, where $|S| = N$. Here, $\epsilon(h_A,N)$ stands for the error of hypothesis $h$ trained using algorithm $A$ from $N$ observations. We will first discuss in the asymptotic setting, which means the number of training data points is infinity.</p> <p>If the two classes is linear separable, which means both models are correct, then we have \(\epsilon(h_{NB}, \infty) = \epsilon(h_{LR},\infty)\) which means, asymptotically, NB and LR produce the identical classifier.</p> <p>If the linear assumption does not hold, which is usually the case, we claim LR is expected to outperform NB, i.e. \(\epsilon(h_{LR},\infty) \leq \epsilon(h_{NB},\infty)\) Intuitively, this can be shown by observing that, since Logistic Regression assumes no other assumption other than linear classification, $\epsilon(h_{LR},\infty)$ converges to $inf_{h\in \mathcal{H}} L(h)$, where $\mathcal{H}$ is the class of all linear classifiers, it must therefore be asymptotically no worse than the linear classifier picked by Naive Bayes, which assumes conditional independence between features.</p> <h2 id="non-asymptotic-regime"> <a href="#non-asymptotic-regime" class="anchor-heading" aria-labelledby="non-asymptotic-regime"><svg viewBox="0 0 16 16" aria-hidden="true"> <use xlink:href="#svg-link"></use> </svg></a> Non-asymptotic Regime </h2> <p>In a more real setting, where we do not have infinite number of training data, we must talk about how fast (how many training sample needed) an estimator converges to its asymptotic limit. This ’rate of convergence’ for Logistic Regression is as below</p> <div id="thm:LR_converge" class="theorem"> **Theorem 2**. *Let $h_{LR,N}$ be a Logistic Regression model in $d$ dimensions. Then, with high probability, $$\epsilon(h_{LR,N} \leq \epsilon(h_{LR},\infty)) + O \left( \sqrt{\frac{d}{N} \log \frac{N}{d}} \right)$$ Thus, for $\epsilon(h_{LR}, N) \leq \epsilon(h_{LR},\infty) + \epsilon_0$ to hold up for a fixed constant $\epsilon_0$, it suffices to pick $N=\Omega(d)$* </div> <p>The proof of theorem <a href="#thm:LR_converge" data-reference-type="ref" data-reference="thm:LR_converge">2</a> follows the application of the uniform convergence bounds to logistic regression, and using the fact that the $d$ dimensional linear classifier $\mathcal{H}$ has a VC dimension of $d+1$. These concepts will be covered in Lecture 5.</p> <p>Now, we want to draw a similar picture for the Naive Bayes classifier and compare it the Logistic Regression. It turns out this is a more challenging task and we will break our analysis into two parts</p> <ul> <li> <p>How fast do parameters of NB converge to their optimal values</p> </li> <li> <p>How fast do the risk if NB converge to the asymptotic risk</p> </li> </ul> <p>For the first part, formally, we have the following theroem</p> <div id="thm:NB_convergence" class="theorem"> **Theorem 3** (). *Let any $\epsilon_1$, $delta &gt;0$ and any $l \leq 0$ be fixed. Assume that for some fixed $\rho_0 &gt; 0$, we hvae $\rho_0 \leq P(y=1) \leq 1- \rho_0$. Let $d = O ((1/\epsilon_1^2)\log(d/\delta))$, then with probability at least $1-\delta$: $$\begin{aligned} |\hat{P}(X_j|Y=b) - P(X_j|Y=b)| \leq \epsilon_1 \end{aligned}$$ and $$\begin{aligned} |\hat{P}(Y=b) - P(Y=b)| \leq \epsilon_1 \end{aligned}$$ for all $j=1,...,d$ and $b \in \mathcal{Y}$* </div> <p>This theorem states that with a number of samples that is only <em>logarithmic</em>, rather than linear, in $d$, the parameters of Naive Bayes are uniformly close to their asymptotic values in $h_{NB,\infty}$.</p> <p>Proof of this theorem is a straightforward application of the Hoeffding bound, here we provide a simple setting where $X \in {0,1 }$ and $P(X=1)=p$. Suppose we have $N$ i.i.d. samples $(x_1,…,x_N)$, then the maximum likelihood estimation for $p$ is simply \(\hat{p} = \frac{1}{N}\sum_i x_i\) The Hoeffding bound for this case states, for all $p \in [0,1]$, $\epsilon &gt;0$ \(P(|p-\hat{p}|&gt;\epsilon) \leq 2 e^{-2N\epsilon^2}\) Intuitively, this means that the probability of the empirical estimation being epsilon-far from the ground truth decays exponentially fast with the number of samples $N$. For a detailed proof of theorem <a href="#thm:NB_convergence" data-reference-type="ref" data-reference="thm:NB_convergence">3</a>, please look at paper <a href="#Ng" data-reference-type="ref" data-reference="Ng">[Ng]</a>.</p> <p>Now we know the parameters of Naive Bayes converge logarithmically to its optimal values, but this doesn’t directly imply the error of Naive Bayes also converges with this rate. To intuitively show why the error also converges, we can first show that the convergence of the parameters implies that $h_{NB,N}$ is very likely to make the same predictions as $h_{NB,\infty}$. Recall that $h_{NB}(x)$ makes its predictions according to \(\ell_{NB}(x)=\log \frac{\hat{P}(Y=1)\prod_j \hat{P}(x_j|Y=1)}{\hat{P}(Y=0)\prod_j \hat{P}(x_j|Y=0)} &gt; 0\) For every example for which both $\ell_{NB}(x)$ and $\ell_{NB,\infty}(x)$ have the same sign, $h_{NB, N}$ and $h_{NB,\infty}$ will make the same prediction. If $h_{NB,\infty} \gg 0$ or $h_{NB,\infty} \ll 0$, as $\ell_{NB}(x)$ is only a small perturbation of $\ell_{NB,\infty}(x)$, they will be on the same side of $0$ with high probability.</p> <p>With some steps of derivation, we will eventually reach the formal risk convergence of Naive Bayes</p> <div id="thm:NB_risk_convergence" class="theorem"> **Theorem 4** (). *Define $G(\tau) = P_{(x,y)\sim \mathbb{P}}\left[ (\ell_{NB,\infty}(x) \in [0,\tau d] \wedge y=1 ) \vee (\ell_{NB,\infty}(x) \in [-\tau d,0] \wedge y=0 ) \right]$. Assume that for some fixed $\rho_0 &gt; 0$, we have $\rho_0 \leq P(y=1) \leq 1-\rho_0)$, and that $\rho_0 \leq P(x_j=1|y=b) \leq 1-\rho_0)$ for all $j$,$b$, then with high probability, $$\epsilon(h_{NB,N}) \leq \epsilon(h_{NB, \infty}) + G\left(O\left(\sqrt{\frac{1}{N}\log d}\right)\right)$$* </div> <p>Here, $G$ defines the fraction of points that are very close to the decision boundary. Intuitively, if we can understand and control the event $G(\tau)$, then we can obtain a more precise control on the bound of the error.</p> <div id="thm:NB_risk_convergence_final" class="theorem"> **Theorem 5** (). *Let the conditions of Theorem <a href="#thm:NB_risk_convergence" data-reference-type="ref" data-reference="thm:NB_risk_convergence">4</a> hold, and suppose $G(\tau) \leq \epsilon/2 +F(\tau)$, for some function $F(\tau)$ that statisfies $F(\tau) \to 0$ as $\tau \to 0$, and some fixed $\epsilon_0 &gt; 0$. Then for $\epsilon(h_{NB,N}) \leq \epsilon (h_{NB,\infty}) + \epsilon_0$ to hold with high probability, it suffices to pick $N=\Omega(\log d)$* </div> <p>Thus, we can conclude that though the asymptotic error of Naive Bayes is greater than Logistic Regression, the convergence rate of NB is only $\Omega(\log d)$, which is faster than $\Omega(d)$ of Logistic Regression.</p> </main> <hr> <footer> <p><a href="#top" id="back-to-top">Back to top</a></p> <div class="d-flex mt-2"> <p class="text-small text-grey-dk-000 mb-0"> <a href="https://shenshen.mit.edu/git/shensquared/gradml/tree/main/supervised/DiscriminativeGenerative.md" id="edit-this-page">Edit this page on Git</a> | This page was last updated on 18-Jun-2023 </p> </div> </footer> </div> </div> <div class="search-overlay"></div> </div> </body> </html>
